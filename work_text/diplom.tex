\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}
\usepackage{cmap}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{longtable}  % Для многостраничных таблиц
\usepackage{array}      % Для дополнительного форматирования столбцов
\usepackage{booktabs}   % Для улучшенного оформления линий

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, д.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
В настоящее время оперативный поиск информации становится критически важной
задачей. Однако анализ полного массива данных невозможен из-за его масштабов,
что создаёт необходимость в классификации и последующей фильтрации данных для
выделения релевантной информации. Решением этой проблемы может служить
тематическая классификация.

Большие объёмы данных, такие как новостные потоки, часто не имеют системной
тематической разметки. Даже при наличии рубрикации, её субъективность может
приводить к проблемам: некорректному присвоению тем, избыточности тематических
категорий и их недостаточному охвату. Это вызывает ошибки при поиске и анализе
информации. Для устранения этих недостатков требуется механизм, обеспечивающий
точную тематическую классификацию с возможностью автоматической разметки
новостных материалов.

Одним из инструментов для реализации такого подхода являются тематические
модели в сочетании с алгоритмами глубокого обучения. Первые позволяют выявить
скрытые темы в текстовых данных и подготовить разметку для обучения вторых.
Алгоритмы глубокого обучения, в свою очередь, могут классифицировать новые
тексты по заданным темам.

Таким образом, целью данной работы является разработка нейросетевого метода
автоматической классификации новостей на основе тематической модели предметной
области.

Для достижения цели необходимо решить следующие задачи:
\begin{enumerate}
    \item Выполнить парсинг новостных данных и их текстовую предобработку;
    \item Провести анализ характеристик и параметров набора данных;
    \item Выполнить тематическое моделирование подготовленных данных с
    оптимальными параметрами;
    \item Разметить данные для обучения нейронной сети"=классификатора с
    помощью тематического моделирования;
    \item Выполнить обучение нейронной сети"=классификатора на размеченных
    данных;
    \item Провести анализ качетсва обученной модели;
    \item Проанализировать эффективность разработанного метода автоматической
    тематической классификации.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Теоретические и методологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Место автоматической тематической классификации новостей в поиске}
Эффективный поиск информации требует предварительной организации данных.
Тематическая классификация улучшает этот процесс за счёт структуризации
контента, фильтрации нерелевантных материалов и выделения целевых категорий.

Таким образом, тематическая классификация будет иметь в процессе поиска
следующий практический смысл:

\begin{enumerate}
    \item Скорость обработки: ручная классификация тысяч новостных
    статей в день невозможна. Алгоритмы на базе BigARTM и
    глубокого обучения справляются с этим за минуты, обеспечивая
    актуальность данных для принятия решений;
    \item Масштабируемость: автоматизация позволяет работать с
    постоянно растущими объёмами информации без значительного увеличения
    ресурсных затрат;
    \item Снижение субъективности: исключаются человеческие ошибки,
    связанные с усталостью или предвзятостью, что повышает достоверность
    результатов.
\end{enumerate}

Автоматическая классификация новостей не заменяет экспертов, но
становится их основным помощником, беря на себя рутинные задачи.
Например, в разведочном поиске это критически важно, так как позволяет
перейти от обработки данных к их осмысленному использованию "--- будь
то стратегическое планирование или оперативное управление.

Технологии вроде BigARTM и методов глубокого обучения обеспечивают
баланс между скоростью, точностью и адаптивностью, что делает их
незаменимыми в работе с динамичными новостными потоками.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Сбор новостных данных данных}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор метода получения новостных данных}
Для получения данных с сайтов существует три основных метода:
\begin{itemize}
\item Ручной сбор "--- извлечение информации человеком вручную;
\item Запрос данных "--- получение информации от владельцев с последующим
скачиванием;
\item Программный сбор "--- автоматизированное извлечение данных.
\end{itemize}

Первый метод можно исключить из рассмотрения из"=за низкой эффективности.
Второй метод применим не во всех случаях: владельцы информационных платформ
вряд ли будут оперативно предоставлять данные по каждому запросу. Таким
образом, наиболее целесообразным остаётся третий метод "--- программный сбор.

Среди методов программного сбора оперативно и эффективно получать данные
в большинстве случаев позволяют инструменты веб"=скрапинга~\cite{mthd_prcng}.
Далее в работе будет использован именно этот метод для формирования новостного
массива, так как он прост в изученни, а также обеспечивает баланс между
скоростью получения данных и минимальными требованиями к стороннему участию.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор новостной платформы для сбора данных}
В рамках данной работы основным объектом исследования являются новостные
текстовые данные. Для их сбора необходимо выбрать подходящий веб"=ресурс.

При наличии нескольких потенциальных источников выбор следует осуществлять
на основе анализа HTML структуры сайта по следующим критериям:
\begin{enumerate}
    \item Единая структура документов на всём сайте;
    \item Отсутствие блокировок HTTP"=запросов от скраперов;
    \item Статичность контента "--- полная доступность HTML"=кода страницы при
    первичном запросе без динамической подгрузки.
\end{enumerate}

Идеальный случай "--- соответствие всем трём пунктам. При этом:

\begin{enumerate}
    \item Ограничения по пунктам 2 и 3 в большинстве случаев можно обойти
    стандартными методами;
    \item Нарушение пункта 1 создаёт принципиальные сложности: обработка
    разноформатных данных может потребовать ручной настройки для каждого
    документа.
\end{enumerate}

В качестве источника выбран новостной сайт НИУ ВШЭ. Этот ресурс:

\begin{enumerate}
    \item Имеет единую структуру новостных материалов;
    \item Не блокирует автоматизированные запросы;
    \item Предоставляет полный HTML"=код страницы без динамической генерации
    контента.
\end{enumerate}

Указанные характеристики делают сайт ВШЭ оптимальным вариантом для реализации
поставленных задач.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка собранных данных}  
Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы предобработки
включают~\cite{mthd_prpr_txt}:

\begin{enumerate}
    \item Очистка от технического шума:
    \begin{itemize}
        \item Удаление лишних пробелов и переносов строк;
        \item Очистка от специальных символов (скобки, HTML"=теги, эмодзи);
        \item Нормализация регистра (приведение текста к нижнему регистру).
    \end{itemize}

    \item Токенизация: разделение текста на семантические единицы
    (слова, предложения);  

    \item Лемматизация: приведение словоформ к лемме (словарной форме);  

    \item Удаление стоп"=слов: исключение частотных слов с низкой
    смысловой нагрузкой (предлоги, союзы, частицы);  
\end{enumerate}

\paragraph{Обоснование выбора способа приведения слова к начальной форме.}
\phantom{перенос}

В отличие от стемминга (например, алгоритм Snowball), который применяет
шаблонное усечение окончаний, лемматизация обеспечивает точное приведение
слов к нормальной форме с сохранением семантики~\cite{mthd_prpr_txt}.
Это критически важно для тематического моделирования, где искажение смысла
слов может привести к некорректной интерпретации контекста.
На рис.~\ref{fig:03} показаны принципиальные различия между двумя подходами.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/different_stem_and_lem.png}
	\caption{\label{fig:03}%
	Иллюстрация разницы между стеммингом и лемматизацией}
\end{figure}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных, который
позволяет выявить семантические структуры в коллекциях документов.

Основная идея тематического моделирования~\cite{BigARTM} заключается
в том, что слова в тексте связаны не с конкретным документом, а с темами.
Сначала текст разбивается на темы, и каждая из них генерирует слова для
соответствующих позиций в документе. Таким образом, сначала формируется тема,
а затем тема определяет термины.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и совместной встречаемости слов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Ниже приведены
основные количественные характеристики, использующиеся при тематическом
моделировании~\cite{ARTM}:

\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и
аксиомы~\cite{BigARTM}:

\begin{itemize}
    \item независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом~\cite{BigARTM}:

\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируется
    терм $p(w|d, t)$.
\end{enumerate}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_1.png}
	\caption{\label{fig:2}%
	Алгоритм формирования документа}
\end{figure}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности~\cite{teorver, BigARTM}:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:

\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного
разложения~\ref{fig:1}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_2.png}
	\caption{\label{fig:1}%
	Стохастическое матричное разложение}
\end{figure}

Таким образом, тематическое моделирование ищет величину $p(w|d)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Решение задачи тематического моделирования (обратной задачи)}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Лемма о максимизации функции на единичных симплексах.} \label{sec:06}
\phantom{перенос}

Перед тем как перейти к решению обратной задачи, сформулируем лемму,
которая поможет в этом процессе~\cite{BigARTM}.

Приведём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}{norm}(x_i) =
    \frac{max\{x_i, 0\}}{\sum_{k \in I} max\{x_k, 0\}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных
симплексах~\cite{BigARTM, simplex}:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Сведение обратной задачи к максимизации функционала.}
\phantom{перенос}

Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия~\cite{teorver}, согласно которому будут подобраны параметры
$\Phi, \Theta$ такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} \to \underset{\Phi, \Theta}{max}
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation} \label{eq:02}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation} \label{eq:03}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
	\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

Таким образом, обратная задача сводится к задаче максимизации
функционала~\cite{ARTM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Аддитивная регуляризация тематических моделей.}
\phantom{перенос}

Задача, описываемая уравнением~\ref{eq:02}, не соответствует критериям корректно
поставленной задачи по Адамару~\cite{Adamar}, поскольку в общем случае она имеет
бесконечное множество решений. Это свидетельствует о необходимости доопределения
задачи. 

Для доопределения некорректно поставленных задач применяется
регуляризация~\cite{Adamar}: к основному критерию добавляется дополнительный
критерий "--- регуляризатор, который соответствует специфике решаемой задачи. 

Метод ARTM (аддитивная регуляризация тематических моделей~\cite{BigARTM})
основывается на максимизации линейной комбинации логарифма правдоподобия и
регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами
регуляризации $\tau_i, \;\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки~\ref{eq:03}.

Регуляризатор (или набор регуляризаторов) выбирается в соответствии с решаемой
задачей.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Сведение задачи тематического моделирования к E"=M алгоритму.}
\phantom{перенос}

Из представленных выше ограничений~\ref{eq:03} следует, что столбцы матриц можно
считать неотрицательными единичными векторами. Таким образом, задача сводится к
максимизации функции на единичных симплексах~\cite{BigARTM}.

Воспользуемся леммой о максимизации функции на единичных
симплексах~\ref{sec:06} и перепишем задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений с вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation} \label{eq:04}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E"=шагу, а вторая и третья строки "--- M"=шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные варианты регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Дивергенция Кульбака"=Лейблера.} \label{sec:07}
\phantom{перенос}

Перед тем как перейти к регуляризаторам необходимо ввести меру оценки близости
тем.

Чтобы оценить близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера~\cite{Adamar, BigARTM} (KL или KL"=дивергенция).
KL"=дивергенция позволяет оценить степень вложенности одного распределения в
другое, в случае тематического моделирования будет оценитьваться вложенность
матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$ \label{it:kl3}
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

Теперь можно перейти к рассмотрению регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор сглаживания.}
\phantom{перенос}

Сглаживание предполагает сематническое сближение тем, это может быть полезно в
следующих случаях~\cite{reg}:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:05}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм соответствующий модели
LDA~\cite{BigARTM, Adamar}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор разреживания.}
\phantom{перенос}

Разреживание подразумевает разделение тем и документов, исключая общие слова из
них. Этот тип регуляризации основывается на предположении, что темы и документы
в основном являются специфичными и описываются относительно небольшим набором
терминов, которые не встречаются в других темах~\cite{reg, BigARTM}.

Определим регуялризатор разреживания:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу разреживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:06}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, разреживающий
матрицы $\Phi$ и $\Theta$~\cite{reg, BigARTM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор декоррелирования тем.}
\phantom{перенос}

Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем~\cite{reg, BigARTM}.

Определим регуляризатор декоррелирования:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:07}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{s \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, декоррелирующий
темы~\cite{reg, BigARTM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Оценка качества моделей}
После построения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических
моделей~\cite{BigARTM}:

\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item полнота и точность тематического поиска;
        \item качество ранжирования при тематическом поиске;
        \item качество классификации / категоризации документов;
        \item качество суммаризации / сегментации документов;
        \item экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item правдоподобие и перплексия;
        \item средняя когерентность (согласованность тем);
        \item разреженность матриц $\Phi$ и $\Theta$;
        \item различность тем;
        \item статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Поскольку оценка по внешним критериям невозможна в рамках данной работы,
сосредоточимся на внутренних критериях оценки, которые можно вычислять
автоматически.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Правдоподобие и перплексия.}
\phantom{перенос}

Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией~\cite{BigARTM}.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что при равномерном распределении слов в тексте выполняется
равенство $p(w|d) = \frac{1}{|W|}$. В этом случае значение перплексии равно
мощности словаря $P = |W|$. Это позволяет сделать вывод, что перплексия является
мерой разнообразия и неопределенности слов в тексте: чем меньше значение
перплексии, тем более разнообразны вероятности появления слов.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Когерентность.}
\phantom{перенос}

Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем~\cite{BigARTM}.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PMI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$; $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация;
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно); $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Разреженность.}
\phantom{перенос}

Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и
$\Theta$.

Разреженность играет ключевую роль в выявлении различий между
темами~\cite{BigARTM}. Каждая тема формируется на основе ограниченного набора
слов, в то время как остальные слова должны встречаться реже, что отражается в
нулевых элементах матриц. Оптимальный уровень разреженности должен быть высоким, но не чрезмерным:
в таком случае темы будут четко различимы. Если разреженность слишком низка,
темы могут сливаться, а если слишком высока "--- содержать недостаточное
количество слов для адекватного представления.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Чистота темы.}
\phantom{перенос}

Чистота темы:
\begin{equation}
	\sum_{w \in W_t} p(w|t),
\end{equation}

где $W_t$ "--- ядро темы:
$W_t = \{w: p(w|t) > \alpha\}$, где $\alpha$ подбирается по разному,
например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$.

Данная характеристика показывает как вероятностно относится ядро темы к фоновым
словам темы, следовательно, чем больше вероятность ядра, тем
лучше~\cite{reg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Контрастность темы.}
\phantom{перенос}

Контрастность темы:
\begin{equation}
	\frac{1}{|W_t|} \sum_{w \in W_t} p(t|w).
\end{equation}

Данная характеристика показывает насколько часто слова из ядра темы
встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
в других темах, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Методы обработки текста с помощью нейросетей}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Проблема представления текста в пространстве чисел}
Нейронные сети умеют работать только с числами, поэтому встаёт вопрос о
том, как наилучшим образом переносить текст в пространство чисел. Такой
способ переноса должен быть не только быстрым, точным и способным
вмещать в себя тысячи слов, но ещё и учитывать, что естественный язык
имеет временную зависимость: слова в предложении складываются
последовательно и зависят друг от друга, а не существуют в вакууме,
что дополнительно усложняет задачу.

Тогда формализуем качества, которыми должен обладать способ представления
текста в виде чисел:

\begin{itemize}  
    \item Выразительность:  
    \begin{enumerate}  
        \item Способность различать тысячи слов;  
        \item Способность учитывать контекст (временную зависимость между
        словами).  
    \end{enumerate}  
    
    \item Скорость: эффективно работать с высокоразмерными данными
    на современном оборудовании;  
    
    \item Эффективность: иметь компактное представление и
    адаптироваться к новым словам.  
\end{itemize}  

Теперь кратко рассмотрим некоторые из методов представления текста в виде
чисел:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Мешок слов (Bag"=of"=Words).}
\phantom{перенос}

Одним из самых простых способов численного представления текста является
мешок слов~\cite{DLS-embeddings}.

Данный метод работает следующим образом~\cite{goldberg-nlp}:

\begin{enumerate}  
    \item Создаётся словарь с уникальными индексами;  
    \item Каждое слово кодируется one"=hot вектором:  
    \begin{equation}  
        v_i = [a_1, \dots, a_N], \quad a_j = \begin{cases}  
            1, & j = i \\  
            0, & j \neq i  
        \end{cases}  
    \end{equation}  
    где $N$ "--- размер словаря.  
    
    \item Предложение представляется суммой векторов слов:  
    \begin{equation}  
        s = [f_1, \dots, f_N], \quad f_j = \text{частота слова } j \text{ в предложении}.  
    \end{equation}  
\end{enumerate} 

Данный метод, несмотря на свою простоту, не может быть выбран из"=за
ряда существенных недостатков~\cite{goldberg-nlp, DLS-embeddings}:

\begin{enumerate}  
    \item Высокая размерность и разреженность данных;  
    \item Игнорирование порядка слов;  
    \item Отсутствие учёта семантики (все слова ортогональны);  
    \item Сложность адаптации к новым словам (требуется пересчёт словаря).  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{TF"=IDF взвешивание.}
\phantom{перенос}

Улучшение BoW: элементы вектора предложения умножаются на TF"=IDF веса слов,
это частично решает проблему семантической значимости, но сохраняет другие
недостатки BoW~\cite{goldberg-nlp, DLS-embeddings}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Эмбеддинги слов.}
\phantom{перенос}

Семантические векторные представления слов~\cite{goldberg-nlp, DLS-embeddings}:  
\begin{itemize}  
    \item Каждому слову сопоставляется плотный вектор фиксированной
    размерности (обычно 50"=300);  
    \item Векторы обучаются так, чтобы семантически близкие слова имели
    схожие эмбеддинги;  
    \item Матрица эмбеддингов "--- обучаемый параметр нейросети.  
\end{itemize}

Данный способ максимально полно соответствует описанным ранее критериям,
обладая благодаря своей природе следующими
преимуществами~\cite{goldberg-nlp, DLS-embeddings}:

\begin{enumerate}  
    \item Низкая размерность;  
    \item Учёт семантики;
    \item Возможность учёта контекста;  
    \item Гибкость: новые слова можно добавлять через дообучение.  
\end{enumerate} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор архитектуры нейронной сети}  

Так как представление текста в виде эмбеддингов удовлетворяет описанным
выше критериям то, будем рассматривать архитектуры нейронных сетей,
разработанные для работы с ними: рекуррентные нейронные сети (RNN) и
трансформеры.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Рекуррентные нейронные сети (RNN).}
\phantom{перенос}

Рекуррентные нейронные сети обрабатывают последовательность слов рекуррентно,
шаг за шагом обновляя своё состояние на основе текущего слова и предыдущих
значений~\cite{goldberg-nlp, DLS-RNN}. Это позволяет
учитывать~\cite{goldberg-nlp, DLS-RNN}:

\begin{itemize}  
    \item Порядок слов;  
    \item Контекст (благодаря механизмам памяти в LSTM/GRU).  
\end{itemize}  

Недостатки~\cite{goldberg-nlp, DLS-RNN}:

\begin{enumerate}  
    \item Низкая скорость: вычисления последовательны, невозможна
    параллелизация;  
    \item Проблемы с длинными последовательностями:  
    \begin{enumerate}  
        \item Забывание раннего контекста;  
        \item Затухание/взрыв градиентов при обучении.  
    \end{enumerate}  
\end{enumerate}  

Преимущества~\cite{goldberg-nlp, DLS-RNN}:

\begin{enumerate}  
    \item Менее требовательны к вычислительным ресурсам;  
    \item Эффективны на малых объёмах данных.  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Трансформеры.}
\phantom{перенос}

Нейронные сети"=трансформеры обрабатывают всю последовательность слов
одновременно благодаря механизму внимания
(attention)~\cite{goldberg-nlp, DLS-Transformer}.

Ключевые особенности~\cite{goldberg-nlp, DLS-Transformer}:

\begin{itemize}  
    \item Параллельные вычисления, а следовательно и высокая скорость;  
    \item Учёт контекста через self"=attention;  
    \item Позиционные энкодинги позволяют учитывать порядок слов.  
\end{itemize}  

Недостатки~\cite{goldberg-nlp, DLS-Transformer}:

\begin{enumerate}  
    \item Высокие требования к вычислительным ресурсам;  
    \item Требуют больших объёмов данных для обучения.  
\end{enumerate}  

Преимущества~\cite{goldberg-nlp, DLS-Transformer}:

\begin{enumerate}  
    \item Эффективны для длинных текстов;  
    \item Имеют лучшее качество на сложных задачах.  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Определение с типом нейронной сети.} \label{sec:17}
\phantom{перенос}

В рамках данной работы рассматривается тематическая классификация текстов,
то есть предполагается, что по содержимому длинной входящей последовательности
принимается решение о её принадлжности к той или иной теме.

Тогда для данной задачи критичны:

\begin{enumerate}  
    \item Обработка длинных последовательностей;  
    \item Скорость предсказания;  
    \item Использование современных вычислительных ресурсов.  
\end{enumerate}  

Таким образом, для решения поставленной задачи больше подходят
сети"=трансформеры, так как:

\begin{itemize}  
    \item Проблемы с ресурсами решаются облачными сервисами;  
    \item Доступны предобученные модели (BERT, GPT);  
    \item Механизм внимания~\cite{goldberg-nlp, DLS-Transformer} лучше
    улавливает тематические связи.  
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Оценка качества работы нейронных сетей}
Для оценки качества классификации нейронными сетями используются несколько
базовых метрик~\cite{metriki_klf}.

Прежде чем перейти к рассмотрению метрик, приведём основные
обозначения~\cite{metriki_klf}:

\begin{itemize}
    \item $TP$ (true positive) "--- объект верно отнесён к целевому классу;
    \item $TN$ (true negative) "--- объект верно не отнесён к целевому классу;
    \item $FP$ (false positive) "--- объект ошибочно отнесён к целевому классу;
    \item $FN$ (false negative) "--- объект ошибочно не отнесён к целевому классу.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Accuracy.}
\phantom{перенос}

Accuracy вычисляется по формуле~\cite{metriki_klf}:

\begin{equation}
    Acc = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Эта метрика показывает общую долю верных классификаций. Несмотря на простоту
интерпретации, accuracy часто оказывается недостаточно информативной при
работе с несбалансированными данными~\cite{metriki_klf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Precision.}
\phantom{перенос}

Precision (точность предсказания) вычисляется как~\cite{metriki_klf}:

\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

Метрика отражает долю верно классифицированных объектов среди всех примеров,
отнесённых классификатором к целевому классу~\cite{metriki_klf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Recall.}
\phantom{перенос}

Recall (полнота) определяется формулой~\cite{metriki_klf}:

\begin{equation}
    Recall = \frac{TP}{TP + FN}
\end{equation}

Метрика показывает долю верно распознанных объектов целевого класса относительно
их общего количества~\cite{metriki_klf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{F1-мера.}
\phantom{перенос}

F1-мера вычисляется по формуле~\cite{metriki_klf}:

\begin{equation}
    F_{1} = 2 \cdot \frac{precision \cdot recall}{precision + recall}
\end{equation}

Эта метрика представляет собой гармоническое среднее precision и recall. Она
полезна при необходимости балансировки двух показателей~\cite{metriki_klf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Confusion matrix.}
\phantom{перенос}

Матрица ошибок (confusion matrix) наглядно визуализирует распределение ошибок
классификации по классам. Хотя её использование для сравнения моделей может
быть затруднительно из"=за большого размера, она эффективна для демонстрации
качества итоговой модели~\cite{metriki_klf}.

Пример матрицы ошибок представлен на рис.~\ref{fig:04}:

\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/confusion_matrix.png}
	\caption{\label{fig:04}%
	Иллюстрация примера матрицы ошибок}
\end{figure}
\newpage
\phantom{чтобы рисунок был вверху}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Практико"=технологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Получение новостного массива путём веб"=скраппинга}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов получения новостных данных}
Для веб"=скрапинга доступны библиотеки на разных языках, однако выбор логично
сделать в пользу Python "--- наиболее популярного языка для обработки данных
и работы с глубоким обучением. Среди Python"=библиотек ключевыми
являются~\cite{parsing}:

\begin{itemize}
    \item requests "--- для отправки HTTP"=запросов;
    \item BeautifulSoup4 "--- для парсинга HTML"=кода в удобную объектную
    структуру;
    \item selenium "--- для работы с динамическими сайтами, где контент
    генерируется JavaScript.
\end{itemize}

Первые две библиотеки эффективны для статических страниц: requests получает
исходный код, а BeautifulSoup4 извлекает данные через поиск по тегам.
Selenium же имитирует взаимодействие реального браузера, что позволяет
обрабатывать страницы с отложенной загрузкой контента~\cite{parsing}.

Этот набор инструментов покрывает потребности работы с подавляющим
большинством сайтов "--- от простых статических ресурсов до сложных
веб"=приложений~\cite{parsing}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Реализация алгоритма сбора новостных данных}
Для такого простого и имеющего хорошую структуру новостного сайта ВШЭ
не потребуется библиотеки Selenium, достаточно только BeautifulSoup4
и requests.

Алгоритм сбора данных включает следующие этапы~\cite{parsing}:

\begin{enumerate}
    \item Анализ структуры сайта:
    \begin{itemize}
        \item Многостраничный ресурс с 10 новостными карточками на каждой
        странице;
        \item Карточка новости содержит: ссылку, дату публикации, заголовок,
        краткое содержание;
        \item Полный текст доступен по отдельной ссылке внутри карточки.
    \end{itemize}

    Пример страницы сайта можно увидеть в приложении~\ref{sec:15}.

    \item Получение и сохранение данных с сайта с помощью методов библиотеки
    requests~\cite{parsing, requests} (листинг~\ref{lst:01}):
    \begin{itemize}
        \item Получение HTML"=кода страницы через requests.get();
        \item Сохранение сырых данных для последующей обработки.
    \end{itemize}

    \item Извлечение метаданных с помощью библиотеки
    BeautifulSoup4~\cite{parsing, bs4} (листинг~\ref{lst:02}):
    \begin{itemize}
        \item Парсинг сохранённого HTML через BeautifulSoup4;
        \item Поиск элементов по тегам и CSS"=классам (find(), find\_all());
        \item Извлечение текстового содержимого (text, get()).
    \end{itemize}

    \item Получение полного текста новостис помощью библиотеки
    BeautifulSoup4~\cite{parsing, bs4} (листинг~\ref{lst:03}):
    \begin{itemize}
        \item Рекурсивное использование get\_page() для целевых URL;
        \item Анализ структуры контентной страницы.
    \end{itemize}

    \item Обработка страницы целиком (листинг~\ref{lst:04}):
    \begin{itemize}
        \item Итерация по 10 элементам div.post на странице;
        \item Использование find\_next\_sibling() для навигации;
        \item Сохранение результатов в pandas DataFrame для анализа.
    \end{itemize}

    \item Масштабирование на все страницы (листинг~\ref{lst:05}):
    \begin{itemize}
        \item Динамическое формирование URL через модификацию параметров;
        \item Пакетная обработка через цикл с изменяемым индексом страницы.
    \end{itemize}

    \item Оптимизация производительности с помощью средств языка
    Python~\cite{python-book, pandas} (листинг~\ref{lst:06}):
    \begin{itemize}
        \item Реализация многопоточности через стандартные средства Python;
        \item Создание изолированных DataFrame для каждого потока;
        \item Агрегация результатов после завершения параллельных задач.
    \end{itemize}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Результаты сбора данных с сайта ВШЭ} \label{sec:21}
В результате выполнения кода был получен набор данных в формате
Excel.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=17cm]{./images/primer_parcing_news.png}
	\caption{\label{fig:08}%
	Иллюстрация структуры собранных данных}
\end{figure}
\newpage

Количественные характеристики полученного набора данных представлены в
таблице~\ref{tab:parcing_dataset_char}.

\begin{center}
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{4pt}
\footnotesize
\end{center}

\begin{longtable}{|l|c|}
\caption{Характеристики исходного набора данных}\label{tab:parcing_dataset_char} \\
\hline
\textbf{Характеристика} & \textbf{Значение} \\
\hline
\endfirsthead

\multicolumn{2}{c}{{\normalsize Продолжение таблицы \ref{tab:parcing_dataset_char}}} \\
\hline
\textbf{Характеристика} & \textbf{Значение} \\
\hline
\endhead

\hline
\multicolumn{2}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17430 \\
\hline
Кол. токенов & 12 131 111 \\
\hline
Кол. уник. ток. & 278 724 \\
\hline
Мин. кол. ток. в док. & 6 \\
\hline
Модальное кол. ток. в док. & 47 \\
\hline
Среднее кол. ток. в док. & 695 \\
\hline
Макс. кол. ток. в док. & 6514 \\
\hline
Мин. кол. уник. ток. в док. & 6 \\
\hline
Мод. кол. уник. ток. в док. & 39 \\
\hline
Сред. кол. уник. ток. в док. & 346 \\
\hline
Макс. кол. уник. ток. в док. & 2287 \\
\hline
\end{longtable}

Анализ представленных характеристик показывает, что документы имеют
значительный объём (большая длина текстов), при этом общий размер набора
данных ограничен (17 тыс. документов). Это может повлиять на результаты
тематического моделирования и глубокого обучения.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка новостного массива}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов для подготовки данных}  
Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim~\cite{nauk-NLTK-Spacy}.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов~\cite{nauk-NLTK-Spacy, habr-NLTK-Spacy}. Определение
стоп"=слов в обоих случаях происходит по заранее заданным словарям, поэтому
разницы здесь нет~\cite{nauk-NLTK-Spacy, habr-NLTK-Spacy}. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает её использование~\cite{nauk-NLTK-Spacy, habr-NLTK-Spacy}.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по
предопределённому списку, который не является исчерпывающим. Это связано с
тем, что набор стоп"=слов зависит от тематики текста, и универсального
решения не существует. Для дополнительной фильтрации применим метрику
TF"=IDF, которая оценивает значимость слов. Формула расчёта~\cite{TF-IDF}:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа
$d$~\cite{TF-IDF}. Её расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп"=слова) и Gensim (расширенная
фильтрация через TF"=IDF).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление лишних пробелов и переносов строк}
Для корректной токенизации и анализа текстовых данных требуется
предварительная очистка от лишних пробелов и переносов строк.
Реализацию этой процедуры можно выполнить с помощью встроенных
методов обработки строк в Python.

Алгоритм функции включает три этапа:
\begin{enumerate}
    \item Копирование значимых символов:
    Посимвольное добавление содержимого исходной строки в результирующий
    буфер до обнаружения пробела или переноса строки.

    \item Нормализация пробелов: 
    При обнаружении пробела/переноса:
    \begin{itemize}
        \item Добавление одного пробела в буфер
        \item Пропуск всех последующих пробелов/переносов до первого
        непробельного символа
    \end{itemize}

    \item Циклическая обработка: 
    Повтор шагов 1"=2 до полного прохода исходной строки.
\end{enumerate}

Реализация функции представлена в листинге на
рис.~\ref{lst:07}~\cite{python-book}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разделение строк на русские и английские фрагменты}
Библиотека SpaCy использует предобученные языковые модели, каждая из
которых оптимизирована для обработки одного языка (например, отдельно
для русского и английского)~\cite{spacy}.

Для новостных материалов ВШЭ, содержащих смешанные языковые фрагменты,
применение единой модели недопустимо. Решение заключается в предварительном
разделении текста на русскоязычные и англоязычные сегменты с последующей
обработкой соответствующими моделями.

Алгоритм разделения текста:
\begin{enumerate}
    \item Инициализация языка:
    \begin{itemize}
        \item Определение языка первого буквенного символа строки
        \item Установка текущего языкового идентификатора (RU/EN)
    \end{itemize}

    \item Построение сегментов:
    \begin{itemize}
        \item Посимвольное накопление символов во временном буфере
        \item Прерывание потока при обнаружении символа другого языка
    \end{itemize}

    \item Сохранение результата:
    \begin{itemize}
        \item Фиксация сегмента в формате (язык, текст)
        \item Сброс временного буфера
    \end{itemize}

    \item Циклическое выполнение:
    Повтор шагов 2"=3 до полной обработки строки с автоматическим переключением
    языкового идентификатора.
\end{enumerate}

Реализация функции представлена в листинге на
рис.~\ref{lst:08}~\cite{python-book}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Очистка от неалфавитных токенов и удаление крайних неалфавитных
символов из токенов}
В текстах часто встречаются токены, содержащие неалфавитные символы. Кроме
того, при токенизации могут сохраняться примыкающие к словам знаки пунктуации
(например, точки или дефисы), что требует дополнительной обработки.

Удаление всех неалфавитных символов из текста некорректно, поскольку некоторые
из них могут быть частью терминов и аббревиатур. Их полное удаление может
исказить смысл. Более обоснованный подход "--- удаление токенов, в которых доля
неалфавитных символов превышает установленный порог (например, 50 процентов).
Это реализуется путём подсчёта соотношения буквенных и небуквенных символов в
каждом токене.

Алгоритм фильтрации включает следующие шаги:
\begin{enumerate}
    \item Удаление неалфавитных символов в начале и конце токена;
    \item Подсчёт количества неалфавитных символов в токене;
    \item Удаление токена, если доля неалфавитных символов превышает 50
    процентов.
\end{enumerate}

Реализация соответствующих функций представлена в
листинге на рис.~\ref{lst:09}~\cite{python-book}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Токенизация, лемматизация и удаление стоп"=слов по словарю}
Библиотека SpaCy предоставляет унифицированный интерфейс для лингвистической
обработки текста~\cite{spacy}. Её функционал позволяет выполнять всё в одном
конвейере~\cite{spacy}:

\begin{itemize}
    \item Токенизацию;
    \item Лемматизацию;
    \item Идентификацию стоп"=слов
\end{itemize}

Принцип работы~\cite{spacy}:

\begin{enumerate}
    \item На вход подаётся текстовая строка;
    \item Обработанные данные возвращаются в виде последовательности токенов;
    \item Каждый токен содержит:
    \begin{itemize}
        \item Исходную словоформу;
        \item Нормализованную лемму;
        \item Флаг принадлежности к стоп"=словам
    \end{itemize}
\end{enumerate}

Результирующая строка формируется путём фильтрации: сохраняются только
леммы токенов, не отнесённых к стоп"=словам.

Пример обработки русскоязычного текста показан в
листинге на рис.~\ref{lst:10}~\cite{spacy}.

Полный алгоритм предобработки, объединяющий нормализацию пробелов,
токенизацию и фильтрацию, реализован в листинге на
рис.~\ref{lst:11}~\cite{spacy}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление высокочастотных и низкочастотных токенов}
Помимо стандартных стоп"=слов и стоп"=слов, вычисленных с помощью метрики
TF"=IDF, следует учитывать токены, встречающиеся либо в слишком большом, либо в
слишком малом количестве документов.

Токены, присутствующие в подавляющем большинстве документов, обычно не несут
смысловой нагрузки для конкретной темы, поскольку являются общеупотребительными
для всего корпуса.

Токены, встречающиеся в крайне малом числе документов, также имеют ограниченную
ценность, так как их редкость снижает способность характеризовать тематические
различия.

Алгоритм удаления таких токенов включает следующие шаги:
\begin{enumerate}
    \item Определение нижнего и верхнего порогов встречаемости токенов в
    документах;
    \item Вычисление для каждого токена количества документов, в которых он
    встречается;
    \item Удаление токенов, частота встречаемости которых выходит за
    установленные пороги.
\end{enumerate}

Реализация алгоритма представлена в
листинге на рис.~\ref{lst:34}~\cite{pandas, python-book}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление стоп"=слов с помощью метрики TF"=IDF}
Как отмечалось ранее, удаление стоп"=слов исключительно по предзаданному
словарю имеет ограниченную эффективность. Для повышения качества фильтрации
предлагается дополнительное использование метрики TF"=IDF, позволяющей
оценивать значимость терминов в корпусе документов~\cite{TF-IDF}.

Алгоритм расширенной фильтрации:

\begin{enumerate}
    \item Вычисление TF"=IDF:
    \begin{enumerate}
        \item Формирование словаря терминов с помощью Gensim;
        \item Построение частотного корпуса документов;
        \item Расчёт весов TF"=IDF для каждого термина
    \end{enumerate}

    Реализация базового расчёта представлена в
    листинге на рис.~\ref{lst:12}~\cite{gensim}.

    \item Коррекция словаря:
    \begin{enumerate}
        \item Добавление терминов с нулевым TF"=IDF, исключённых Gensim
        по умолчанию~\cite{gensim};
        \item Нормализация структуры данных для последующего анализа;
    \end{enumerate}

    Соответствующая доработка реализована в листинге на
    рис.~\ref{lst:13}~\cite{gensim}.

    \item Определение порога отсечения:
    \begin{enumerate}
        \item Вычисление n"=го процентиля распределения TF"=IDF;
        \item Установка границы для отбора малозначимых терминов;
    \end{enumerate}

    Логика расчёта границы показана в листинге на
    рис.~\ref{lst:14}~\cite{numpy}.

    \item Фильтрация датасета:
    \begin{enumerate}
        \item Итеративное удаление терминов с TF'=IDF ниже порога;
        \item Дополнительная очистка низкочастотных слов (менее k вхождений);
    \end{enumerate}

    Финальный этап обработки представлен в листинге на рис.~\ref{lst:15}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Очистка набора данных от пустых документов}
После удаления стоп"=слов и неалфавитных символов необходимо выполнить
заключительный шаг "--- удаление документов, содержащих недостаточное
количество токенов или не содержащих их вовсе. Это важно для обеспечения
корректности последующего тематического моделирования и глубокого обучения.

Реализация данного этапа представлена в
листинге на рис.~\ref{lst:35}~\cite{python-book}.

Таким образом, был реализован полный процесс подготовки текстовых данных для
последующего анализа. Исходный код обработчика данных доступен в
приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Вычисление тематической модели}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов для тематического моделирования}
При разработке системы автоматической классификации новостей выбор
инструментов напрямую влияет на гибкость, скорость и качество модели.
Библиотека BigARTM (Additive Regularization of Topic Models) была выбрана
по нескольким ключевым критериям, которые делают её предпочтительной на
фоне альтернатив, таких как Gensim или Mallet.

Критерии выбора:

\begin{enumerate}
    \item Удобный интерфейс: BigARTM предоставляет простой API для
    работы с тематическими моделями, что ускоряет интеграцию в существующие
    пайплайны обработки текстов. Например, загрузка данных, настройка
    параметров и запуск обучения выполняются минимальным количеством кода,
    снижая риск ошибок и время на разработку;
    \item Разнообразие регуляризаторов: библиотека поддерживает множество
    регуляляризаторов (например, сглаживание, разреживание тем),
    которые можно комбинировать для улучшения интерпретируемости и
    точности модели. Это критически важно для новостных данных, где
    темы часто пересекаются (например, «экономика» и «политика»);
    \item Блочный синтаксис: настройка модели в BigARTM осуществляется
    через декларативное описание компонентов (блоков), что упрощает
    эксперименты с архитектурой. Например, можно быстро добавить
    регуляризатор для контроля за размером тем или подключить модуль
    для обработки мультимодальных данных;
    \item Доступность туториалов: BigARTM имеет подробную документацию
    и примеры использования, включая готовые сценарии для классификации
    текстов. Это сокращает время на изучение библиотеки и позволяет
    сосредоточиться на решении прикладных задач.
\end{enumerate}

BigARTM сочетает в себе специализацию для работы с текстами, гибкость
настройки и низкий порог входа благодаря понятному синтаксису. Это делает
её оптимальным выбором для задач автоматической классификации новостей,
где важно быстро адаптировать модель под изменяющиеся условия (например,
появление новых тем) и контролировать качество результатов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Недостающий функционал библиотеки BigARTM}
Тематическое моделирование с использованием библиотеки BigARTM обладает
практической ценностью, но имеет ряд ограничений:

\begin{enumerate}
\item Отсутствие встроенной метрики оценки когерентности тематик;
\item Сложность интеграции регуляризаторов из"=за многоэтапного API;
\item Трудоёмкое преобразование данных в требуемый формат представления;
\item Недостаток инструментов визуализации для мониторинга качества моделей;
\item Отсутствие автоматизированных методов подбора гиперпараметров.
\end{enumerate}

Наибольшее влияние на качество моделирования оказывает первый фактор.
Остальные ограничения преимущественно связаны с эргономикой рабочего
процесса, но их совокупность существенно увеличивает сложность поддержки
кодовой базы.

Для компенсации выявленных недостатков предлагается разработка двух
вспомогательных классов, расширяющих функционал библиотеки:

\begin{enumerate}
    \item My\_BigARTM\_model "--- обёртка над BigARTM для добавления
    недостающих метрик, их визуализаций, а также для упрощения
    взаимодействия с BigARTM;
    \item Hyperparameter\_optimizer "--- автоматический оптимизатор
    гиперпараметров.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Функциональности классов My\_BigARTM\_model и
\\ Hyperparameter\_optimizer}
В рамках класса My\_BigARTM\_Model целесообразно реализовать:

\begin{itemize}
    \item Расчёт метрик когерентности тематик;
    \item Упрощённый интерфейс для добавления регуляризаторов;
    \item Автоматизация преобразования данных в требуемый формат;
    \item Визуализация динамики метрик качества через графики.
\end{itemize}

Интеграция функциональности по подбору гиперпараметров в данный класс
нецелесообразно, так как это:

\begin{itemize}
    \item Нарушит принцип единственной ответственности;
    \item Усложнит поддержку кодовой базы;
    \item Снизит читаемость реализации.
\end{itemize}

Для решения этих задач предложено выделение отдельного класса
Hyperparameter\_optimizer, который:

\begin{itemize}
    \item Реализует логику оптимизации гиперпараметров;
    \item Обеспечивает удобное сохранение настроенных моделей.
\end{itemize}

Такое разделение обеспечивает модульность архитектуры и упрощает
дальнейшее расширение системы.

Следующим этапом работы является последовательная реализация обоих
классов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Преобразование новостного массива в приемлемый для BigARTM
формат}
Модель BigARTM поддерживает ограниченный набор форматов данных, включая
Vowpal Wabbit~\cite{BigARTM-docs}. Для интеграции с pandas DataFrame требуется
предварительное преобразование новостного массива, которое целесообразно
реализовать отдельной функцией.

Алгоритм преобразования:

\begin{enumerate}
    \item Извлечение строки из DataFrame;
    \item Конкатенация ячеек строки в единый текстовый блок;
    \item Запись результата в файл формата Vowpal Wabbit с меткой документа;
    \item Итеративная обработка всего массива новостей.
\end{enumerate}

Реализация функции преобразования представлена в
листинге на рис.~\ref{lst:16}~\cite{python-book,pandas}.

Последующие этапы обработки:

\begin{enumerate}
    \item Разделение данных на батчи;
    \item Генерация словаря терминов.
\end{enumerate}

Оба действия выполняются средствами библиотеки BigARTM. Соответствующий
код приведён в листинге на рис.~\ref{lst:17}~\cite{BigARTM-docs}.

Подготовленные данные готовы для передачи в модель BigARTM для тематического
моделирования.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удобное добавление регуляризаторов}
Библиотека BigARTM предоставляет обширный набор регуляризаторов,
однако их интеграция в модель требует знания непростого синтаксиса, что
затрудняет их использование. Для упрощения процесса предложен двухуровневый
подход:

\begin{enumerate}
    \item Базовая функция "--- добавляет регуляризатор по имени и значению
    гиперпараметра;
    \item Обёрточная функция "--- применяет первый метод для массового
    добавления.
\end{enumerate}

Преимущества решения:

\begin{itemize}
    \item Устранение необходимости работы с низкоуровневым API BigARTM;
    \item Единообразный интерфейс для одиночных и групповых операций;
    \item Повышение читаемости и поддерживаемости кода.
\end{itemize}

Фрагмент реализации базовой функции (листинг на рис.~\ref{lst:18}~\cite{BigARTM-docs}).

Реализация массового добавления регуляризаторов (листинг на рис.~\ref{lst:19}).

Данное решение существенно упрощает эксперименты с различными комбинациями
регуляризаторов, сохраняя при этом гибкость подхода BigARTM.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление когерентности}
Библиотека BigARTM включает набор встроенных метрик оценки качества,
однако не поддерживает расчёт когерентности "--- ключевого показателя
тематической согласованности~\cite{BigARTM}. Для восполнения этого функционала
предлагается интеграция с библиотекой Gensim, предоставляющей
методы вычисления различных видов когерентности~\cite{gensim}.

Алгоритм расчёта метрики:

\begin{enumerate}
    \item Экспорт тематических ядер:
    
    Получение списка тем, где каждая тема представлена N ключевыми терминами;

    \item Подготовка текстового корпуса:
    
    Преобразование документов в структуру вида:

    [[токен\_1\_док\_1, токен\_2\_док\_1, ...], [токен\_1\_док\_2, ...], ...];

    \item Вычисление показателя:
    
    Передача данных в Gensim для расчёта выбранного типа когерентности.
\end{enumerate}

Реализация функции представлена в листинге на рис.~\ref{lst:20}~\cite{gensim}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление тематической модели и формирование графиков метрик}
Библиотека BigARTM не поддерживает мониторинг динамики метрик качества в
процессе обучения~\cite{BigARTM-docs}, особенно для пользовательских метрик. Для реализации
этого функционала требуется дополнительная разработка.

Алгоритм отслеживания метрик:

\begin{enumerate}
    \item Итеративное обучение модели:
    \begin{itemize}
        \item Установка num\_collection\_passes=1 для пошагового
        прохода~\cite{BigARTM-docs};
        \item Циклическое выполнение обучения с накоплением метрик после
        каждой эпохи.
    \end{itemize}

    \item Визуализация результатов:
    \begin{itemize}
        \item Использование matplotlib для построения графиков;
        \item Унифицированный подход для различных типов метрик.
    \end{itemize}
\end{enumerate}

Реализация итеративного обучения представлена в
листинге на рис.~\ref{lst:21}~\cite{BigARTM-docs}.

Пример визуализации для метрики когерентности
(листинг на рис.~\ref{lst:22}~\cite{matplotlib}).

Для других метрик применяется аналогичная логика с заменой целевого
показателя.

Данная реализация завершает базовый функционал класса My\_BigARTM\_model.
Полный код доступен в приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор гиперпараметров для тематического моделирования}
Для интеллектуального подбора гиперпараметров целесообразно использовать
библиотеку Optuna, которая предоставляет~\cite{optuna}:

\begin{itemize}
    \item Упрощённый API для настройки экспериментов;
    \item Поддержку байесовской оптимизации (вместо полного перебора);
    \item Автоматическое сокращение вычислительных ресурсов за счёт
    адаптивного выбора параметров.
\end{itemize}

Алгоритм работы:

\begin{enumerate}
    \item Реализация целевой функции:
    \begin{itemize}
        \item Определение пространства поиска гиперпараметров через
        trial.suggest\_int() и trial.suggest\_float();
        \item Вычисление и возврат метрик качества модели.
    \end{itemize}

    Ключевой фрагмент реализации (листинг на рис.~\ref{lst:23}~\cite{optuna}).

    \item Запуск оптимизации:
    \begin{itemize}
        \item Использование study.optimize() для выполнения экспериментов;
        \item Получение набора попыток с параметрами и метриками.
    \end{itemize}

    \item Выбор оптимальной конфигурации:
    \begin{itemize}
        \item Нормализация метрик;
        \item Выбор попытки с минимальной совокупной ошибкой.
    \end{itemize}

    Логика выбора (листинг на рис.~\ref{lst:24}~\cite{optuna}).

    \item Финализация модели:
    \begin{itemize}
        \item Обучение на лучших гиперпараметрах;
        \item Возврат оптимизированной модели.
    \end{itemize}

    Завершающий этап (листинг на рис.~\ref{lst:25}~\cite{optuna}).
\end{enumerate}

Полная реализация класса Hyperparameter\_optimizer доступна в
приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разметка данных на основе результатов тематического
моделирования}
В данной работе тематическое моделирование используется для автоматической
тематической разметки обучающих данных. Разметка формируется на основе матрицы
$\theta$, полученной в результате моделирования~\cite{BigARTM-docs}.

Матрица $\theta$ имеет следующую структуру (рис.~\ref{fig:06}):

\begin{figure}[!ht]
	\centering
	\includegraphics[width=13cm]{./images/primer_theta.png}
	\caption{\label{fig:06}%
	Пример матрицы $\theta$}
\end{figure}

Строки матрицы соответствуют документам, столбцы "--- темам. Элементы матрицы
содержат вероятности принадлежности документов к темам.

На основе этой матрицы определяется тематическая принадлежность каждого
документа. В простейшем случае документу присваивается тема с максимальной
вероятностью. Реализация данного подхода представлена в листинге на
рис.~\ref{lst:36}.

В результате формируется размеченный набор данных, готовый для обучения
классификатора (рис.~\ref{fig:07}).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=10cm]{./images/primer_labeled_news.png}
	\caption{\label{fig:07}%
	Пример размеченных данных}
\end{figure}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Обучение модели классификатора}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор модели для тематической классификации}  
Как установлено ранее~\ref{sec:17}, для решения задачи наиболее эффективны
сети"=трансформеры. Существует три основных типа архитектур~\cite{goldberg-nlp}:  

\begin{itemize}  
    \item Encoder"=only (BERT, RoBERTa): Содержат только кодирующую
    часть;  
    \item Decoder"=only (GPT): Содержат только декодирующую часть;  
    \item Encoder"=Decoder (BART, T5): Комбинируют обе части.  
\end{itemize}  

Их функциональные различия можно описать следующим образом~\cite{goldberg-nlp}:

\begin{itemize}  
    \item Encoder модели (BERT, RoBERTa) специализируются на
    понимании текста (задачи классификации, извлечения информации);  
    \item Decoder модели (GPT) оптимизированы для задачи генерации текста;  
    \item Гибридные модели (BART, T5) предназначены для  задачи трансформации
    текста (перевод, суммаризация).  
\end{itemize}  

Для тематической классификации требуется глубокое понимание контекста,
поэтому оптимальны encoder"=only модели. Среди них RoBERTa
(Robustly optimized BERT approach) демонстрирует преимущества перед
BERT~\cite{Roberta_vs_Bert}:  

\begin{itemize}  
    \item Обучена на большем объёме данных;  
    \item Использует динамическое маскирование слов;  
    \item Исключает задачу предсказания следующего предложения;  
    \item Показывает лучшие результаты на NLU"=задачах.  
\end{itemize}  

Таким образом, для классификации новостей выберем RoBERTa. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор способа для получения предобученных моделей}
Существует несколько способо получения весов предобученной модели: от
их скачивания с облака и github репозиториев, до получения через API
разных сайтов. Из этих методов будет предпочтительнее выбрать последний,
так как есть портал Hugging Face.

Hugging Face предсталяет собой большое хранилище различных моделей,
в том числе и предобученных крупными компаниями и исследователями (Google,
Facebook, Sberbank). Кроме того, данный сайи предоставляет удобный,
лаконичный и унифицированный интерфейс для работы с ним, что позволяет
делать код максимально компактным и читабельным.

Таким образом, будет получать предобученные модели с помощью портала
Hugging Face.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Получение весов предобученной модели}
Для начала работы с нейронными сетями с платформы Hugging Face необходимо
подключить следующие зависимости, как показано в листинге на
рис.~\ref{lst:26}~\cite{hugging_face_docs}.

С помощью данных библиотек будут происходить подготовка данных, загрузка
весов моделей и их обучение.

Для загрузки модели потребуется класс AutoModelForSequenceClassification
и его метод from\_pretrained, в который будут задаваться параметры
загрузки (название модели и тип решаемой ей задачи, для загрузки предобученной
на соответствующих данных модели)~\cite{hugging_face_docs}. Реализация
соответствующего кода представлена в соответствующем листинге на
рис.~\ref{lst:27}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подготовка данных для работы с моделью}  
Для обработки текста используется токенизатор, соответствующй
выбранной модели. Его загрузка осуществляется через класс
\texttt{AutoTokenizer}~\cite{hugging_face_docs} (листинг на рис.~\ref{lst:28}).

Токенизатор преобразует сырой текст в формат, пригодный для нейросети.
Обработка данных выполняется через метод \texttt{map} класса
\texttt{Dataset} с применением функции токенизации
(листинг на рис.~\ref{lst:29})~\cite{hugging_face_docs}.

Отдельно преобразуются текстовые метки классов в числовые индексы
(листинг на рис.~\ref{lst:30})~\cite{hugging_face_docs}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Дообучение модели}
Выбранная модель (RoBERTa) не является сверхбольшой, а ресурсы
Google Colab предоставляют доступ к мощным GPU (Tesla T4/V100), что
позволяет дообучить всю архитектуру без заморозки слоёв.  

Перед обучением нужно сначала задать его параметры, реализуется
это с помощью класса \texttt{TrainingArguments}, в конструктор которого
передаются соответствующие параметры~\cite{hugging_face_docs}. Среди них можно
выделить следующие~\cite{hugging_face_docs}:

\begin{itemize}
    \item Стратегия обучения (eval\_strategy);
    \item Стратегия сохранения результата (save\_strategy);
    \item Шаг ошибки (learning\_rate);
    \item Размер батча (per\_device\_train\_batch\_size, per\_device\_eval\_batch\_size);
    \item Количество эпох обучения (num\_train\_epochs);
    \item Метрика качества подбора лучшей модели (metric\_for\_best\_model).
\end{itemize}

Соответствующий код можно увидеть в следующем листинге на рис.~\ref{lst:31}.

Осталось только создать объект тренировщика и запустить его. Делается
это с помощью класса Trainer следующим образом~\ref{lst:32}.

Таким образом, была реализована основная функциональнось для обучения
тематического классификатора. Полный код можно увидеть в
соответствующем приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Итоги по реализации инструментов автоматической тематической
классификации}
На данном этапе разработан полный комплект программных компонентов, необходимых
для реализации описанного алгоритма автоматической тематической классификации.

Перечислим основные реализованные компоненты:

\begin{enumerate}
    \item Класс для сбора данных с новостного сайта ВШЭ;
    \item Класс для предобработки текстовых данных;
    \item Класс для анализа результатов предобработки (реализация не
    детализирована в работе, но включена в состав (код можно найти в
    приложении~\ref{sec:20}));
    \item Классы для тематического моделирования:
    \begin{enumerate}
        \item Класс для работы с библиотекой BigARTM;
        \item Класс для автоматизации настройки гиперпараметров;
    \end{enumerate}
    \item Класс для анализа результатов тематического моделирования (реализация
    не детализирована в работе, но включена в состав(код можно найти в
    приложении~\ref{sec:20}));
    \item Класс для обучения и оценки нейросетевого классификатора.
\end{enumerate}

Таким образом, создана необходимая программная основа для проведения
экспериментальной оценки эффективности предложенного подхода, которая
представлена в следующих разделах.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Результаты экспериментов по тестированию эффективности
предложенного алгоритма автоматической тематической классификации}
В этом разделе будут рассмотрены экспериментальные результаты проверки
эффективности предложенного алгоритма автоматической тематической классификации.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Результаты подготовки данных} \label{sec:14}
Набор данных был обработан с различными параметрами:

\begin{enumerate}
    \item Без фильтрации стоп-слов методом TF"=IDF;
    \item С фильтрацией стоп-слов методом TF"=IDF с порогами от 1 до 10
    процентов.
\end{enumerate}

Количественные характеристики обработанных данных представлены в
таблицах~\ref{sec:03}.

Анализ результатов показывает успешность обработки:
\begin{enumerate}
    \item Эффективное удаление неалфавитных и нерелевантных токенов(количество
    уникальных токенов снизилось с 278724 до 18707 при обработке без TF"=IDF
    фильтрации);
    \item Успешное удаление документов с недостаточным содержанием (минимальное
    количество токенов в документе увеличилось с 6 до 79).
\end{enumerate}

Эффективность удаления стоп"=слов подтверждается распределением частот токенов,
соответствующим закону Ципфа (рис.~\ref{fig:09}).

\begin{figure}[!ht]
	\centering
	\includegraphics[width=10cm]{./images/zips_law_not_processing_data.png}
    \includegraphics[width=10cm]{./images/zips_law_processing_data.png}
	\caption{\label{fig:09}%
	Распределение частот токенов: исходные данные (сверху) и обработанные
    данные (снизу)}
\end{figure}
\newpage

На графиках видно, что в обработанных данных устранены токены с экстремально
высокой и низкой частотой встречаемости, которые, как отмечалось ранее,
обладают низкой тематической различительной способностью.

Следует отметить сокращение размера набора данных с 17430 до 11860 документов,
что может ограничить возможности тематического моделирования и глубокого
обучения.

Эксперименты проводились с использованием кода, который можно найти в
приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Результаты тематического моделирования} \label{sec:12}
В ходе исследования проведено тематическое моделирование для 11 (помимо таблиц
ещё 1) конфигураций предобработанных данных. Для каждой конфигурации выполнены:

\begin{enumerate}
    \item Оптимизация гиперпараметров;
    \item Построение финальной модели;
    \item Оценка метрик качества.
\end{enumerate}

Результаты оценки представлены в таблице~\ref{tab:metrics} (перплексия и
когерентность) и таблице~\ref{tab:hyperparams} (оптимальные гиперпараметры).

\newlength{\mydatalength}
\setlength{\mydatalength}{6cm}

\begin{longtable}{|p{\mydatalength}|c|c|}
  \caption{Метрики моделей} \label{tab:metrics} \\
  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline 
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без TF"=IDF.  & 3299 & 0.413 \\
  \hline
  С tfidf 1 пр. & 2881 & 0.511 \\
  \hline
  С tfidf 2 пр. & 2972 & 0.518 \\
  \hline
  С tfidf 3 пр. & 2998 & 0.525 \\
  \hline
  С tfidf 4 пр. & 3478 & 0.469 \\
  \hline
  С tfidf 5 пр. & 3374 & 0.494 \\
  \hline
  С tfidf 6 пр. & 3364 & 0.495 \\
  \hline
  С tfidf 7 пр. & 3158 & 0.501 \\
  \hline
  С tfidf 8 пр. & 3391 & 0.509 \\
  \hline
  С tfidf 9 пр. & 3208 & 0.535 \\
  \hline
  С tfidf 10 пр.& 3144 & 0.537 \\
  \hline
\end{longtable}

\begin{longtable}{|p{\mydatalength}|c|c|c|}
  \caption{Гиперпараметры моделей} \label{tab:hyperparams} \\
  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} \\
  \hline
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без TF"=IDF.  & 7 & 5 & 5 \\
  \hline
  С tfidf 1 пр. & 8 & 6 & 6 \\
  \hline
  С tfidf 2 пр. & 8 & 6 & 7 \\
  \hline
  С tfidf 3 пр. & 8 & 6 & 7 \\
  \hline
  С tfidf 4 пр. & 7 & 3 & 7 \\
  \hline
  С tfidf 5 пр. & 6 & 4 & 7 \\
  \hline
  С tfidf 6 пр. & 7 & 4 & 7 \\
  \hline
  С tfidf 7 пр. & 7 & 6 & 7 \\
  \hline
  С tfidf 8 пр. & 8 & 7 & 5 \\
  \hline
  С tfidf 9 пр. & 8 & 7 & 6 \\
  \hline
  С tfidf 10 пр.& 8 & 6 & 7 \\
  \hline
\end{longtable}

В таблицах представлены результаты LDA"=моделирования без регуляризаторов.
Анализ матриц пересечения тем показал их избыточное перекрытие, что видно на
рисунке~\ref{fig:10}.

\begin{figure}[!ht]
\centering
\includegraphics[width=13cm]{./images/peresech.png}
\caption{\label{fig:10}%
Распределение тем по документам (моделирование без TF"=IDF фильтрации)}
\end{figure}

Для улучшения результатов была выбрана модель с TF"=IDF фильтрацией
(порог 1 процент) и пересчитана с регуляризаторами декорреляции матриц $\phi$ и
$\theta$. Полученные значения метрик: перплексия 2810, когерентность 0.501.
Однако распределение тем существенно не изменилось.

\begin{figure}[!ht]
\centering
\includegraphics[width=13cm]{./images/peresech_reg.png}
\caption{\label{fig:11}%
Распределение тем по документам (с регуляризаторами декорреляции)}
\end{figure}

Это свидетельствует, что регуляризаторы слабо влияют на уже вычисленные модели и
могут рассматриваться лишь как инструмент калибровки.

Основные наблюдения:

\begin{itemize}
    \item Качество моделей на разных наборах данных сопоставимо. Оптимальная
    перплексия достигнута при TF"=IDF фильтрации с порогом 1 процент,
    максимальная когерентность "--- при пороге 10 процентов. Это указывает на
    слабое влияние TF"=IDF фильтрации;
    
    \item Все модели демонстрируют высокое пересечение тем
    (рис.~\ref{fig:10},~\ref{fig:11}), вероятно из"=за позднего применения
    регуляризаторов;
    
    \item Отклонение от эталонного распределения тем сайта ВШЭ составляет 
    $\geq$ 84 процента, что указывает на ограничения метода;
    
    \item Возможные причины:
    \begin{itemize}
        \item Ограниченный перебор гиперпараметров;
        \item Позднее применение регуляризаторов;
        \item Недостаточный объём данных для чёткого разделения тем.
    \end{itemize}
\end{itemize}

Возможные пути улучшения:

\begin{itemize}
    \item Расширение пространства гиперпараметров;
    \item Комбинированные стратегии предобработки;
    \item Эксперименты с регуляризаторами на всех этапах.
\end{itemize}

Эксперименты проводились с использованием кода, который можно найти в
приложении~\ref{sec:20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Результаты обучения классификатора} \label{sec:11}
Эксперименты по обучению классификатора на основе тематического моделирования
показали низкую эффективность (таблица~\ref{tab:class_train}):

\begin{longtable}{|p{\mydatalength}|c|c|}
  \caption{Метрики моделей} \label{tab:class_train} \\
  \hline
  \textbf{Данные} & \textbf{Accuracy} & \textbf{F1} \\
  \hline 
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{Accuracy} & \textbf{F1} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без TF"=IDF.  & 0.291 & 0.252 \\
  \hline
  С tfidf 1 пр. & 0.191 & 0.095 \\
  \hline
  С tfidf 1 пр. рег. & 0.180 & 0.042 \\
  \hline
  С tfidf 2 пр. & 0.183 & 0.065 \\
  \hline
  С tfidf 3 пр. & 0.178 & 0.037 \\
  \hline
  С tfidf 4 пр. & 0.198 & 0.047 \\
  \hline
  С tfidf 5 пр. & 0.235 & 0.119 \\
  \hline
  С tfidf 6 пр. & 0.196 & 0.081 \\
  \hline
  С tfidf 7 пр. & 0.193 & 0.085 \\
  \hline
  С tfidf 8 пр. & 0.166 & 0.035 \\
  \hline
  С tfidf 9 пр. & 0.179 & 0.038 \\
  \hline
  С tfidf 10 пр.& 0.201 & 0.109 \\
  \hline
\end{longtable}

Для улучшения результатов были предприняты следующие меры:

\begin{enumerate}
    \item Сокращение словаря до 5000 наиболее значимых слов (по матрице
    $\phi$);
    \item Использование биграмм;
    \item Применение альтернативных моделей (FastText, полносвязные нейронные
    сети).
\end{enumerate}

Ни один из методов не привел к улучшению качества. Сокращение словаря не дало
положительного эффекта, а использование биграмм снизило значения accuracy и
F1"=меры. Альтернативные модели (FastText и полносвязные сети) также не
показали значимого улучшения.

Основная гипотеза заключается в некорректности тематических меток. Для проверки
была использована оригинальная разметка сайта ВШЭ, что дало следующие
результаты:

\begin{itemize}
    \item Точность на первой эпохе: $\text{Accuracy} = 0.60$;
    \item Максимальная достигнутая точность: $\text{Accuracy} = 0.71$;
    \item Подтверждение: низкое качество связано с ошибками тематического
    моделирования.
\end{itemize}

Таким образом, ключевая проблема заключается в некорректном тематическом
распределении документов, что подтверждается:

\begin{enumerate}
    \item Низкими метриками при использовании разметки BigARTM и
    высокими "--- при использовании разметки ВШЭ;
    \item Сопоставимыми объемами данных в обоих случаях (количество документов
    и токенов).
\end{enumerate}

Эксперименты проводились с использованием кода, который можно найти по в
приложении~\ref{sec:20}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Выводы и возможные улучшения по практико-методической части}
Исходя из разделов~\ref{sec:11},~\ref{sec:12},~\ref{sec:14} и ~\ref{sec:21}
узким местом выбранного подхода автоматической классификации новостей является
этап тематического моделирования.  

Для решения этой проблемы предлагаются следующие методы:  

\begin{enumerate}  
    \item Улучшение подготовки данных;  
    \item Расширенная настройка гиперпараметров и регуляризаторов.  
\end{enumerate}  

Однако оба подхода имеют ограничения:

\begin{itemize}  
    \item Подготовка данных уже включает стандартные методы
    (кроме продвинутой коррекции опечаток), что снижает потенциал улучшений;  
    \item Библиотека BigARTM не поддерживает GPU"=ускорение, что делает
    широкий поиск гиперпараметров вычислительно неэффективным.  
\end{itemize}  

Возможные улучшения классификатора:

\begin{itemize}  
    \item Автоматический подбор гиперпараметров (например, через Optuna);  
    \item Тестирование альтернативных моделей (CTM, BERTopic).  
\end{itemize}  

Перспективы развития работы, если будет решена проблема с тематическим
моделированием:

\begin{enumerate}  
    \item Рефакторинг кода: повышение модульности и читаемости классов;  
    \item Создание API для интеграции классификатора в приложения;  
    \item Разработка веб"=интерфейса для пользовательской классификации.  
\end{enumerate}  

% Раздел "Заключение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conclusion
В ходе данной дипломной работы был разработан алгоритм автоматической
классификации новостей на основе тематической модели предметной области.

Для этого было выполнено следующее:

\begin{enumerate}
    \item Проведён анализ инструментов по сбору данных и выбраны
    наиболее удобные из них (BeautifulSoup4, requests);
    \item Проведён сбор данных;
    \item Проанализированы способы обработки текстовых данных
    и выбранны наиболее удобные из них;
    \item Проанализированы популярные инструменты для обработки
    текстовых данных (NLTK, Pymorphy3, SpaCy) и выбран наиболее
    удобный и точный из них (SpaCy);
    \item Проведена подготовка данных для тематического моделирования и
    проведён анализ её результатов;
    \item Изучен механизм тематического моделирования с помошью
    аддитивной тематической регуляризации;
    \item Разработаны инструменты для тематической классификации с
    помощью библиотеки BigARTM;
    \item Проведены эксперименты по проведению тематической классификации
    над подготовленными различными способами данными, а также
    проведён анализ результатов экспериментов;
    \item Рассмотрены различные способы обработки текстовых данных нейронными
    сетями и выбран наиболее подходящий из них (семантическое векторное
    представление);
    \item Проведён анализ архитектур подходящих типов нейронных сетей
    и выбрана наиболее подходящая из них (transformer);
    \item Проведён анализ доступных предобученных сетей и сервисов, которые
    их предоставляют, в ходе которого выбран наиболее удобный из них (Hugging
    Face и Roberta);
    \item Проведены эксперименты по обучению тематического классификатора
    новостей, а также выполнен анализ результатов и сделаны соответствующие
    выводы.
\end{enumerate}

Основной вывод по итогам работы: предложенный метод автоматической классификации
имеет перспективу применения при более тщательном тематическом моделировании
исходного набора данных.

Таким образом, все поставленные задачи работы были решены, а следовательно цель
достигнута.

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Пример страницы новостного сайта ВШЭ} \label{sec:15}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/primer_hse.png}
	\caption{\label{fig:05}%
	Пример страницы новостного сайта ВШЭ}
\end{figure}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Листинги посвящённые реализации веб"=скраппера}
\lstinputlisting[
caption={Функция получения HTML"=кода страницы},
label={lst:01}
]{code/parcing/get_page_function.py}

\lstinputlisting[
caption={Извлечение ссылок и кратких описаний},
label={lst:02}
]{code/parcing/get_link_and_summary.py}

\lstinputlisting[
caption={Функция извлечения полного текста новости},
label={lst:03}
]{code/parcing/get_news_content.py}

\lstinputlisting[
caption={Обработка новостной страницы},
label={lst:04}
]{code/parcing/get_one_news_page.py}

\lstinputlisting[
caption={Функция обработки всего архива новостей},
label={lst:05}
]{code/parcing/crawling_pages_function.py}

\lstinputlisting[
caption={Многопоточная реализация парсера},
label={lst:06}
]{code/parcing/multithreading_apply_function.py}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Листинги посвящённые реализации обработчика данных}
\lstinputlisting[
caption={Функция нормализации пробелов и переносов строк},
label={lst:07}
]{code/prepeare_text/remove_extra_spaces_and_line_breaks.py}

\lstinputlisting[
caption={Функция разделения текста на русско- и англоязычные фрагменты},
label={lst:08}
]{code/prepeare_text/split_into_en_and_ru.py}

\lstinputlisting[
caption={Реализация удаления неалфавитных токенов},
label={lst:09}
]{code/prepeare_text/remove_non_alpha_tokens.py}

\lstinputlisting[
caption={Обработка строки русского языка средствами SpaCy},
label={lst:10}
]{code/prepeare_text/apply_spacy_for_one_str.py}

\lstinputlisting[
caption={Комплексная обработка текста: нормализация, токенизация,
лемматизация, фильтрация стоп"=слов по словарю},
label={lst:11}
]{code/prepeare_text/tokenize_lemmatize_and_del_stop_words.py}

\lstinputlisting[
caption={Удаление токенов с экстремальной частотой встречаемости в документах},
label={lst:34}
]{code/prepeare_text/del_high_down_docs_words.py}

\lstinputlisting[
caption={Вычисление TF"=IDF метрик для текстового корпуса},
label={lst:12}
]{code/prepeare_text/calc_dict_corpus_and_tfidf.py}

\lstinputlisting[
caption={Дополнение словаря токенами с нулевыми TF"=IDF значениями},
label={lst:13}
]{code/prepeare_text/add_missing_tfidf_words.py}

\lstinputlisting[
caption={Определение порогового значения TF"=IDF},
label={lst:14}
]{code/prepeare_text/calc_tfidf_threshold.py}

\lstinputlisting[
caption={Удаление стоп"=слов на основе TF"=IDF метрики},
label={lst:15}
]{code/prepeare_text/del_tfidf_stop_words.py}

\lstinputlisting[
caption={Удаление документов с недостаточным количеством токенов},
label={lst:35}
]{code/prepeare_text/del_void_docs.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Листинги посвящённые реализации классов для тематического
моделирования}
\lstinputlisting[
caption={Преобразование новостного массива в формат Vowpal Wabbit},
label={lst:16}
]{code/calc_topic_model/make_vowpal_wabbit.py}

\lstinputlisting[
caption={Функция создания батчей и словаря},
label={lst:17}
]{code/calc_topic_model/make_batches.py}

\lstinputlisting[
caption={Функция добавления одиночного регуляризатора},
label={lst:18}
]{code/calc_topic_model/add_regularizer.py}

\lstinputlisting[
caption={Функция добавления набора регуляризаторов},
label={lst:19}
]{code/calc_topic_model/add_regularizers.py}

\lstinputlisting[
caption={Функция вычисления метрики когерентности},
label={lst:20}
]{code/calc_topic_model/calc_coherence.py}

\lstinputlisting[
caption={Функция вычисления тематической модели с пошаговым рассчётом метрик},
label={lst:21}
]{code/calc_topic_model/calc_model.py}

\lstinputlisting[
caption={Функция построения графика динамики когерентности},
label={lst:22}
]{code/calc_topic_model/calc_coherence_graphic.py}

\lstinputlisting[
caption={Целевая функция для оптимизации гиперпараметров},
label={lst:23}
]{code/calc_topic_model/objective.py}

\lstinputlisting[
caption={Функция выбора оптимальной конфигурации},
label={lst:24}
]{code/calc_topic_model/select_best_trial.py}

\lstinputlisting[
caption={Обучение модели с оптимальными параметрами},
label={lst:25}
]{code/calc_topic_model/optimizer.py}

\lstinputlisting[
caption={Получение размеченных данных},
label={lst:36}
]{code/calc_topic_model/calc_labeled_news.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Листинги посвящённые реализации обучения нейронной
сети"=классификатора}
\lstinputlisting[
caption={Подключение необходимых зависимсотей для работы с Hugging Face},
label={lst:26}
]{code/calc_classificator/hugging_face_dependicies.py}

\lstinputlisting[
caption={Загрузка весов модели},
label={lst:27}
]{code/calc_classificator/load_model.py}

\lstinputlisting[
caption={Загрузка предобученного токенизатора},
label={lst:28}
]{code/calc_classificator/load_tokenizer.py}

\lstinputlisting[
caption={Функция токенизации текста},
label={lst:29}
]{code/calc_classificator/tokenize_data.py}

\lstinputlisting[
caption={Кодировка меток классов},
label={lst:30}
]{code/calc_classificator/prepeare_data.py}

\lstinputlisting[
caption={Код установки параметров обучения},
label={lst:31}
]{code/calc_classificator/train_args.py}

\lstinputlisting[
caption={Код класса обучения},
label={lst:32}
]{code/calc_classificator/trainer.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Количественные характеристики подготовленного и неподготовленного
новостного массива} \label{sec:03}

\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Без TF"=IDF ф.}} & 
\rotatebox{90}{\textbf{TF"=IDF 1\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 2\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 3\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 4\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Без TF"=IDF ф.}} & 
\rotatebox{90}{\textbf{TF"=IDF 1\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 2\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 3\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 4\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 11860 & 11860 & 11860 & 11860 & 11860 \\
\hline
Кол. токенов & 12131111 & 5233704 & 5181364 & 5129026 & 5076687 & 5024348 \\
\hline
Кол. уник. ток. & 278724 & 18707 & 18707 & 18707 & 18707 & 18707 \\
\hline
Мин. кол. ток. в док. & 6 & 79 & 79 & 79 & 79 & 79 \\
\hline
Модальное кол. ток. в док. & 47 & 130 & 130 & 130 & 461 & 277 \\
\hline
Медианное кол. ток. в док. & - & 389 & 388 & 385 & 382 & 379 \\
\hline
Среднее кол. ток. в док. & 695 & 441 & 436 & 432 & 428 & 423 \\
\hline
Макс. кол. ток. в док. & 6514 & 2556 & 2407 & 2318 & 2243 & 2185 \\
\hline
Мин. кол. уник. ток. в док. & 6 & 27 & 27 & 27 & 27 & 27 \\
\hline
Мод. кол. уник. ток. в док. & 39 & 187 & 187 & 141 & 187 & 208 \\
\hline
Мед. кол. уник. ток. в док. & - & 237 & 236 & 233 & 230 & 227 \\
\hline
Сред. кол. уник. ток. в док. & 346 & 259 & 255 & 251 & 246 & 242 \\
\hline
Макс. кол. уник. ток. в док. & 2287 & 1183 & 1151 & 1113 & 1079 & 1040 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 4\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 5\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 6\%.}} & 
\rotatebox{90}{\textbf{TF"=IDF 7\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 8\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 9\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 5\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 6\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 7\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 8\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 9\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 10\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 11860 & 11860 & 11860 & 11860 & 11860 & 11860 \\
\hline
Кол. токенов & 4972009 & 4919670 & 4876331 & 4814992 & 4762654 & 4710315 \\
\hline
Кол. уник. ток. & 18707 & 18707 & 18707 & 18707 & 18707 & 18707 \\
\hline
Мин. кол. ток. в док. & 79 & 79 & 79 & 79 & 79 & 79 \\
\hline
Модальное кол. ток. в док. & 359 & 167 & 355 & 372 & 282 & 186 \\
\hline
Среднее кол. ток. в док. & 377 & 373 & 371 & 368 & 364 & 361 \\
\hline
Медианное кол. ток. в док. & 419 & 414 & 410 & 405 & 401 & 397 \\
\hline
Макс. кол. ток. в док. & 2107 & 2053 & 2001 & 1955 & 1912 & 1877 \\
\hline
Мин. кол. уник. ток. в док. & 27 & 27 & 27 & 27 & 27 & 27 \\
\hline
Мод. кол. уник. ток. в док. & 184 & 216 & 183 & 208 & 224 & 138 \\
\hline
Сред. кол. уник. ток. в док. & 224 & 221 & 218 & 215 & 212 & 208 \\
\hline
Мед. кол. уник. ток. в док. & 238 & 234 & 231 & 227 & 223 & 219 \\
\hline
Макс. кол. уник. ток. в док. & 991 & 957 & 925 & 891 & 856 & 832 \\
\hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ссылка на исходные материалы работы} \label{sec:20}
Полные материалы работы доступны:

\begin{itemize}
    \item Исходный код реализации;
    \item Результаты экспериментальных вычислений;
    \item Обработанные наборы данных (включая размеченные данные);
    \item Построенные тематические модели.
\end{itemize}

в облачном хранилище \href{https://drive.google.com/drive/folders/1-nN0fz84nv42JtxSS0sVl5WbhbwCX8xl?usp=sharing}{Google Drive}.


\end{document}
