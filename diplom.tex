\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{longtable}  % Для многостраничных таблиц
\usepackage{array}      % Для дополнительного форматирования столбцов
\usepackage{booktabs}   % Для улучшенного оформления линий

\usepackage{xltabular}  % Добавьте в преамбулу
\usepackage{booktabs}   % Для улучшенного стиля линий
\usepackage{array}      % Для управления выравниванием

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
В настоящее время обработка больших объёмов текстовых данных, включа
новостные потоки, становится критически важной задачей. Как в научной
среде, так и в бизнесе требуется оперативно анализировать информацию,
отслеживать тенденции и принимать решения. Однако анализ всего массива
данных невозможен из"=за его масштабов. Необходимо фильтровать информацию,
оставляя только релевантную.

Решением этой проблемы может стать тематическая классификация. Хотя многие
сайты и порталы предлагают рубрикацию контента, её точность часто оказывается
низкой: теги присваиваются некорректно или поверхностно. Это приводит к
ошибкам в поиске и анализе информации.

Для устранения этих недостатков необходим механизм, обеспечивающий точную
тематическую классификацию данных с возможностью автоматической разметки
новостей. Одним из инструментов для реализации такого подхода являются
тематические модели в сочетании с алгоритмами машинного и глубокого обучения.
Первые позволяют выявить скрытые темы в текстовых данных и подготовить
разметку для обучения вторых. Алгоритмы машинного и глубокого обучения, в
свою очередь, могут классифицировать новые тексты по заданным темам.

Таким образом, целью данной работы является создание механизма автоматической
тематической классификации новостей с использованием методов тематического
моделирования, машинного и глубокого обучения.

Для достижения цели необходимо решить следующие задачи:
\begin{enumerate}
    \item Реализовать сбор новостных данных;
    \item Разработать механизм предобработки текстовых данных;
    \item Вычислить количественные характеристи данных и провести их анализ;
    \item Построить тематические модели;
    \item Выбрать оптимальную тематическую модель с помощью сравнительного
    анализа;
    \item Подготовить размеченные данные для обучения моделей;
    \item Обучить и сравнить эффективность различных моделей машинного и
    глубокого обучения;
    \item Провести анализ полученных результатов.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Теоретические и методологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Сбор новостных данных данных}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор метода получения новостных данных}
Для получения данных с сайтов существует три основных метода:
\begin{itemize}
\item Ручной сбор "--- извлечение информации человеком вручную;
\item Запрос данных "--- получение информации от владельцев с последующим
скачиванием;
\item Программный сбор "--- автоматизированное извлечение данных.
\end{itemize}

Первый метод можно исключить из рассмотрения из"=за низкой эффективности.
Второй метод применим не во всех случаях: владельцы информационных платформ
вряд ли будут оперативно предоставлять данные по каждому запросу. Таким
образом, наиболее целесообразным остаётся третий метод "--- программный сбор.

Среди методов программного сбора оперативно и эффективно получать данные
в большинстве случаев позволяют инструменты веб"=скрапинга, который мы
выбираем в качестве основного подхода. Далее в работе будет использован
именно этот метод для формирования новостного массива, так как он
прост в изученни, а также обеспечивает баланс между скоростью получения
данных и минимальными требованиями к стороннему участию.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор новостной платформы для сбора данных}
В рамках данной работы основным объектом исследования являются новостные
текстовые данные. Для их сбора необходимо выбрать подходящий веб"=ресурс.

При наличии нескольких потенциальных источников выбор следует осуществлять
по следующим критериям:
\begin{enumerate}
    \item Единая структура документов на всём сайте;
    \item Отсутствие блокировок HTTP"=запросов от скраперов;
    \item Статичность контента "--- полная доступность HTML"=кода страницы при
    первичном запросе без динамической подгрузки.
\end{enumerate}

Идеальный случай "--- соответствие всем трём пунктам. При этом:

\begin{enumerate}
    \item Ограничения по пунктам 2 и 3 в большинстве случаев можно обойти
    стандартными методами;
    \item Нарушение пункта 1 создаёт принципиальные сложности: обработка
    разноформатных данных может потребовать ручной настройки для каждого
    документа.
\end{enumerate}

В качестве источника выбран новостной сайт НИУ ВШЭ. Этот ресурс:

\begin{enumerate}
    \item Имеет единую структуру новостных материалов;
    \item Не блокирует автоматизированные запросы;
    \item Предоставляет полный HTML"=код страницы без динамической генерации
    контента.
\end{enumerate}

Указанные характеристики делают сайт ВШЭ оптимальным вариантом для реализации
поставленных задач.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка собранных данных}  
Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы предобработки включают:

\begin{enumerate}
    \item \textbf{Очистка от технического шума:}
    \begin{itemize}
        \item Удаление лишних пробелов и переносов строк;
        \item Очистка от специальных символов (скобки, HTML"=теги, эмодзи);
        \item Нормализация регистра (приведение текста к нижнему регистру).
    \end{itemize}

    \item \textbf{Токенизация:} разделение текста на семантические единицы
    (слова, предложения);  

    \item \textbf{Лемматизация:} приведение словоформ к лемме (словарной форме);  

    \item \textbf{Удаление стоп"=слов:} исключение частотных слов с низкой
    смысловой нагрузкой (предлоги, союзы, частицы);  
\end{enumerate}

\textbf{Обоснование выбора лемматизации:}
В отличие от стемминга (например, алгоритм Snowball), который применяет
шаблонное усечение окончаний, лемматизация обеспечивает точное приведение
слов к нормальной форме с сохранением семантики. Это критически важно для
тематического моделирования, где искажение смысла слов может привести к
некорректной интерпретации контекста. На рис.~\ref{fig:03} показаны
принципиальные различия между двумя подходами.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/different_stem_and_lem.png}
	\caption{\label{fig:03}%
	Иллюстрация разницы между стеммингом и лемматизацией}
\end{figure}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов}  
Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов. Определение стоп"=слов в обоих случаях происходит по
заранее заданным словарям, поэтому разницы здесь нет. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает её использование.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по
предопределённому списку, который не является исчерпывающим. Это связано с
тем, что набор стоп"=слов зависит от тематики текста, и универсального
решения не существует. Для дополнительной фильтрации применим метрику
TF"=IDF, которая оценивает значимость слов. Формула расчёта:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа $d$. Её
расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп"=слова) и Gensim (расширенная
фильтрация через TF"=IDF).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных,
который позволяет выявить семантические структуры в коллекциях документов.

Основная идея тематического моделирования заключается в том, что
слова в тексте связаны не с конкретным документом, а с темами. Сначала текст
разбивается на темы, и каждая из них генерирует слова для соответствующих
позиций в документе. Таким образом, сначала формируется тема, а затем тема
формирует терм.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и взаимовстречаемости слов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Ниже приведены
основные количественные характеристики, использующиеся при тематическом
моделировании:

\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и
аксиомы:

\begin{itemize}
    \item независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом:

\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируется
    терм $p(w|d, t)$.
\end{enumerate}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_1.png}
	\caption{\label{fig:2}%
	Алгоритм формирования документа}
\end{figure}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:

\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного
разложения~\ref{fig:1}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_2.png}
	\caption{\label{fig:1}%
	Стохастическое матричное разложение}
\end{figure}

Таким образом, тематическое моделирование ищет величину $p(w|d)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Решение обратной задачи}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Лемма о максимизации функции на единичных симплексах:} \label{sec:06}
Перед тем как перейти к решению обратной задачи, сформулируем лемму,
которая поможет в этом процессе.

Введём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}(x_i) =
    \frac{\max{x_i, 0}}{\sum_{k \in I} \max{x_k, 0}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных симплексах:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Сведение обратной задачи к максимизации функционала:}
Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия, согласно которому будут подобраны параметры
$\Phi, \Theta$ такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} = n_{dw} \to max
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation} \label{eq:02}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation} \label{eq:03}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
	\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

Таким образом, обратная задача сводится к задаче максимизации
функционала.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Аддитивная регуляризация тематических моделей:}
Задача~\ref{eq:02} не соответствует критериям корректно поставленной задаче
по Адамару, поскольку в общем случае она имеет бесконечное множество решений.
Это свидетельствует о необходимости доопределения задачи. 

Для доопределения некорректно поставленных задач применяется регуляризация: к
основному критерию добавляется дополнительный критерий "--- регуляризатор,
который соответствует специфике решаемой задачи. 

Метод ARTM (аддитивная регуляризация тематических моделей)
основывается на максимизации линейной комбинации логарифма правдоподобия и
регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами
регуляризации $\tau_i, \;\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки~\ref{eq:03}.

Регуляризатор (или набор регуляризаторов) выбирается в соответствии с решаемой
задачей.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{E"=M алгоритм:}
Из представленных выше ограничений~\ref{eq:03} следует, что столбцы матриц можно
считать неотрицательными единичными векторами. Таким образом, задача сводится к
максимизации функции на единичных симплексах.

Воспользуемся леммой о максимизации функции на единичных
симплексах~\ref{sec:06} и перепишем задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений с вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation} \label{eq:04}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E"=шагу, а вторая и третья строки "--- M"=шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные варианты регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Дивергенция Кульбака"=Лейблера:} \label{sec:07}
Перед тем как перейти к регуляризаторам необходимо ввести меру оценки близости
тем.

Чтобы оценить близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера (KL или KL"=дивергенция). KL"=дивергенция
позволяет оценить степень вложенности одного распределения в другое, в случае
тематического моделирования будет оценитьваться вложенность матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$ \label{it:kl3}
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

Теперь можно перейти к рассмотрению регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор сглаживания:}
Сглаживание предполагает сематническое сближение тем, это может быть полезно в
следующих случаях:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:05}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм соответствующий модели
LDA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор разреживания:}
Разреживание подразумевает разделение тем и документов, исключая общие слова из
них. Этот тип регуляризации основывается на предположении, что темы и документы
в основном являются специфичными и описываются относительно небольшим набором
терминов, которые не встречаются в других темах.

Определим регуялризатор разреживания:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:06}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, разреживающий
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор декоррелирования тем:}
Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем:

Определим регуляризатор декоррелирования:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:07}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{t \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, декоррелирующий
темы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Оценка качества моделей}
После построения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических
моделей:

\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item полнота и точность тематического поиска;
        \item качество ранжирования при тематическом поиске;
        \item качество классификации / категоризации документов;
        \item качество суммаризации / сегментации документов;
        \item экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item правдоподобие и перплексия;
        \item средняя когерентность (согласованность тем);
        \item разреженность матриц $\Phi$ и $\Theta$;
        \item различность тем;
        \item статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Поскольку оценка по внешним критериям невозможна в рамках данной работы,
сосредоточимся на внутренних критериях оценки, которые можно вычислять
автоматически.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Правдоподобие и перплексия:}
Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что при равномерном распределении слов в тексте выполняется
равенство $p(w|d) = \frac{1}{|W|}$. В этом случае значение перплексии равно
мощности словаря $P = |W|$. Это позволяет сделать вывод, что перплексия является
мерой разнообразия и неопределенности слов в тексте: чем меньше значение
перплексии, тем более разнообразны вероятности появления слов.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Когерентность:}
Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PNI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$, $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация,
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно), $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости,
тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Разреженность:}
Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и
$\Theta$.

Разреженность играет ключевую роль в выявлении различий между темами.
Каждая тема формируется на основе ограниченного набора слов, в то время как
остальные слова должны встречаться реже, что отражается в нулевых элементах
матриц. Оптимальный уровень разреженности должен быть высоким, но не чрезмерным:
в таком случае темы будут четко различимы. Если разреженность слишком низка,
темы могут сливаться, а если слишком высока "--- содержать недостаточное
количество слов для адекватного
представления.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Чистота темы:}
Чистота темы:
\begin{equation}
	\sum_{w \in W_t} p(w|t),
\end{equation}

где $W_t$ "--- ядро темы:
$W_t = \{w: p(w|t) > \alpha\}$, где $\alpha$ подбирается по разному,
например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$.

Данная характеристика показывает как вероятностно относится ядро темы к фоновым
словам темы, следовательно, чем больше вероятность ядра, тем
лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Контрастность темы:}
Контрастность темы:
\begin{equation}
	\frac{1}{|W_T|} \sum_{w \in W_t} p(t|w).
\end{equation}

Данная характеристика показывает насколько часто слова из ядра темы
встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
в других темах, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Практико"=технологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Получение новостного массива путём веб"=скраппинга}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов получения новостных данных}
Для веб"=скрапинга доступны библиотеки на разных языках, однако выбор логично
сделать в пользу Python "--- наиболее популярного языка для обработки данных
и работы с машинным обучением. Среди Python"=библиотек ключевыми являются:

\begin{itemize}
    \item requests "--- для отправки HTTP"=запросов;
    \item BeautifulSoup4 "--- для парсинга HTML"=кода в удобную объектную
    структуру;
    \item selenium "--- для работы с динамическими сайтами, где контент
    генерируется JavaScript.
\end{itemize}

Первые две библиотеки эффективны для статических страниц: requests получает
исходный код, а BeautifulSoup4 извлекает данные через поиск по тегам.
Selenium же имитирует взаимодействие реального браузера, что позволяет
обрабатывать страницы с отложенной загрузкой контента.

Этот набор инструментов покрывает потребности работы с подавляющим
большинством сайтов "--- от простых статических ресурсов до сложных
веб"=приложений.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Реализация алгоритма сбора новостных данных}
библиотек requests и BeautifulSoup4 без привлечения Selenium.

Алгоритм сбора данных включает следующие этапы:

\begin{enumerate}
    \item Анализ структуры сайта:
    \begin{itemize}
        \item Многостраничный ресурс с 10 новостными карточками на каждой
        странице;
        \item Карточка новости содержит: ссылку, дату публикации, заголовок,
        краткое содержание;
        \item Полный текст доступен по отдельной ссылке внутри карточки.
    \end{itemize}

    \item Реализация базовых функций (листинг~\ref{lst:01}):
    \begin{itemize}
        \item Получение HTML"=кода страницы через requests.get();
        \item Сохранение сырых данных для последующей обработки.
    \end{itemize}

    \lstinputlisting[
    caption={Функция получения HTML"=кода страницы},
    label={lst:01}
    ]{code/get_page_function.py}

    \item Извлечение метаданных (листинг~\ref{lst:02}):
    \begin{itemize}
        \item Парсинг сохранённого HTML через BeautifulSoup4;
        \item Поиск элементов по тегам и CSS"=классам (find(), find\_all());
        \item Извлечение текстового содержимого (text, get()).
    \end{itemize}

    \lstinputlisting[
    caption={Извлечение ссылок и кратких описаний},
    label={lst:02}
    ]{code/get_link_and_summary.py}

    \item Получение полного текста новости (листинг~\ref{lst:03}):
    \begin{itemize}
        \item Рекурсивное использование get\_page() для целевых URL;
        \item Анализ структуры контентной страницы.
    \end{itemize}

    \lstinputlisting[
    caption={Функция извлечения полного текста новости},
    label={lst:03}
    ]{code/get_news_content.py}

    \item Обработка страницы целиком (листинг~\ref{lst:04}):
    \begin{itemize}
        \item Итерация по 10 элементам div.post на странице;
        \item Использование find\_next\_sibling() для навигации;
        \item Сохранение результатов в pandas DataFrame для анализа.
    \end{itemize}

    \lstinputlisting[
    caption={Обработка новостной страницы},
    label={lst:04}
    ]{code/get_one_news_page.py}

    \item Масштабирование на все страницы (листинг~\ref{lst:05}):
    \begin{itemize}
        \item Динамическое формирование URL через модификацию параметров;
        \item Пакетная обработка через цикл с изменяемым индексом страницы.
    \end{itemize}

    \lstinputlisting[
    caption={Функция обработки всего архива новостей},
    label={lst:05}
    ]{code/crawling_pages_function.py}

    \item Оптимизация производительности (листинг~\ref{lst:06}):
    \begin{itemize}
        \item Реализация многопоточности через стандартные средства Python;
        \item Создание изолированных DataFrame для каждого потока;
        \item Агрегация результатов после завершения параллельных задач.
    \end{itemize}

    \lstinputlisting[
    caption={Многопоточная реализация парсера},
    label={lst:06}
    ]{code/multithreading_apply_function.py}
\end{enumerate}

Полная реализация веб"=скрапера доступна в приложении~\ref{sec:01}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка новостного массива}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление лишних пробелов и переносов строк}
Для корректной токенизации и анализа текстовых данных требуется
предварительная очистка от лишних пробелов и переносов строк.
Реализацию этой процедуры можно выполнить с помощью встроенных
методов обработки строк в Python.

Алгоритм функции включает три этапа:
\begin{enumerate}
    \item \textbf{Копирование значимых символов:}
    Посимвольное добавление содержимого исходной строки в результирующий
    буфер до обнаружения пробела или переноса строки.

    \item \textbf{Нормализация пробелов:} 
    При обнаружении пробела/переноса:
    \begin{itemize}
        \item Добавление одного пробела в буфер
        \item Пропуск всех последующих пробелов/переносов до первого
        непробельного символа
    \end{itemize}

    \item \textbf{Циклическая обработка:} 
    Повтор шагов 1"=2 до полного прохода исходной строки.
\end{enumerate}

Реализация функции представлена в листинге~\ref{lst:07}:

\lstinputlisting[
caption={Функция нормализации пробелов и переносов строк},
label={lst:07}
]{code/remove_extra_spaces_and_line_breaks.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разделение строк на русские и английские фрагменты}
Библиотека SpaCy использует предобученные языковые модели, каждая из
которых оптимизирована для обработки одного языка (например, отдельно
для русского и английского).

Для новостных материалов ВШЭ, содержащих смешанные языковые фрагменты,
применение единой модели недопустимо. Решение заключается в предварительном
разделении текста на русскоязычные и англоязычные сегменты с последующей
обработкой соответствующими моделями.

Алгоритм разделения текста:
\begin{enumerate}
    \item \textbf{Инициализация языка:}
    \begin{itemize}
        \item Определение языка первого буквенного символа строки
        \item Установка текущего языкового идентификатора (RU/EN)
    \end{itemize}

    \item \textbf{Построение сегментов:}
    \begin{itemize}
        \item Посимвольное накопление символов во временном буфере
        \item Прерывание потока при обнаружении символа другого языка
    \end{itemize}

    \item \textbf{Сохранение результата:}
    \begin{itemize}
        \item Фиксация сегмента в формате (язык, текст)
        \item Сброс временного буфера
    \end{itemize}

    \item \textbf{Циклическое выполнение:}
    Повтор шагов 2"=3 до полной обработки строки с автоматическим переключением
    языкового идентификатора.
\end{enumerate}

Реализация функции представлена в листинге~\ref{lst:08}:

\lstinputlisting[
caption={Функция разделения текста на русско- и англоязычные фрагменты},
label={lst:08}
]{code/split_into_en_and_ru.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Обработка двоеточий и временных меток}
Библиотека BigARTM интерпретирует двоеточие как служебный символ, что
может привести к ошибкам обработки текстовых данных. Для устранения
проблемы требуется предварительная нормализация символа.

Стратегия обработки:

\begin{enumerate}
    \item Сохранение смысла в временных обозначениях: замена шаблонов
    времени (например, "12:30") на текстовый маркер "time";
    \item Удаление избыточных символов: устранение всех других
    двоеточий, не входящих в временные конструкции
\end{enumerate}

Алгоритм реализует контекстно"=зависимую обработку: анализ окружения
символа определяет его замену или удаление.

Реализация функции приведена в листинге~\ref{lst:09}:

\lstinputlisting[
caption={Функция нормализации двоеточий в тексте},
label={lst:09}
]{code/processing_token.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Токенизация, лемматизация и удаление стоп"=слов по словарю}
Библиотека SpaCy предоставляет унифицированный интерфейс для лингвистической
обработки текста. Её функционал позволяет выполнять в одном конвейере:

\begin{itemize}
    \item Токенизацию;
    \item Лемматизацию;
    \item Идентификацию стоп-слов
\end{itemize}

Принцип работы:

\begin{enumerate}
    \item На вход подаётся текстовая строка;
    \item Обработанные данные возвращаются в виде последовательности токенов;
    \item Каждый токен содержит:
    \begin{itemize}
        \item Исходную словоформу;
        \item Нормализованную лемму;
        \item Флаг принадлежности к стоп"=словам
    \end{itemize}
\end{enumerate}

Результирующая строка формируется путём фильтрации: сохраняются только
леммы токенов, не отнесённых к стоп"=словам.

Пример обработки русскоязычного текста показан в листинге~\ref{lst:10}:

\lstinputlisting[
caption={Обработка строки русского языка средствами SpaCy},
label={lst:10}
]{code/apply_spacy_for_one_str.py}

Полный алгоритм предобработки, объединяющий нормализацию пробелов,
токенизацию и фильтрацию, реализован в листинге~\ref{lst:11}:

\lstinputlisting[
caption={Комплексная обработка текста: нормализация, токенизация,
лемматизация, фильтрация стоп"=слов},
label={lst:11}
]{code/tokenize_lemmatize_and_del_stop_words.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление стоп"=слов с помощью метрики tfidf}
Как отмечалось ранее, удаление стоп"=слов исключительно по предзаданному
словарю имеет ограниченную эффективность. Для повышения качества фильтрации
предлагается дополнительное использование метрики TF"=IDF, позволяющей
оценивать значимость терминов в корпусе документов.

Алгоритм расширенной фильтрации:

\begin{enumerate}
    \item Вычисление TF"=IDF:
    \begin{enumerate}
        \item Формирование словаря терминов с помощью Gensim;
        \item Построение частотного корпуса документов;
        \item Расчёт весов TF"=IDF для каждого термина
    \end{enumerate}

    Реализация базового расчёта представлена в листинге~\ref{lst:12}:

    \lstinputlisting[
    caption={Вычисление TF"=IDF метрик для текстового корпуса},
    label={lst:12}
    ]{code/calc_dict_corpus_and_tfidf.py}

    \item Коррекция словаря:
    \begin{enumerate}
        \item Добавление терминов с нулевым TF"=IDF, исключённых Gensim
        по умолчанию;
        \item Нормализация структуры данных для последующего анализа;
    \end{enumerate}

    Соответствующая доработка реализована в листинге~\ref{lst:13}:

    \lstinputlisting[
    caption={Дополнение словаря нулевыми TF"=IDF значениями},
    label={lst:13}
    ]{code/add_missing_tfidf_words.py}

    \item Определение порога отсечения:
    \begin{enumerate}
        \item Вычисление n"=го процентиля распределения TF"=IDF;
        \item Установка границы для отбора малозначимых терминов;
    \end{enumerate}

    Логика расчёта границы показана в листинге~\ref{lst:14}:
    
    \lstinputlisting[
    caption={Определение порогового значения TF"=IDF},
    label={lst:14}
    ]{code/add_missing_tfidf_words.py}

    \item Фильтрация датасета:
    \begin{enumerate}
        \item Итеративное удаление терминов с TF'=IDF ниже порога;
        \item Дополнительная очистка низкочастотных слов (менее k вхождений);
    \end{enumerate}

    Финальный этап обработки представлен в листинге~\ref{lst:15}:

    \lstinputlisting[
    caption={Удаление стоп"=слов на основе TF"=IDF метрики},
    label={lst:15}
    ]{code/del_tfidf_stop_words.py}
\end{enumerate}

Полная реализация обработчика данных доступна в приложении~\ref{sec:02}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Количественные характеристики обработанного и необработанного
датасета}
В рамках данной работы была выполнена обработка новостного массива с
различными параметрами (имеется ввиду разные пороги для tfidf метрик, а также
некоторые другие приёмы). Количественные характеристики представлены
в соответствующих таблицах~\ref{sec:03}. Согласно значениям в них можно
сказать, что новости достаточно объёмные (среднее медианное количество токенов
в документе равно 305).

Также стоит упомянуть, что удаление стоп"=слов было результативно, так как
частота самого популярного слова для подготовленного новостного массива
упала с более, чем восьмиста тысяч до пятидесяти тысяч.

\begin{figure}[!ht]
	\centering
	\includegraphics{./images/zips_law_for_not_prepearing_data.png}
	\caption{\label{fig:01}%
	Закон Ципфа для неподготовленных данных}
\end{figure}
\newpage

\begin{figure}[!ht]
	\centering
	\includegraphics{./images/zips_law_for_prepearing_data.png}
	\caption{\label{fig:02}%
	Закон Ципфа для подготовленных данных}
\end{figure}

Так же косвенно это можно проследить по количеству уникальных токенов для
в коллекции (сократилось для подготовленных данных почти вдвое).

Однако строит заметить, что количество уникальных токенов остаётся огромным,
что может свидетельствовать о существовании большого количества шума и
опечаток, что может негативно сказаться как на тематическом моделировании,
так и на обучении алгоритмов глубокого и машинного обучения.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Вычисление тематической модели}
Тематическое моделирование с помощью библиотеки BigARTM достаточно удобно,
однако имеет ряд недостатков:

\begin{enumerate}
	\item Отсутствие такой встроенной метрики как когерентность;
	\item Громоздкое добавление регуляризаторов;
	\item Громоздное преобразование данных для вычислений в
	нужный формат;
	\item Отсутствие интерфейса для визуального отслеживания значения метрик
	качества тематических моделей;
	\item Отсутствие возможности подбора оптимальных гиперпараметров;
\end{enumerate}

Среди описанных выше изъянов наиболее существенным является первый,
остальные являются скорее неудобствами, которые тем неменее могут сделать
код громоздким и нечитабельным.

Чтобы решить описанные выше проблемы можно реализовать два отдельных класса,
которые будут добавлять необходимую функциональность.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Функциональности классов My\_BigARTM\_model и
\\ Hyperparameter\_optimizer}
В данном классе разумно добавить вычисление когерентности, удобное добавление
регуляризаторов и преобразование данных для вычислений в нужный формат, а также
создание графиков, визуализирующих изменение метрик для их удобного
отслеживания.

Добавлять в класс My\_BigARTM\_model функциональность по подбору гиперпараметров
будет излишним, так как это сделает код слишком громоздким и нелогичным, поэтому
она будет вынесена в отдельный класс \\ Hyperparameter\_optimizer. Это позволит
сделать код более простым и читаемым, а также удобно сохранять модели
с оптимально подобранными параметрами для различных типов подготовки данных.

Теперь можно приступить к планомерной реализации обоих классов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Преобразование новостного массива в приемлемый для BigARTM
формат}
BigARTM модель умеет работать только с несколькими форматами данных, например,
vowpal\_wabbit, описание этого формата можно увидеть ниже.

С pandas DataFrame BigARTM работать не умеет, поэтому новострой массив нужно
будет преобразовать. Разумно будет это сделать с помощью отдельной функции.

Алгоритм у данной функции будет следующий:

\begin{enumerate}
	\item Получаем строку из pandas DataFrame;
	\item Объединяем ячейки строки в единый текст;
	\item Записываем полученную текстовую строку с меткой, что это
	отдельный документ в соответствующий файл;
	\item Повторяем описанные выше действия, пока не будет пройден
	весь новостной массив.
\end{enumerate}

Реализация соответствующей функции выглядит следующим образом~\ref{lst:16}.

\lstinputlisting[
    caption={Функция преобразования новостного массива к vowpal\_wabbit формату},
    label={lst:16}
]{code/make_vowpal_wabbit.py}

После того как данные преобразованы к нужному формату, нужно их разделить
на батчи и вычислить словарь, делается это с помощью функций библиотеки
BigARTM. Код реализации соответствующей функции представлен в следующем
листинге~\ref{lst:17}.

\lstinputlisting[
    caption={Функция вычисления батчей и словаря},
    label={lst:17}
]{code/make_batches.py}

Теперь данные можно передавать для вычисления тематической модели.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удобное добавление регуляризаторов}
Модель BigARTM предоставляет большое количество регуляризаторов для
использования, однако их добавление в тематическую модель достаточно громоздко
и неудобно для массового использования. Поэтому есть необходимость добавления
регуляризатора лишь по одному переданному имени и гипермараметру, минуя
трудный синтаксис BigARTM. 

Решить данную проблему с точки зрения читабельности кода лучше с помощью
двух функций: первая будет добавлять один регуляризатор, а вторая,
вызывая первую, будет добавлять сразу несколько регуляризаторов.

Фрагмет реализации функции, добавляющей 1 решуляризатор представлен
в следующем листинге~\ref{lst:18}.

\lstinputlisting[
    caption={Фрагмент функции добавляющей 1 регуляризатор},
    label={lst:18}
]{code/add_regularizer.py}

Реализация функции, добавляющей несколько регуляризаторов, выглядит
следующим образом~\ref{lst:19}.

\lstinputlisting[
    caption={Функция добавляющая несколько регуляризаторов},
    label={lst:19}
]{code/add_regularizers.py}

Таким образом, были добавлены инструменты для удобной работы с
BigARTM регуляризаторами.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление когерентности}
Библиотека BigARTM обладает несколькими встроенными метриками качества,
однако одной из таких наиважнейших метрик, как когерентность она не обладает.
Исправить это можно, реализовав соответствующую функцию на базе библиотеки
Gensim (данная библиотека позволяет вычислять различные виды когерентности).

Чтобы вычислить когерентность с помощью библиотеки Gensim необходимо выполнить
следующие действия:

\begin{enumerate}
	\item Получить темы в виде списка ядер тем;
	\item Получить документы в виде двумерного списка слов, в котором каждая
	строка соответствует набору токенов одного документа;
	\item Передать вычисленные данные для вычисления когерентности.
\end{enumerate}

Реализация соответствующей функции выглядит следующим образом~\ref{lst:20}.

\lstinputlisting[
    caption={Функция вычисление когерентности},
    label={lst:20}
]{code/calc_coherence.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление тематической модели и формирование графиков метрик}
Сама библиотека BigARTM не предоставляет возможности отслеживать процесс
изменения метрик при обучении, особенно невстроенных метрик, поэтому данный
функционал прийдёт реализовать отдельно.

Чтобы получить графики изменения метрик нужно их вычистять каждую эпоху
формирования тематической модели, за это при её создании отвечает параметр
num\_collection\_passes. Однако если мы зададим его отличным от единицы,
то получим значение метрик уже после полного вычисления. Тогда необходимо
данный параметр передавать не в модель, а в цикл, который будет вычислять
модель только для одного прохода по коллекции, а после этого переходить к
вычислению значения метрик в текущую эпоху. Таким образом, получим
значение метрик за каждую эпоху.

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:21}.

\lstinputlisting[
    caption={Функция вычисление модели и метрик качества},
    label={lst:21}
]{code/calc_model.py}

После этого остаётся только вычислить соответствующие графики с помощью
библиотеки matplotlib. Функция построения графика изменения когерентности
выглядит следующим образом.

\lstinputlisting[
    caption={Функция вычисление графика изменения когерентности},
    label={lst:22}
]{code/calc_coherence_graphik.py}

Для остальных метрик код будет аналогичным.

Таким образом, основная функциональность класса My\_BigARTM\_model была
реализована. Полный код можно увидеть в соответствующем приложении~\ref{sec:04}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор гиперпараметров для тематического моделирования}
Реализовать подбор гиперпараметров удобно с помощью библиотеки optuna,
у неё достаточно простой и удобный интерфейс, а также есть возможно более
интеллектуального подбора, не по сетке параметров, а спомощью байесовской
оптимизации, что позволяет заметно сократить число попыток на подборку
большого числа параметров.

Для работы с optuna требуется функция, которая будет производить нужные
вычисления и возвращать в качестве результата метрики качества. Также
именно в этой функции задаются диапазоны значений гиперпараметров с помощью
методов trial.suggest\_int и trial.suggest\_float. Ключевые фрагменты
соответствующей функции представлены в следующем листинге~\ref{lst:23}.

\lstinputlisting[
    caption={Функция вычисления тематической модели для подбора гиперпараметров},
    label={lst:23}
]{code/objective.py}

Теперь получившуюся функцию можно вызвать для произведения вычислений
с помощью метода study.optimize, на выходе он вернёт набор попыток,
в каждой из которых будут содержаться выбранные гиперпараметры и
полученные при обучении метрики качества.

Следующим шагом станет выбор из попыток той, чьи параметры были оптимальными.
Для этого нужно будет отмасштабировать метрики и выбрать попытку по минимальной
сумме метрик (чем меньше значение, тем качественнее модель). Реализация
соответствующей функции представлена в следующем листинге~\ref{lst:24}.

\lstinputlisting[
    caption={Функция вычисления лучшей попытки},
    label={lst:24}
]{code/select_best_trial.py}

Осталось только по полученным оптимальным гиперпараметрам обучить модель и
вернуть её в качестве результата. Сделать это можно следующим образом~\ref{lst:25}.

\lstinputlisting[
    caption={Функция вычисления тематической модели с лучшими параметрами},
    label={lst:25}
]{code/optimizer.py}

Таким образом, был реализован основной функционал класса
Hyperparameter\_optimizer. Посмотреть его код полностью можно в
соответствующем приложении~\ref{sec:05}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Результаты тематического моделирования}
В рамках данной работы было проведено моделирование со всеми представленными
выше подготовленными данными. Для каждого новостного массива были подобраны
оптимальные гиперпараметры, с которыми были вычислены финальные модели.

Всего тематических моделей получилось 13, значение когерентности и перплексии
для них можно увидеть в соответствующих таблицах.

% Фиксированная ширина для колонки "Данные"
\newlength{\mydatalength}
\setlength{\mydatalength}{6cm}

\begin{longtable}{|p{\mydatalength}|c|c|}
  \caption{Метрики моделей} \label{tab:metrics} \\
  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline 
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 3486 & 0.470 \\
  \hline
  Без tfidf с add. & 2974 & 0.456 \\
  \hline
  С tfidf 1 пр. & 3643 & 0.476 \\
  \hline
  С tfidf 2 пр. & 3848 & 0.479 \\
  \hline
  С tfidf 3 пр. & - & - \\
  \hline
  С tfidf 4 пр. & - & - \\
  \hline
  С tfidf 5 пр. & 4094 & 0.495 \\
  \hline
  С tfidf 6 пр. & 3982 & 0.505 \\
  \hline
  С tfidf 7 пр. & 4620 & 0.491 \\
  \hline
  С tfidf 8 пр. & 4183 & 0.514 \\
  \hline
  С tfidf 9 пр. & 3811 & 0.496 \\
  \hline
  С tfidf 10 пр. & 4022 & 0.490 \\
  \hline
  С tfidf 10 пр. с add. & 3284 & 0.486 \\
  \hline
\end{longtable}

\begin{longtable}{|p{\mydatalength}|c|c|c|c|c|}
  \caption{Гиперпараметры моделей} \label{tab:hyperparams} \\
  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 8 & 6 & 7 & -1.561 & 0.809 \\
  \hline
  Без tfidf с add. & 8 & 5 & 6 & -0.004 & -0.653 \\
  \hline
  С tfidf 1 пр. & 6 & 7 & 5 & -1.540 & -0.038 \\
  \hline
  С tfidf 2 пр. & 8 & 6 & 4 & -0.101 & 0.146 \\
  \hline
  С tfidf 3 пр. & - & - & - & - & - \\
  \hline
  С tfidf 4 пр. & - & - & - & - & - \\
  \hline
  С tfidf 5 пр. & 8 & 6 & 6 & 1.139 & -1.981 \\
  \hline
  С tfidf 6 пр. & 8 & 6 & 7 & 0.954 & -1.353 \\
  \hline
  С tfidf 7 пр. & 8 & 5 & 5 & 0.942 & -0.102 \\
  \hline
  С tfidf 8 пр. & 6 & 7 & 7 & 1.757 & -1.222 \\
  \hline
  С tfidf 9 пр. & 8 & 6 & 7 & -0.449 & -0.365 \\
  \hline
  С tfidf 10 пр. & 8 & 5 & 6 & -0.184 & -1.826 \\
  \hline
  С tfidf 10 пр. с add. & 8 & 5 & 6 & 0.385 & -1.165 \\
  \hline
\end{longtable}

По ним можно сказать, что наилучшим качеством для тематического моделирования
обладает подготовка данных с удалением низкочастотных слов, но без удаления
стоп"=слов с помощью метрики tfidf. Объясняться это может следующим:

\begin{enumerate}
	\item Подбор гиперпараметров прошёл недостаточно полно, что
	не позволило в полной мере выбрать оптимальные гиперпараметры;
	\item Рассмотренно недостаточно вариантов подготовки данных
	(из"=за ограниченности времени не были рассмотрены варианты с
	tfidf удалением стоп"=слов и удалением низкочастотных слов);
	\item Удаление стоп"=слов с помощью метрики tfidf некорректно.
\end{enumerate}

На данный момент можно сказать, что наиболее вероятны первые две причины,
для подтверждения третьей не хватает данных.

Также по результатам можно сказать, что высокий процент порога для
tfidf метрики негативно влияет на обучение. Связано это, вероятно,
с тем, что начинают удаляться уже не только стоп"=слова и порог нужно
понизить.

% Раздел "Заключение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conclusion


%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Листинг вебскраппера} \label{sec:01}
\lstinputlisting[
    caption={Полный код вебскраппера}
]{code/parse_news.py}

\section{Листинг обработчика новостного массива} \label{sec:02}
\lstinputlisting[
    caption={Полный код подготовки новостного массива}
]{code/parse_news.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Количественные характеристики подготовленного и неподготовленного
новостного массива} \label{sec:03}

\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF-IDF 1\%}} & 
\rotatebox{90}{\textbf{TF-IDF 2\%}} & 
\rotatebox{90}{\textbf{TF-IDF 3\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF-IDF 1\%}} & 
\rotatebox{90}{\textbf{TF-IDF 2\%}} & 
\rotatebox{90}{\textbf{TF-IDF 3\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 12131111 & 6545045 & - & 6479545 & 6414045 & 6348544 \\
\hline
Кол. уник. ток. & 278724 & 148677 & - & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 47 & 31 & - & 31 & 31 & 30 \\
\hline
Среднее кол. ток. в док. & 695 & 375 & - & 371 & 367 & 364 \\
\hline
Медианное кол. ток. в док. & - & 313 & - & 312 & 310 & 309 \\
\hline
Макс. кол. ток. в док. & 6514 & 3151 & - & 2903 & 2825 & 2766 \\
\hline
Мин. кол. уник. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 39 & 27 & - & 27 & 27 & 30 \\
\hline
Сред. кол. уник. ток. в док. & 346 & 214 & - & 211 & 208 & 205 \\
\hline
Мед. кол. уник. ток. в док. & - & 186 & - & 185 & 183 & 182 \\
\hline
Макс. кол. уник. ток. в док. & 2287 & 1353 & - & 1299 & 1262 & 1214 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 4\%}} & 
\rotatebox{90}{\textbf{TF-IDF 5\%}} & 
\rotatebox{90}{\textbf{TF-IDF 6\%.}} & 
\rotatebox{90}{\textbf{TF-IDF 7\%}} & 
\rotatebox{90}{\textbf{TF-IDF 8\%}} & 
\rotatebox{90}{\textbf{TF-IDF 9\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 4\%}} & 
\rotatebox{90}{\textbf{TF-IDF 5\%}} & 
\rotatebox{90}{\textbf{TF-IDF 6\%}} & 
\rotatebox{90}{\textbf{TF-IDF 7\%}} & 
\rotatebox{90}{\textbf{TF-IDF 8\%}} & 
\rotatebox{90}{\textbf{TF-IDF 9\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 6283046 & 6217544 & 6152044 & 6086544 & 6021044 & 5955543 \\
\hline
Кол. уник. ток. & 148677 & 148677 & 148677 & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 30 & 30 & 30 & 30 & 29 & 29 \\
\hline
Среднее кол. ток. в док. & 360 & 356 & 352 & 349 & 345 & 341 \\
\hline
Медианное кол. ток. в док. & 307 & 306 & 305 & 303 & 301 & 299 \\
\hline
Макс. кол. ток. в док. & 2713 & 2662 & 2595 & 2545 & 2501 & 2424 \\
\hline
Мин. кол. уник. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 27 & 29 & 29 & 28 & 28 & 28 \\
\hline
Сред. кол. уник. ток. в док. & 201 & 198 & 195 & 192 & 189 & 186 \\
\hline
Мед. кол. уник. ток. в док. & 181 & 179 & 177 & 176 & 174 & 172 \\
\hline
Макс. кол. уник. ток. в док. & 1164 & 1122 & 1085 & 1047 & 1018 & 986 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{3}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 10\%}} & 
\rotatebox{90}{\textbf{TF-IDF 10\% + Низк.}} \\
\hline
\endfirsthead

\multicolumn{3}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 10\%}} & 
\rotatebox{90}{\textbf{TF-IDF 10\% + Низк.}} \\
\hline
\endhead

\hline
\multicolumn{3}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 \\
\hline
Кол. токенов & 5890042 & - \\
\hline
Кол. уник. ток. & 148677 & - \\
\hline
Мин. кол. ток. в док. & 4 & - \\
\hline
Модальное кол. ток. в док. & 30 & - \\
\hline
Среднее кол. ток. в док. & 337 & - \\
\hline
Медианное кол. ток. в док. & 297 & - \\
\hline
Макс. кол. ток. в док. & 2391 & - \\
\hline
Мин. кол. уник. ток. в док. & 4 & - \\
\hline
Мод. кол. уник. ток. в док. & 28 & - \\
\hline
Сред. кол. уник. ток. в док. & 182 & - \\
\hline
Мед. кол. уник. ток. в док. & 170 & - \\
\hline
Макс. кол. уник. ток. в док. & 946 & - \\
\hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Полный код класса My\_BigARTM\_model} \label{sec:04}
\lstinputlisting[
    caption={Полный код класса My\_BigRTM\_model}
]{code/My_BigARTM_model.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Полный код класса Hyperparameter\_optimizer} \label{sec:05}
\lstinputlisting[
    caption={Полный код класса Hyperparameter\_optimizer}
]{code/hyperparameter_optimizer.py}


\end{document}
