\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{longtable}  % Для многостраничных таблиц
\usepackage{array}      % Для дополнительного форматирования столбцов
\usepackage{booktabs}   % Для улучшенного оформления линий

\usepackage{xltabular}  % Добавьте в преамбулу
\usepackage{booktabs}   % Для улучшенного стиля линий
\usepackage{array}      % Для управления выравниванием

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
\intro

В настоящее время обработка больших объёмов текстовых данных,
включая новостные потоки, становится критически важной задачей. Как в
научной среде, так и в бизнесе требуется оперативно анализировать информацию,
отслеживать тенденции и принимать решения. Однако анализ всего массива данных
невозможен из-за его масштабов, необходимо фильтровать информацию, оставляя
только нужную. 

Помочь в решении этой проблемы может тематическая классификация. Хотя многие
сайты и порталы предлагают рубрикацию контента, её точность часто оказывается
низкой: теги присваиваются некорректно или поверхостно. Это приводит к ошибкам
в поиске и анализе информации.

В таком случае необходим механизм позволяющий получать правильную тематическую
классификацию данных, который смог бы присваивать темы тем же новостям
автоматически. Одни из возможных инструментов, которые позволяют реализовать
подобие такого механизма "--- это тематические модели и алгоритмы машинного и
глубокого обучения. Первый из них позволяет косвенно выявить темы текстового
набора данных и разметить данные для обучения второго инструмента, который
сможет тематически классифицировать последующий текст.

Таким образом, целью данной работы является реализация механизма автоматической
тематической классификации новостей с помощью методов тематического моделирования
и глубокого и машинного обучения.

Для достижения этой цели необходимо решить следующие задачи:

\begin{enumerate}
	\item Реализовать механизм получения новостных массивов данных;
	\item Реализовать механизм подготовки текстовых данных;
	\item Вычислить тематические модели;
	\item Путём сравнительного анализа выявить наиболее удачную тематическую
	модель;
	\item Разметить данные для обучения на них моделей машинного и глубокого
	обучения;
	\item Обучить несколько моделей машинного и глубокого обучения и выявить
	наиболее удачную путём сравнительного анализа;
	\item Провести анализ получившихся результатов.
\end{enumerate}

\section{Теоретические и методологические основы}

\subsection{Получение текстовых данных}

\subsubsection{Выбор инструмента}
Для получения каких"=либо данных с сайта существует три основных метода:
\begin{itemize}
	\item Ручной метод "--- выписывание необходимой информации с помощью
	человека;
	\item Получение данных путём предоставления их запроса у владельца, с
	их последующим скачиванием;
	\item Получение данных программным путём.
\end{itemize}

Первый метод из"=за своей неэффективности можно сразу отбросить. Второй метод
далеко не всегда можно применить, кроме того вряд ли владельцы информационных
платформ будут оперативно отсылать все данные по первой просьбе. Таким образом,
остаётся только третий метод.

Оперативно и достаточно эффективно в большинстве случаев можно получить данные
применяя инструменты вебскраппинга. Дальше будет использоваться этот вариант
получения новостного массива.

Различные библиотеки для вебскраппинга доступны на разных языках, однако
исходя из того, что наиболее популярным языком для обработки данных
и работы с машинным и глубоким обучением является python, выберем библиотеки
доступные на нём. Такими библиотеками ялвяются requests, beautifulsoap4
и selenium. Первая бибилиотека позволяет отсылать http запросы. Вторая
библиотека позволяет преобразовывать html код в подобие классов для
удобного получения информации. Последняя библиотека позволяет обрабатывать
сайта, которые по http запросу не выдают html код наблюдаемой пользователем
страницы. Данная библиотека позволяет эмулировать работу браузера и получать
html код страницы прямо из него.

Такого набора хватит для обработки подавляющего большинства сайтов.

\subsubsection{Подбор информационной платформы}
В рамках данной работы среди всех типов текстовых данных будут рассматриваться
новостные. Теперь нужно подобрать сайт.

Если для получения информации есть несоклько возможных веб"=источников,
то стоит выбирать сайт по следующим критериям:

\begin{enumerate}
	\item Сайт имеет единую структуру документов;
	\item Сайт не блокирует http запросы отправляемые вебскраппером;
	\item Сайт не является реактивным, то есть в момент просмотра страницы
	html код страницы полностью сформирован и доступен по запросу клиенту.
\end{enumerate}

Будет идеально, если все пункты соблюдаются, одако, даже в случае отсутствия
пунктов 2 и 3, ограничения в большинстве случаев можно достаточно просто
обойти. В случае несоответствия пункта 1 могут возникнуть серьёзные трудности,
которые, в худшем случае, решить только методами веб скраппинга не получится.

В рамках данной работы будет использоваться новостной сайт ВШЭ. Данный сайт
соответствует всем описанным выше критериям.

\subsection{Подготовка текстовых данных}  

Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы включают:  

\begin{enumerate}  
    \item Очистка от технического шума: 
    \begin{itemize}  
        \item Удаление лишних пробелов, переносов строк;  
        \item Очистка от спецсимволов (скобки, HTML-теги, эмодзи);  
        \item Нормализация регистра (приведение всего текста к нижнему регистру).  
    \end{itemize}  
    
    \item Токенизация:  
    Разделение текста на слова или предложения;
    
    \item Лемматизация:  
    Приведение слов к начальной форме (например, «бежал» $\Rightarrow$ «бежать»);
    
    \item Удаление стоп-слов:  
    Исключение частых слов без смысловой нагрузки (предлоги, частицы, местоимения);  
\end{enumerate}  

\textbf{Обоснование выбора лемматизации вместо стемминга:}  
Стемминг (например, алгоритм Snowball) «обрубает» окончания по шаблонам («бежал» $\Rightarrow$ «беж»),
что искажает смысл. Лемматизация сохраняет семантику, что критично для
тематического моделирования.

\subsubsection{Выбор инструментов}  

Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов. Определение стоп"=слов в обоих случаях происходит по
заранее заданным словарям, поэтому разницы здесь нет. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает интеграцию в проект.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по предопределённому
списку, который не является исчерпывающим. Это связано с тем, что набор
стоп"=слов зависит от тематики текста, и универсального решения не существует.
Для дополнительной фильтрации применим метрику TF-IDF, которая оценивает
значимость слов. Формула расчёта:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа $d$. Её
расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп-слов?) и Gensim (расширенная
фильтрация через TF-IDF).

\subsection{Математические основы тематического моделирования}

\subsubsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных,
который позволяет выявить семантические структуры в коллекциях документов.

Основная идея тематического моделирования заключается в том, что
слова в тексте связаны не с конкретным документом, а с темами. Сначала текст
разбивается на темы, и каждая из них генерирует слова для соответствующих
позиций в документе. Таким образом, сначала формируется тема, а затем тема
формирует терм.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и взаимовстречаемости слов.

\subsubsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Ниже приведены
основные количественные характеристики, использующиеся при тематическом
моделировании:
\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и
аксиомы:
\begin{itemize}
    \item независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}
\section{Практико"=технологические основы}

\subsection{Получение новостного массива путём вебскраппинга}
Для обработки такого простого новостного сайта как у ВШЭ достаточно
использования requests и beautifulsoap4, без selenium.

Чтобы наиболее просто и эффективно получить данные необходимо разобрать
структуру сайта и разработать соответствующие функции под каждую из частей.
Сам портал представляет собой многостраничный сайт, на каждой странице
которого расположено по 10 новостей с краткой информацией по каждой:
ссылка, дата, заголовок, краткое содержание. На каждую новость можно перейти
по ссылке для получения полного её содержания. 

Теперь последовательно реализуем функции"=обработчики под соответствующие части
сайта.

Чтобы получать html код страницы, необходимо воспользоваться библиотекой
requests и методом get. Данный метод отправляет запрос на сайт и получает
соответствующий код в качестве ответа, который можно сохранить в файл
для последующей выгрузки и обработки. Соответствующая функция расположена
в листинге~\ref{lst:01}.

\lstinputlisting[
    caption={Функция получения html кода страницы},
    label={lst:01}
]{code/get_page_function.py}

Далее нужно реализовать получение краткой информации о новости:
ссылка, дата, краткое содержание. Для этого нужно загрузить код страницы
из файла и преобразовать его к классам с помощью библиотеки beautifulsoap4.
Далее можно будет воспользоваться поиском по тегам и классам с помощью
метода find и получить текстовое содержимое с помощью методов text и get.
Пример получения ссылки и краткого содержания новости можно увидеть
в данном листинге~\ref{lst:02}.

\lstinputlisting[
    caption={Получение ссылки и краткого содержания},
    label={lst:02}
]{code/get_link_and_summary.py}

Теперь нужно реализовать функцию получения полного содержания новости. Для
этого нужно воспользоваться реализованной функцией get\_page (получить
код страницы по полученной ранее ссылке на новость), преобразовать его
в классы с помощью beautifulsoap4 и получить текстовое содержимое
с помощью методов find и text. Реализацию соответствующей функции
можно увидеть в листинге~\ref{lst:03}.

\lstinputlisting[
    caption={Функция получения полного текстового содержания новости},
    label={lst:03}
]{code/get_news_content.py}

Следующим шагом нужно вспомнить, что на странице располагается 10 новостей,
каждая новость располагается в теге div с классом post. Таким образом, нужно
10 раз проитерироваться по данным тегам и получить 10 новостей. Сделать это
можно с помощью метода find\_next\_sibling (он ищет следующий тег, который
идентичен по типу и классу предыдущему) и обычного цикла. Хранить полученное
содержимое удобно в pandas DataFrame, так как с помощью него удобно
обрабатывать полученные массивы данных и вычислять их колличественные
характеристики. Ключевые части соответствующей функции представлены в
следующем листинге~\ref{lst:04}.

\lstinputlisting[
    caption={Функция обработки одной страницы новостей},
    label={lst:04}
]{code/get_one_news_page.py}

Далее необходимо реализовать функцию обрабатывающую все страницы с новостями.
Сделать это можно путём многократного применения описанной выше функции обработки
одной новостной страницы к изменяемой ссылке страницы. Благодаря простому
устройству сайта ВШЭ менять эту ссылку можно достаточно просто с помощью
обычного цикла путём изменения индекса в одной части. Соответствующий код
представлен в следующем листинге~\ref{lst:05}.

\lstinputlisting[
    caption={Функция обработки всех страниц новостей},
    label={lst:05}
]{code/crawling_pages_function.py}

Осталось только для ускорения получения данных с файла добавить многопоточность.
Сделать это можно с помощью стандартных средств языка python, только стоит учесть,
что под каждый отдельный поток нужно будет создать свой отдельный контейнер
pandas DataFrame, чтобы избежать проблем с записью. Соответствующий код
представлен в следующем листинге~\ref{lst:06}.

\lstinputlisting[
    caption={Многопоточное получение новостей},
    label={lst:06}
]{code/multithreading_apply_function.py}

Полный код вебскраппера можно увидеть в соответствующем приложении~\ref{sec:01}.

\subsection{Подготовка новостного массива}

\subsubsection{Удаление лишних пробелов и переносов строк}
Для корректной токенизации и просто для удобства анализа текстовых данных
важно удалить из них лишние пробелы и переносы строк, сделать это можно 
с помощью стандартных средств языка python.

Функция будет иметь следующий алгоритм:

\begin{enumerate}
	\item Записываем в результирующую строку символы из исходной
	строки, пока не будет встречен символ пробела или переноса строки;
	\item Добавляем к результирующей строке 1 символ пробела и
	прекращаем добавление символов, пока не встретим символ, отличный
	от пробела или переноса строки;
	\item В случае, когда встретится символ не являющийся пробелом
	или переносом строки переходим к пункту 1. Повторяем описанные выше действия
	пока не будет пройдена вся исходная строка.
\end{enumerate}

Реализация соответствующей функции представлена в следующем листнге~\ref{lst:07}.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк},
    label={lst:07}
]{code/remove_extra_spaces_and_line_breaks.py}

\subsubsection{Разделение строк на русские и английские фрагменты}
Библиотека SpaCy обрабатывает текст с помощью различных предобученных нейронных
сетей, такие сети обучаются работе только на одном языке, например, только
на русском или английском языке.

Текст новостей с новостного сайта ВШЭ имеет вставки на английском языке,
что делает некорректным использование только одной предобученной нейронной сети.
Поэтому, чтобы применять сразу два типа нейронных сетей необходимо
разбивать строки на русские и английские фрагменты. Решить данную задачу
можно с помощью стандартных средств языка python.

Функция будет иметь следующий алгоритм:

\begin{enumerate}
	\item Определяем к какому алфавиту принадлежит первый буквенный символ строки
	и устанавливаем идентификатор в состояние соответствующее типу алфавита;
	\item Записываем последовательно символы строки во временную подстроку,
	пока не встретим букву другого алфавита;
	\item После встречи символа противоположного алфавита записываем в список
	кортеж вида (идентификатор алфавита, временная подстрока);
	\item Очищаем временную подстроку и изменяем состояние идентификатора на
	противоположное. После этого повторяем описанные выше действия, пока не
	будет пройдена вся исходная строка.
\end{enumerate}

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:08}.

\lstinputlisting[
    caption={Функция разбиения строки на русские и английские фрагменты},
    label={lst:08}
]{code/split_into_en_and_ru.py}

\subsubsection{Обработка двоеточий и временных меток}
При вычислении тематической модели BigARTM использует символ двоеточия как
служебный, поэтому наличие его в текстовых данных приведёт к возникновению
ошибок.

Само двоеточие, чаще всего, используется при написании времени, данные случаи
можно обработать. Другие случаи применения предусмотреть проблематично, поэтому
работать функция будет следующим образом: если двоеточие располагается в
шаблоне временной метки, то будем заменять её на строку time, в противном
случае будем просто удалять двоеточие.

Реализация соответствуещей функции представлена в следующем листинге~\ref{lst:09}.

\lstinputlisting[
    caption={Функция обработки двоеточий и временных меток},
    label={lst:09}
]{code/processing_token.py}

\subsubsection{Токенизация, лемматизация и удаление стоп"=слов по словарю}
Библиотека SpaCy имеет простой и удобный интерфейс. Для проведения
токенизации, лемматизации и обнаружении стоп слов достаточно просто передать
ей на вход строку. На выходе будет получен список объектов, в каждом из которых
содержится по одному из токенов, их принадлежность к стоп"=словам из словаря,
начальная и исходная формы. С помощью этих объектов удобно записать в
результирующую строку начальные формы токенов, которые не являются
стоп"=словами.

Пример применения библиотеки SpaCy к одной строке русского языка имеет
следующий вид~\ref{lst:10}.

\lstinputlisting[
    caption={Пример применения библиотеки SpaCy для обработки одной строки
	русского языка},
    label={lst:10}
]{code/apply_spacy_for_one_str.py}

Реализация полного алгоритма, сожержащего описанные выше функции представлена
в следующем листинге~\ref{lst:11}.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк,
	токенизации, лемматизации и удаления стоп"=слов по словарю},
    label={lst:11}
]{code/tokenize_lemmatize_and_del_stop_words.py}

\subsubsection{Удаление стоп"=слов с помощью метрики tfidf}
Как говорилось ранее удаление стоп"=слов только по словарю не может быть
исчерпывающим, поэтому можно применить метрику tfidf для расчёта значимости
слов и удалять слова с малой значимостью.

Расчёт этой метрики удобно с помощью библиотеки Gensim. Для этого нужно
вычислить по коллекции документов словарь, затем по словарю сформировать
частотный словаь "--- corpus, а уже по нему вычислить tfidf метрики для
слов.

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:12}.

\lstinputlisting[
    caption={Функция вычисления tfidf метрики для слов документов},
    label={lst:12}
]{code/calc_dict_corpus_and_tfidf.py}

Однако данное вычисление не является полным, так как библиотека Gensim не
добавляет в словарь слова, значение tfidf которых точно будет равняться нулю.
В таком случае необходимо добавить недостающие слова. Реализация соответствующей
функции представлена в следующем листинге~\ref{lst:13}.

\lstinputlisting[
    caption={Функция добавление недостающих tfidf слов},
    label={lst:13}
]{code/add_missing_tfidf_words.py}

Последним шагом перед удалением стоп"=слов является вычисление границы,
по которой будет определяться принадлежность к стоп"=словам. Сделать это
можно следующим образом~\ref{lst:14}.

\lstinputlisting[
    caption={Функция вычисления tfidf границы},
    label={lst:14}
]{code/add_missing_tfidf_words.py}

В данном случае к стоп"=словам будут относиться слова, значение tfidf
метрики которых будет относится к $n$ минимальным процентам значений.

Теперь осталось только пройтись по датасету и удалить соответствующие
стоп"=слова. Реализация соответсвующей функции представлена в следующем
листинге~\ref{lst:15}.

\lstinputlisting[
    caption={Функция удаление вычисленных по метрике tfidf стоп"=слов},
    label={lst:15}
]{code/del_tfidf_stop_words.py}

Также стоит сказать, что также дополнительно стоит добавить удаление
низкочастотных слов, так как это может положительно повлиять на результаты
тематического моделирования.

Полный код обработчика новостного массива можно увидеть в соответствующем
приложении~\ref{sec:02}.

\subsection{Количественные характеристики обработанного и необработанного
датасета}
В рамках данной работы была выполнена обработка новостного массива с
различными параметрами (имеется ввиду разные пороги для tfidf метрик, а также
некоторые другие приёмы). Количественные характеристики представлены
в соответствующих таблицах~\ref{sec:03}. Согласно значениям в них можно
сказать, что новости достаточно объёмные (среднее медианное количество токенов
в документе равно 305).

Также стоит упомянуть, что удаление стоп"=слов было результативно, так как
частота самого популярного слова для подготовленного новостного массива
упала с более, чем восьмиста тысяч до пятидесяти тысяч.

\begin{figure}[!ht]
	\centering
	\includegraphics{./images/zips_law_for_not_prepearing_data.png}
	\caption{\label{fig:01}%
	Закон Ципфа для неподготовленных данных}
\end{figure}
\newpage

\begin{figure}[!ht]
	\centering
	\includegraphics{./images/zips_law_for_prepearing_data.png}
	\caption{\label{fig:02}%
	Закон Ципфа для подготовленных данных}
\end{figure}

Так же косвенно это можно проследить по количеству уникальных токенов для
в коллекции (сократилось для подготовленных данных почти вдвое).

Однако строит заметить, что количество уникальных токенов остаётся огромным,
что может свидетельствовать о существовании большого количества шума и
опечаток, что может негативно сказаться как на тематическом моделировании,
так и на обучении алгоритмов глубокого и машинного обучения.

\subsection{Вычисление тематической модели}
Тематическое моделирование с помощью библиотеки BigARTM достаточно удобно,
однако имеет ряд недостатков:

\begin{enumerate}
	\item Отсутствие такой встроенной метрики как когерентность;
	\item Громоздкое добавление регуляризаторов;
	\item Громоздное преобразование данных для вычислений в
	нужный формат;
	\item Отсутствие интерфейса для визуального отслеживания значения метрик
	качества тематических моделей;
	\item Отсутствие возможности подбора оптимальных гиперпараметров;
\end{enumerate}

Среди описанных выше изъянов наиболее существенным является первый,
остальные являются скорее неудобствами, которые тем неменее могут сделать
код громоздким и нечитабельным.

Чтобы решить описанные выше проблемы можно реализовать два отдельных класса,
которые будут добавлять необходимую функциональность.

\subsubsection{Функциональности классов My\_BigARTM\_model и \\ Hyperparameter\_optimizer}
В данном классе разумно добавить вычисление когерентности, удобное добавление
регуляризаторов и преобразование данных для вычислений в нужный формат, а также
создание графиков, визуализирующих изменение метрик для их удобного
отслеживания.

Добавлять в класс My\_BigARTM\_model функциональность по подбору гиперпараметров
будет излишним, так как это сделает код слишком громоздким и нелогичным, поэтому
она будет вынесена в отдельный класс \\ Hyperparameter\_optimizer. Это позволит
сделать код более простым и читаемым, а также удобно сохранять модели
с оптимально подобранными параметрами для различных типов подготовки данных.

Теперь можно приступить к планомерной реализации обоих классов.

\subsubsection{Преобразование новостного массива в приемлемый для BigARTM
формат}
BigARTM модель умеет работать только с несколькими форматами данных, например,
vowpal\_wabbit, описание этого формата можно увидеть ниже.

С pandas DataFrame BigARTM работать не умеет, поэтому новострой массив нужно
будет преобразовать. Разумно будет это сделать с помощью отдельной функции.

Алгоритм у данной функции будет следующий:

\begin{enumerate}
	\item Получаем строку из pandas DataFrame;
	\item Объединяем ячейки строки в единый текст;
	\item Записываем полученную текстовую строку с меткой, что это
	отдельный документ в соответствующий файл;
	\item Повторяем описанные выше действия, пока не будет пройден
	весь новостной массив.
\end{enumerate}

Реализация соответствующей функции выглядит следующим образом~\ref{lst:16}.

\lstinputlisting[
    caption={Функция преобразования новостного массива к vowpal\_wabbit формату},
    label={lst:16}
]{code/make_vowpal_wabbit.py}

После того как данные преобразованы к нужному формату, нужно их разделить
на батчи и вычислить словарь, делается это с помощью функций библиотеки
BigARTM. Код реализации соответствующей функции представлен в следующем
листинге~\ref{lst:17}.

\lstinputlisting[
    caption={Функция вычисления батчей и словаря},
    label={lst:17}
]{code/make_batches.py}

Теперь данные можно передавать для вычисления тематической модели.

\subsubsection{Удобное добавление регуляризаторов}
Модель BigARTM предоставляет большое количество регуляризаторов для
использования, однако их добавление в тематическую модель достаточно громоздко
и неудобно для массового использования. Поэтому есть необходимость добавления
регуляризатора лишь по одному переданному имени и гипермараметру, минуя
трудный синтаксис BigARTM. 

Решить данную проблему с точки зрения читабельности кода лучше с помощью
двух функций: первая будет добавлять один регуляризатор, а вторая,
вызывая первую, будет добавлять сразу несколько регуляризаторов.

Фрагмет реализации функции, добавляющей 1 решуляризатор представлен
в следующем листинге~\ref{lst:18}.

\lstinputlisting[
    caption={Фрагмент функции добавляющей 1 регуляризатор},
    label={lst:18}
]{code/add_regularizer.py}

Реализация функции, добавляющей несколько регуляризаторов, выглядит
следующим образом~\ref{lst:19}.

\lstinputlisting[
    caption={Функция добавляющая несколько регуляризаторов},
    label={lst:19}
]{code/add_regularizers.py}

Таким образом, были добавлены инструменты для удобной работы с
BigARTM регуляризаторами.

\subsubsection{Вычисление когерентности}
Библиотека BigARTM обладает несколькими встроенными метриками качества,
однако одной из таких наиважнейших метрик, как когерентность она не обладает.
Исправить это можно, реализовав соответствующую функцию на базе библиотеки
Gensim (данная библиотека позволяет вычислять различные виды когерентности).

Чтобы вычислить когерентность с помощью библиотеки Gensim необходимо выполнить
следующие действия:

\begin{enumerate}
	\item Получить темы в виде списка ядер тем;
	\item Получить документы в виде двумерного списка слов, в котором каждая
	строка соответствует набору токенов одного документа;
	\item Передать вычисленные данные для вычисления когерентности.
\end{enumerate}

Реализация соответствующей функции выглядит следующим образом~\ref{lst:20}.

\lstinputlisting[
    caption={Функция вычисление когерентности},
    label={lst:20}
]{code/calc_coherence.py}

\subsubsection{Вычисление тематической модели и формирование графиков метрик}
Сама библиотека BigARTM не предоставляет возможности отслеживать процесс
изменения метрик при обучении, особенно невстроенных метрик, поэтому данный
функционал прийдёт реализовать отдельно.

Чтобы получить графики изменения метрик нужно их вычистять каждую эпоху
формирования тематической модели, за это при её создании отвечает параметр
num\_collection\_passes. Однако если мы зададим его отличным от единицы,
то получим значение метрик уже после полного вычисления. Тогда необходимо
данный параметр передавать не в модель, а в цикл, который будет вычислять
модель только для одного прохода по коллекции, а после этого переходить к
вычислению значения метрик в текущую эпоху. Таким образом, получим
значение метрик за каждую эпоху.

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:21}.

\lstinputlisting[
    caption={Функция вычисление модели и метрик качества},
    label={lst:21}
]{code/calc_model.py}

После этого остаётся только вычислить соответствующие графики с помощью
библиотеки matplotlib. Функция построения графика изменения когерентности
выглядит следующим образом.

\lstinputlisting[
    caption={Функция вычисление графика изменения когерентности},
    label={lst:22}
]{code/calc_coherence_graphik.py}

Для остальных метрик код будет аналогичным.

Таким образом, основная функциональность класса My\_BigARTM\_model была
реализована. Полный код можно увидеть в соответствующем приложении~\ref{sec:04}.

\subsubsection{Подбор гиперпараметров для тематического моделирования}
Реализовать подбор гиперпараметров удобно с помощью библиотеки optuna,
у неё достаточно простой и удобный интерфейс, а также есть возможно более
интеллектуального подбора, не по сетке параметров, а спомощью байесовской
оптимизации, что позволяет заметно сократить число попыток на подборку
большого числа параметров.

Для работы с optuna требуется функция, которая будет производить нужные
вычисления и возвращать в качестве результата метрики качества. Также
именно в этой функции задаются диапазоны значений гиперпараметров с помощью
методов trial.suggest\_int и trial.suggest\_float. Ключевые фрагменты
соответствующей функции представлены в следующем листинге~\ref{lst:23}.

\lstinputlisting[
    caption={Функция вычисления тематической модели для подбора гиперпараметров},
    label={lst:23}
]{code/objective.py}

Теперь получившуюся функцию можно вызвать для произведения вычислений
с помощью метода study.optimize, на выходе он вернёт набор попыток,
в каждой из которых будут содержаться выбранные гиперпараметры и
полученные при обучении метрики качества.

Следующим шагом станет выбор из попыток той, чьи параметры были оптимальными.
Для этого нужно будет отмасштабировать метрики и выбрать попытку по минимальной
сумме метрик (чем меньше значение, тем качественнее модель). Реализация
соответствующей функции представлена в следующем листинге~\ref{lst:24}.

\lstinputlisting[
    caption={Функция вычисления лучшей попытки},
    label={lst:24}
]{code/select_best_trial.py}

Осталось только по полученным оптимальным гиперпараметрам обучить модель и
вернуть её в качестве результата. Сделать это можно следующим образом~\ref{lst:25}.

\lstinputlisting[
    caption={Функция вычисления тематической модели с лучшими параметрами},
    label={lst:25}
]{code/optimizer.py}

Таким образом, был реализован основной функционал класса
Hyperparameter\_optimizer. Посмотреть его код полностью можно в
соответствующем приложении~\ref{sec:05}.

\subsection{Результаты тематического моделирования}
В рамках данной работы было проведено моделирование со всеми представленными
выше подготовленными данными. Для каждого новостного массива были подобраны
оптимальные гиперпараметры, с которыми были вычислены финальные модели.

Всего тематических моделей получилось 13, значение когерентности и перплексии
для них можно увидеть в соответствующих таблицах.

% Фиксированная ширина для колонки "Данные"
\newlength{\mydatalength}
\setlength{\mydatalength}{6cm}

\begin{longtable}{|p{\mydatalength}|c|c|}
  \caption{Метрики моделей} \label{tab:metrics} \\
  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline 
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 3486 & 0.470 \\
  \hline
  Без tfidf с add. & 2974 & 0.456 \\
  \hline
  С tfidf 1 пр. & 3643 & 0.476 \\
  \hline
  С tfidf 2 пр. & 3848 & 0.479 \\
  \hline
  С tfidf 3 пр. & - & - \\
  \hline
  С tfidf 4 пр. & - & - \\
  \hline
  С tfidf 5 пр. & 4094 & 0.495 \\
  \hline
  С tfidf 6 пр. & 3982 & 0.505 \\
  \hline
  С tfidf 7 пр. & 4620 & 0.491 \\
  \hline
  С tfidf 8 пр. & 4183 & 0.514 \\
  \hline
  С tfidf 9 пр. & 3811 & 0.496 \\
  \hline
  С tfidf 10 пр. & 4022 & 0.490 \\
  \hline
  С tfidf 10 пр. с add. & 3284 & 0.486 \\
  \hline
\end{longtable}

\begin{longtable}{|p{\mydatalength}|c|c|c|c|c|}
  \caption{Гиперпараметры моделей} \label{tab:hyperparams} \\
  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 8 & 6 & 7 & -1.561 & 0.809 \\
  \hline
  Без tfidf с add. & 8 & 5 & 6 & -0.004 & -0.653 \\
  \hline
  С tfidf 1 пр. & 6 & 7 & 5 & -1.540 & -0.038 \\
  \hline
  С tfidf 2 пр. & 8 & 6 & 4 & -0.101 & 0.146 \\
  \hline
  С tfidf 3 пр. & - & - & - & - & - \\
  \hline
  С tfidf 4 пр. & - & - & - & - & - \\
  \hline
  С tfidf 5 пр. & 8 & 6 & 6 & 1.139 & -1.981 \\
  \hline
  С tfidf 6 пр. & 8 & 6 & 7 & 0.954 & -1.353 \\
  \hline
  С tfidf 7 пр. & 8 & 5 & 5 & 0.942 & -0.102 \\
  \hline
  С tfidf 8 пр. & 6 & 7 & 7 & 1.757 & -1.222 \\
  \hline
  С tfidf 9 пр. & 8 & 6 & 7 & -0.449 & -0.365 \\
  \hline
  С tfidf 10 пр. & 8 & 5 & 6 & -0.184 & -1.826 \\
  \hline
  С tfidf 10 пр. с add. & 8 & 5 & 6 & 0.385 & -1.165 \\
  \hline
\end{longtable}

По ним можно сказать, что наилучшим качеством для тематического моделирования
обладает подготовка данных с удалением низкочастотных слов, но без удаления
стоп"=слов с помощью метрики tfidf. Объясняться это может следующим:

\begin{enumerate}
	\item Подбор гиперпараметров прошёл недостаточно полно, что
	не позволило в полной мере выбрать оптимальные гиперпараметры;
	\item Рассмотренно недостаточно вариантов подготовки данных
	(из"=за ограниченности времени не были рассмотрены варианты с
	tfidf удалением стоп"=слов и удалением низкочастотных слов);
	\item Удаление стоп"=слов с помощью метрики tfidf некорректно.
\end{enumerate}

На данный момент можно сказать, что наиболее вероятны первые две причины,
для подтверждения третьей не хватает данных.

Также по результатам можно сказать, что высокий процент порога для
tfidf метрики негативно влияет на обучение. Связано это, вероятно,
с тем, что начинают удаляться уже не только стоп"=слова и порог нужно
понизить.

% Раздел "Заключение"
\conclusion


%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Листинг вебскраппера} \label{sec:01}
\lstinputlisting[
    caption={Полный код вебскраппера}
]{code/parse_news.py}

\section{Листинг обработчика новостного массива} \label{sec:02}
\lstinputlisting[
    caption={Полный код подготовки новостного массива}
]{code/parse_news.py}

\section{Количественные характеристики подготовленного и неподготовленного
новостного массива} \label{sec:03}

\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF-IDF 1\%}} & 
\rotatebox{90}{\textbf{TF-IDF 2\%}} & 
\rotatebox{90}{\textbf{TF-IDF 3\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF-IDF 1\%}} & 
\rotatebox{90}{\textbf{TF-IDF 2\%}} & 
\rotatebox{90}{\textbf{TF-IDF 3\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 12131111 & 6545045 & - & 6479545 & 6414045 & 6348544 \\
\hline
Кол. уник. ток. & 278724 & 148677 & - & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 47 & 31 & - & 31 & 31 & 30 \\
\hline
Среднее кол. ток. в док. & 695 & 375 & - & 371 & 367 & 364 \\
\hline
Медианное кол. ток. в док. & - & 313 & - & 312 & 310 & 309 \\
\hline
Макс. кол. ток. в док. & 6514 & 3151 & - & 2903 & 2825 & 2766 \\
\hline
Мин. кол. уник. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 39 & 27 & - & 27 & 27 & 30 \\
\hline
Сред. кол. уник. ток. в док. & 346 & 214 & - & 211 & 208 & 205 \\
\hline
Мед. кол. уник. ток. в док. & - & 186 & - & 185 & 183 & 182 \\
\hline
Макс. кол. уник. ток. в док. & 2287 & 1353 & - & 1299 & 1262 & 1214 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 4\%}} & 
\rotatebox{90}{\textbf{TF-IDF 5\%}} & 
\rotatebox{90}{\textbf{TF-IDF 6\%.}} & 
\rotatebox{90}{\textbf{TF-IDF 7\%}} & 
\rotatebox{90}{\textbf{TF-IDF 8\%}} & 
\rotatebox{90}{\textbf{TF-IDF 9\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 4\%}} & 
\rotatebox{90}{\textbf{TF-IDF 5\%}} & 
\rotatebox{90}{\textbf{TF-IDF 6\%}} & 
\rotatebox{90}{\textbf{TF-IDF 7\%}} & 
\rotatebox{90}{\textbf{TF-IDF 8\%}} & 
\rotatebox{90}{\textbf{TF-IDF 9\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 6283046 & 6217544 & 6152044 & 6086544 & 6021044 & 5955543 \\
\hline
Кол. уник. ток. & 148677 & 148677 & 148677 & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 30 & 30 & 30 & 30 & 29 & 29 \\
\hline
Среднее кол. ток. в док. & 360 & 356 & 352 & 349 & 345 & 341 \\
\hline
Медианное кол. ток. в док. & 307 & 306 & 305 & 303 & 301 & 299 \\
\hline
Макс. кол. ток. в док. & 2713 & 2662 & 2595 & 2545 & 2501 & 2424 \\
\hline
Мин. кол. уник. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 27 & 29 & 29 & 28 & 28 & 28 \\
\hline
Сред. кол. уник. ток. в док. & 201 & 198 & 195 & 192 & 189 & 186 \\
\hline
Мед. кол. уник. ток. в док. & 181 & 179 & 177 & 176 & 174 & 172 \\
\hline
Макс. кол. уник. ток. в док. & 1164 & 1122 & 1085 & 1047 & 1018 & 986 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{3}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 10\%}} & 
\rotatebox{90}{\textbf{TF-IDF 10\% + Низк.}} \\
\hline
\endfirsthead

\multicolumn{3}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF-IDF 10\%}} & 
\rotatebox{90}{\textbf{TF-IDF 10\% + Низк.}} \\
\hline
\endhead

\hline
\multicolumn{3}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 \\
\hline
Кол. токенов & 5890042 & - \\
\hline
Кол. уник. ток. & 148677 & - \\
\hline
Мин. кол. ток. в док. & 4 & - \\
\hline
Модальное кол. ток. в док. & 30 & - \\
\hline
Среднее кол. ток. в док. & 337 & - \\
\hline
Медианное кол. ток. в док. & 297 & - \\
\hline
Макс. кол. ток. в док. & 2391 & - \\
\hline
Мин. кол. уник. ток. в док. & 4 & - \\
\hline
Мод. кол. уник. ток. в док. & 28 & - \\
\hline
Сред. кол. уник. ток. в док. & 182 & - \\
\hline
Мед. кол. уник. ток. в док. & 170 & - \\
\hline
Макс. кол. уник. ток. в док. & 946 & - \\
\hline
\end{longtable}

\section{Полный код класса My\_BigARTM\_model} \label{sec:04}
\lstinputlisting[
    caption={Полный код класса My\_BigRTM\_model}
]{code/My_BigARTM_model.py}

\section{Полный код класса Hyperparameter\_optimizer} \label{sec:05}
\lstinputlisting[
    caption={Полный код класса Hyperparameter\_optimizer}
]{code/hyperparameter_optimizer.py}


\end{document}
