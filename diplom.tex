\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{longtable}  % Для многостраничных таблиц
\usepackage{array}      % Для дополнительного форматирования столбцов
\usepackage{booktabs}   % Для улучшенного оформления линий

\usepackage{xltabular}  % Добавьте в преамбулу
\usepackage{booktabs}   % Для улучшенного стиля линий
\usepackage{array}      % Для управления выравниванием

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
В настоящее время обработка больших объёмов текстовых данных, включа
новостные потоки, становится критически важной задачей. Как в научной
среде, так и в бизнесе требуется оперативно анализировать информацию,
отслеживать тенденции и принимать решения. Однако анализ всего массива
данных невозможен из"=за его масштабов. Необходимо фильтровать информацию,
оставляя только релевантную.

Решением этой проблемы может стать тематическая классификация. Хотя многие
сайты и порталы предлагают рубрикацию контента, её точность часто оказывается
низкой: теги присваиваются некорректно или поверхностно. Это приводит к
ошибкам в поиске и анализе информации.

Для устранения этих недостатков необходим механизм, обеспечивающий точную
тематическую классификацию данных с возможностью автоматической разметки
новостей. Одним из инструментов для реализации такого подхода являются
тематические модели в сочетании с алгоритмами машинного и глубокого обучения.
Первые позволяют выявить скрытые темы в текстовых данных и подготовить
разметку для обучения вторых. Алгоритмы машинного и глубокого обучения, в
свою очередь, могут классифицировать новые тексты по заданным темам.

Таким образом, целью данной работы является создание механизма автоматической
тематической классификации новостей с использованием методов тематического
моделирования, машинного и глубокого обучения.

Для достижения цели необходимо решить следующие задачи:
\begin{enumerate}
    \item Реализовать сбор новостных данных;
    \item Разработать механизм предобработки текстовых данных;
    \item Вычислить количественные характеристи данных и провести их анализ;
    \item Построить тематические модели;
    \item Выбрать оптимальную тематическую модель с помощью сравнительного
    анализа;
    \item Подготовить размеченные данные для обучения моделей;
    \item Обучить и сравнить эффективность различных моделей машинного и
    глубокого обучения;
    \item Провести анализ полученных результатов.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Теоретические и методологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Место автоматической классификации новостей в разведывательном
поиске}
Разведывательный поиск "--- это процесс сбора, анализа и интерпретации
информации из открытых источников для поддержки принятия решений в
различных сферах: от бизнеса до государственного управления. В условиях
информационной перегрузки автоматическая классификация новостей становится
ключевым инструментом, обеспечивающим структуризацию и фильтрацию данных.
Её задача — преобразовать неупорядоченные массивы текстов в
категоризированные наборы, которые могут быть эффективно использованы для
дальнейшего анализа.

Интеграция в процесс разведывательного поиска:

\begin{enumerate}
    \item Сбор данных: новостные потоки формируют основу для
    разведывательного поиска. Однако их объёмы и разнообразие
    форматов затрудняют ручную обработку;
    \item Предварительная обработка: автоматическая классификация
    группирует статьи по темам, геолокациям, уровням важности или
    эмоциональной окраске, сокращая время на первичный анализ;
    \item Целевой анализ: категоризированные данные позволяют
    экспертам фокусироваться на конкретных аспектах "--- например,
    отслеживать кризисные события или выявлять скрытые тенденции.
\end{enumerate}


Практическая значимость:

\begin{enumerate}
    \item Скорость обработки: ручная классификация тысяч новостных
    статей в день невозможна. Алгоритмы на базе BigARTM, машинного и
    глубокого обучения справляются с этим за минуты, обеспечивая
    актуальность данных для принятия решений;
    \item Масштабируемость: автоматизация позволяет работать с
    постоянно растущими объёмами информации без увеличения
    ресурсных затрат;
    \item Снижение субъективности: исключаются человеческие ошибки,
    связанные с усталостью или предвзятостью, что повышает достоверность
    результатов;
    \item Выявление скрытых паттернов: методы машинного обучения
    обнаруживают неочевидные связи между событиями, например,
    корреляцию между экономическими новостями и колебаниями рынка.
\end{enumerate}

Автоматическая классификация новостей не заменяет экспертов, но
становится их основным помощником, беря на себя рутинные задачи.
В разведывательном поиске это критически важно, так как позволяет
перейти от обработки данных к их осмысленному использованию "--- будь
то стратегическое планирование или оперативное управление.
Технологии вроде BigARTM и методов машинного обучения обеспечивают
баланс между скоростью, точностью и адаптивностью, что делает их
незаменимыми в работе с динамичными новостными потоками.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Сбор новостных данных данных}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор метода получения новостных данных}
Для получения данных с сайтов существует три основных метода:
\begin{itemize}
\item Ручной сбор "--- извлечение информации человеком вручную;
\item Запрос данных "--- получение информации от владельцев с последующим
скачиванием;
\item Программный сбор "--- автоматизированное извлечение данных.
\end{itemize}

Первый метод можно исключить из рассмотрения из"=за низкой эффективности.
Второй метод применим не во всех случаях: владельцы информационных платформ
вряд ли будут оперативно предоставлять данные по каждому запросу. Таким
образом, наиболее целесообразным остаётся третий метод "--- программный сбор.

Среди методов программного сбора оперативно и эффективно получать данные
в большинстве случаев позволяют инструменты веб"=скрапинга, который мы
выбираем в качестве основного подхода. Далее в работе будет использован
именно этот метод для формирования новостного массива, так как он
прост в изученни, а также обеспечивает баланс между скоростью получения
данных и минимальными требованиями к стороннему участию.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор новостной платформы для сбора данных}
В рамках данной работы основным объектом исследования являются новостные
текстовые данные. Для их сбора необходимо выбрать подходящий веб"=ресурс.

При наличии нескольких потенциальных источников выбор следует осуществлять
по следующим критериям:
\begin{enumerate}
    \item Единая структура документов на всём сайте;
    \item Отсутствие блокировок HTTP"=запросов от скраперов;
    \item Статичность контента "--- полная доступность HTML"=кода страницы при
    первичном запросе без динамической подгрузки.
\end{enumerate}

Идеальный случай "--- соответствие всем трём пунктам. При этом:

\begin{enumerate}
    \item Ограничения по пунктам 2 и 3 в большинстве случаев можно обойти
    стандартными методами;
    \item Нарушение пункта 1 создаёт принципиальные сложности: обработка
    разноформатных данных может потребовать ручной настройки для каждого
    документа.
\end{enumerate}

В качестве источника выбран новостной сайт НИУ ВШЭ. Этот ресурс:

\begin{enumerate}
    \item Имеет единую структуру новостных материалов;
    \item Не блокирует автоматизированные запросы;
    \item Предоставляет полный HTML"=код страницы без динамической генерации
    контента.
\end{enumerate}

Указанные характеристики делают сайт ВШЭ оптимальным вариантом для реализации
поставленных задач.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка собранных данных}  
Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы предобработки включают:

\begin{enumerate}
    \item \textbf{Очистка от технического шума:}
    \begin{itemize}
        \item Удаление лишних пробелов и переносов строк;
        \item Очистка от специальных символов (скобки, HTML"=теги, эмодзи);
        \item Нормализация регистра (приведение текста к нижнему регистру).
    \end{itemize}

    \item \textbf{Токенизация:} разделение текста на семантические единицы
    (слова, предложения);  

    \item \textbf{Лемматизация:} приведение словоформ к лемме (словарной форме);  

    \item \textbf{Удаление стоп"=слов:} исключение частотных слов с низкой
    смысловой нагрузкой (предлоги, союзы, частицы);  
\end{enumerate}

\textbf{Обоснование выбора лемматизации:}
В отличие от стемминга (например, алгоритм Snowball), который применяет
шаблонное усечение окончаний, лемматизация обеспечивает точное приведение
слов к нормальной форме с сохранением семантики. Это критически важно для
тематического моделирования, где искажение смысла слов может привести к
некорректной интерпретации контекста. На рис.~\ref{fig:03} показаны
принципиальные различия между двумя подходами.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/different_stem_and_lem.png}
	\caption{\label{fig:03}%
	Иллюстрация разницы между стеммингом и лемматизацией}
\end{figure}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных,
который позволяет выявить семантические структуры в коллекциях документов.

Основная идея тематического моделирования заключается в том, что
слова в тексте связаны не с конкретным документом, а с темами. Сначала текст
разбивается на темы, и каждая из них генерирует слова для соответствующих
позиций в документе. Таким образом, сначала формируется тема, а затем тема
формирует терм.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и взаимовстречаемости слов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Ниже приведены
основные количественные характеристики, использующиеся при тематическом
моделировании:

\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и
аксиомы:

\begin{itemize}
    \item независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом:

\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируется
    терм $p(w|d, t)$.
\end{enumerate}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_1.png}
	\caption{\label{fig:2}%
	Алгоритм формирования документа}
\end{figure}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:

\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного
разложения~\ref{fig:1}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/topic_modeling_task_2.png}
	\caption{\label{fig:1}%
	Стохастическое матричное разложение}
\end{figure}

Таким образом, тематическое моделирование ищет величину $p(w|d)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Решение обратной задачи}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Лемма о максимизации функции на единичных симплексах:} \label{sec:06}
Перед тем как перейти к решению обратной задачи, сформулируем лемму,
которая поможет в этом процессе.

Введём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}(x_i) =
    \frac{\max{x_i, 0}}{\sum_{k \in I} \max{x_k, 0}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных симплексах:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Сведение обратной задачи к максимизации функционала:}
Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия, согласно которому будут подобраны параметры
$\Phi, \Theta$ такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} = n_{dw} \to max
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation} \label{eq:02}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation} \label{eq:03}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
	\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

Таким образом, обратная задача сводится к задаче максимизации
функционала.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Аддитивная регуляризация тематических моделей:}
Задача~\ref{eq:02} не соответствует критериям корректно поставленной задаче
по Адамару, поскольку в общем случае она имеет бесконечное множество решений.
Это свидетельствует о необходимости доопределения задачи. 

Для доопределения некорректно поставленных задач применяется регуляризация: к
основному критерию добавляется дополнительный критерий "--- регуляризатор,
который соответствует специфике решаемой задачи. 

Метод ARTM (аддитивная регуляризация тематических моделей)
основывается на максимизации линейной комбинации логарифма правдоподобия и
регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами
регуляризации $\tau_i, \;\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки~\ref{eq:03}.

Регуляризатор (или набор регуляризаторов) выбирается в соответствии с решаемой
задачей.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{E"=M алгоритм:}
Из представленных выше ограничений~\ref{eq:03} следует, что столбцы матриц можно
считать неотрицательными единичными векторами. Таким образом, задача сводится к
максимизации функции на единичных симплексах.

Воспользуемся леммой о максимизации функции на единичных
симплексах~\ref{sec:06} и перепишем задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений с вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation} \label{eq:04}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E"=шагу, а вторая и третья строки "--- M"=шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные варианты регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Дивергенция Кульбака"=Лейблера:} \label{sec:07}
Перед тем как перейти к регуляризаторам необходимо ввести меру оценки близости
тем.

Чтобы оценить близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера (KL или KL"=дивергенция). KL"=дивергенция
позволяет оценить степень вложенности одного распределения в другое, в случае
тематического моделирования будет оценитьваться вложенность матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$ \label{it:kl3}
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

Теперь можно перейти к рассмотрению регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор сглаживания:}
Сглаживание предполагает сематническое сближение тем, это может быть полезно в
следующих случаях:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:05}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм соответствующий модели
LDA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор разреживания:}
Разреживание подразумевает разделение тем и документов, исключая общие слова из
них. Этот тип регуляризации основывается на предположении, что темы и документы
в основном являются специфичными и описываются относительно небольшим набором
терминов, которые не встречаются в других темах.

Определим регуялризатор разреживания:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:07} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:06}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, разреживающий
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Регуляризатор декоррелирования тем:}
Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем:

Определим регуляризатор декоррелирования:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:04} в соответствии с полученной формулой:
\begin{equation} \label{eq:07}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{t \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, декоррелирующий
темы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Оценка качества моделей}
После построения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических
моделей:

\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item полнота и точность тематического поиска;
        \item качество ранжирования при тематическом поиске;
        \item качество классификации / категоризации документов;
        \item качество суммаризации / сегментации документов;
        \item экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item правдоподобие и перплексия;
        \item средняя когерентность (согласованность тем);
        \item разреженность матриц $\Phi$ и $\Theta$;
        \item различность тем;
        \item статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Поскольку оценка по внешним критериям невозможна в рамках данной работы,
сосредоточимся на внутренних критериях оценки, которые можно вычислять
автоматически.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Правдоподобие и перплексия:}
Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что при равномерном распределении слов в тексте выполняется
равенство $p(w|d) = \frac{1}{|W|}$. В этом случае значение перплексии равно
мощности словаря $P = |W|$. Это позволяет сделать вывод, что перплексия является
мерой разнообразия и неопределенности слов в тексте: чем меньше значение
перплексии, тем более разнообразны вероятности появления слов.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Когерентность:}
Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PNI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$, $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация,
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно), $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости,
тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Разреженность:}
Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и
$\Theta$.

Разреженность играет ключевую роль в выявлении различий между темами.
Каждая тема формируется на основе ограниченного набора слов, в то время как
остальные слова должны встречаться реже, что отражается в нулевых элементах
матриц. Оптимальный уровень разреженности должен быть высоким, но не чрезмерным:
в таком случае темы будут четко различимы. Если разреженность слишком низка,
темы могут сливаться, а если слишком высока "--- содержать недостаточное
количество слов для адекватного
представления.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Чистота темы:}
Чистота темы:
\begin{equation}
	\sum_{w \in W_t} p(w|t),
\end{equation}

где $W_t$ "--- ядро темы:
$W_t = \{w: p(w|t) > \alpha\}$, где $\alpha$ подбирается по разному,
например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$.

Данная характеристика показывает как вероятностно относится ядро темы к фоновым
словам темы, следовательно, чем больше вероятность ядра, тем
лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Контрастность темы:}
Контрастность темы:
\begin{equation}
	\frac{1}{|W_T|} \sum_{w \in W_t} p(t|w).
\end{equation}

Данная характеристика показывает насколько часто слова из ядра темы
встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
в других темах, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Методические основы работы с текстом с помощью нейросетей}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Проблема представления текста}
Нейронные сети умеют работать только с числами, поэтому встаёт вопрос о
том, как наилучшим образом переносить текст в пространство чисел. Такой
способ переноса должен быть не только быстрым, точным и способным
вмещать в себя тысячи слов, но ещё и учитывать, что естественный язык
имеет временную зависимость: слова в предложении складываются
последовательно и зависят друг от друга, а не существуют в вакууме,
что дополнительно усложняет задачу.

Тогда формализуем качества, которыми должен обладать способ представления
текста в виде чисел:

\begin{itemize}  
    \item \textbf{Выразительность}:  
    \begin{enumerate}  
        \item Способность различать тысячи слов;  
        \item Способность учитывать контекст (временную зависимость между
        словами).  
    \end{enumerate}  
    
    \item \textbf{Скорость}: эффективно работать с высокоразмерными данными
    на современном оборудовании;  
    
    \item \textbf{Эффективным}: иметь компактное представление и
    адаптироваться к новым словам.  
\end{itemize}  

Теперь кратко рассмотрим некоторые из методов представления текста в виде
чисел:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Мешок слов (Bag"=of"=Words):}
Одним из самых простых способов численного представления текста является
мешок слов.

Данный метод работает следующим образом:

\begin{enumerate}  
    \item Создаётся словарь слов с уникальными индексами;  
    \item Каждое слово кодируется one"=hot вектором:  
    \begin{equation}  
        v_i = [a_1, \dots, a_N], \quad a_j = \begin{cases}  
            1, & j = i \\  
            0, & j \neq i  
        \end{cases}  
    \end{equation}  
    где $N$ "--- размер словаря.  
    
    \item Предложение представляется суммой векторов слов:  
    \begin{equation}  
        s = [f_1, \dots, f_N], \quad f_j = \text{частота слова } j \text{ в предложении}.  
    \end{equation}  
\end{enumerate} 

Данный метод, несмотря на свою простоту, не может быть выбран из"=за
ряда существенных недостатков:

\begin{enumerate}  
    \item Высокая размерность и разреженность данных;  
    \item Игнорирование порядка слов;  
    \item Отсутствие учёта семантики (все слова ортогональны);  
    \item Сложность адаптации к новым словам (требуется пересчёт словаря).  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{TF-IDF взвешивание:}
Улучшение BoW: элементы вектора предложения умножаются на TF"=IDF веса слов.
Частично решает проблему семантической значимости, но сохраняет другие
недостатки BoW.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Эмбеддинги слов:}
Семантические векторные представления слов:  
\begin{itemize}  
    \item Каждому слову сопоставляется плотный вектор фиксированной
    размерности (обычно 50"=300);  
    \item Векторы обучаются так, чтобы семантически близкие слова имели
    схожие эмбеддинги;  
    \item Матрица эмбеддингов "--- обучаемый параметр нейросети.  
\end{itemize}

Данный способ максимально полно соответствует описанным ранее критериям,
обладая благодаря своей природе следующими преимуществами:

\begin{enumerate}  
    \item Низкая размерность;  
    \item Учёт семантики;
    \item Возможность учёта контекста;  
    \item Гибкость: новые слова можно добавлять через дообучение.  
\end{enumerate} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор архитектуры нейронной сети}  

Так как представление текста в виде эмбеддингов удовлетворяет критериям то,
будем рассматривать архитектуры, разработанные для работы с ними:
рекуррентные нейронные сети (RNN) и трансформеры.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Рекуррентные нейронные сети (RNN)}  
Данные сети обрабатывают последовательность слов рекуррентно, шаг за шагом
обновляя своё состояние на основе текущего слова и предыдущих значений.
Это позволяет учитывать:

\begin{itemize}  
    \item Порядок слов;  
    \item Контекст (благодаря механизмам памяти в LSTM/GRU).  
\end{itemize}  

Недостатки:

\begin{enumerate}  
    \item Низкая скорость: вычисления последовательны, невозможна
    параллелизация;  
    \item Проблемы с длинными последовательностями:  
    \begin{enumerate}  
        \item Забывание раннего контекста;  
        \item Затухание/взрыв градиентов при обучении.  
    \end{enumerate}  
\end{enumerate}  

Преимущества:

\begin{enumerate}  
    \item Менее требовательны к вычислительным ресурсам;  
    \item Эффективны на малых объёмах данных.  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Трансформеры}  
Обрабатывают всю последовательность слов одновременно благодаря
механизму внимания (attention).

Ключевые особенности:

\begin{itemize}  
    \item Параллельные вычисления, а следовательно и высокая скорость;  
    \item Учёт контекста через self"=attention;  
    \item Позиционные энкодинги позволяют учитывать порядок слов.  
\end{itemize}  

Недостатки:

\begin{enumerate}  
    \item Высокие требования к вычислительным ресурсам;  
    \item Требуют больших объёмов данных для обучения.  
\end{enumerate}  

Преимущества:

\begin{enumerate}  
    \item Эффективны для длинных текстов;  
    \item Имеют лучшее качество на сложных задачах.  
\end{enumerate}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Определение с типом}
В рамках данной работы рассматривается тематическая классификация текстов,
то есть предполагается, что по длинной входящей последовательности
принимается решение о её принадлжности к той или иной теме.

Тогда для данной задачи критичны:

\begin{enumerate}  
    \item Обработка длинных последовательностей;  
    \item Скорость предсказания;  
    \item Использование современных вычислительных ресурсов.  
\end{enumerate}  

Таким образом, для решения поставленной задачи больше подходят
сети"=трансформеры, так как:

\begin{itemize}  
    \item Проблемы с ресурсами решаются облачными сервисами;  
    \item Доступны предобученные модели (BERT, GPT);  
    \item Механизм внимания лучше улавливает тематические связи.  
\end{itemize} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Практико"=технологические основы автоматической тематической
классификации}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Получение новостного массива путём веб"=скраппинга}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов получения новостных данных}
Для веб"=скрапинга доступны библиотеки на разных языках, однако выбор логично
сделать в пользу Python "--- наиболее популярного языка для обработки данных
и работы с машинным обучением. Среди Python"=библиотек ключевыми являются:

\begin{itemize}
    \item requests "--- для отправки HTTP"=запросов;
    \item BeautifulSoup4 "--- для парсинга HTML"=кода в удобную объектную
    структуру;
    \item selenium "--- для работы с динамическими сайтами, где контент
    генерируется JavaScript.
\end{itemize}

Первые две библиотеки эффективны для статических страниц: requests получает
исходный код, а BeautifulSoup4 извлекает данные через поиск по тегам.
Selenium же имитирует взаимодействие реального браузера, что позволяет
обрабатывать страницы с отложенной загрузкой контента.

Этот набор инструментов покрывает потребности работы с подавляющим
большинством сайтов "--- от простых статических ресурсов до сложных
веб"=приложений.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Реализация алгоритма сбора новостных данных}
библиотек requests и BeautifulSoup4 без привлечения Selenium.

Алгоритм сбора данных включает следующие этапы:

\begin{enumerate}
    \item Анализ структуры сайта:
    \begin{itemize}
        \item Многостраничный ресурс с 10 новостными карточками на каждой
        странице;
        \item Карточка новости содержит: ссылку, дату публикации, заголовок,
        краткое содержание;
        \item Полный текст доступен по отдельной ссылке внутри карточки.
    \end{itemize}

    \item Реализация базовых функций (листинг~\ref{lst:01}):
    \begin{itemize}
        \item Получение HTML"=кода страницы через requests.get();
        \item Сохранение сырых данных для последующей обработки.
    \end{itemize}

    \lstinputlisting[
    caption={Функция получения HTML"=кода страницы},
    label={lst:01}
    ]{code/get_page_function.py}

    \item Извлечение метаданных (листинг~\ref{lst:02}):
    \begin{itemize}
        \item Парсинг сохранённого HTML через BeautifulSoup4;
        \item Поиск элементов по тегам и CSS"=классам (find(), find\_all());
        \item Извлечение текстового содержимого (text, get()).
    \end{itemize}

    \lstinputlisting[
    caption={Извлечение ссылок и кратких описаний},
    label={lst:02}
    ]{code/get_link_and_summary.py}

    \item Получение полного текста новости (листинг~\ref{lst:03}):
    \begin{itemize}
        \item Рекурсивное использование get\_page() для целевых URL;
        \item Анализ структуры контентной страницы.
    \end{itemize}

    \lstinputlisting[
    caption={Функция извлечения полного текста новости},
    label={lst:03}
    ]{code/get_news_content.py}

    \item Обработка страницы целиком (листинг~\ref{lst:04}):
    \begin{itemize}
        \item Итерация по 10 элементам div.post на странице;
        \item Использование find\_next\_sibling() для навигации;
        \item Сохранение результатов в pandas DataFrame для анализа.
    \end{itemize}

    \lstinputlisting[
    caption={Обработка новостной страницы},
    label={lst:04}
    ]{code/get_one_news_page.py}

    \item Масштабирование на все страницы (листинг~\ref{lst:05}):
    \begin{itemize}
        \item Динамическое формирование URL через модификацию параметров;
        \item Пакетная обработка через цикл с изменяемым индексом страницы.
    \end{itemize}

    \lstinputlisting[
    caption={Функция обработки всего архива новостей},
    label={lst:05}
    ]{code/crawling_pages_function.py}

    \item Оптимизация производительности (листинг~\ref{lst:06}):
    \begin{itemize}
        \item Реализация многопоточности через стандартные средства Python;
        \item Создание изолированных DataFrame для каждого потока;
        \item Агрегация результатов после завершения параллельных задач.
    \end{itemize}

    \lstinputlisting[
    caption={Многопоточная реализация парсера},
    label={lst:06}
    ]{code/multithreading_apply_function.py}
\end{enumerate}

Полная реализация веб"=скрапера доступна в приложении~\ref{sec:01}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Подготовка новостного массива}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов для подготовки данных}  
Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов. Определение стоп"=слов в обоих случаях происходит по
заранее заданным словарям, поэтому разницы здесь нет. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает её использование.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по
предопределённому списку, который не является исчерпывающим. Это связано с
тем, что набор стоп"=слов зависит от тематики текста, и универсального
решения не существует. Для дополнительной фильтрации применим метрику
TF"=IDF, которая оценивает значимость слов. Формула расчёта:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа $d$. Её
расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп"=слова) и Gensim (расширенная
фильтрация через TF"=IDF).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление лишних пробелов и переносов строк}
Для корректной токенизации и анализа текстовых данных требуется
предварительная очистка от лишних пробелов и переносов строк.
Реализацию этой процедуры можно выполнить с помощью встроенных
методов обработки строк в Python.

Алгоритм функции включает три этапа:
\begin{enumerate}
    \item \textbf{Копирование значимых символов:}
    Посимвольное добавление содержимого исходной строки в результирующий
    буфер до обнаружения пробела или переноса строки.

    \item \textbf{Нормализация пробелов:} 
    При обнаружении пробела/переноса:
    \begin{itemize}
        \item Добавление одного пробела в буфер
        \item Пропуск всех последующих пробелов/переносов до первого
        непробельного символа
    \end{itemize}

    \item \textbf{Циклическая обработка:} 
    Повтор шагов 1"=2 до полного прохода исходной строки.
\end{enumerate}

Реализация функции представлена в листинге~\ref{lst:07}:

\lstinputlisting[
caption={Функция нормализации пробелов и переносов строк},
label={lst:07}
]{code/remove_extra_spaces_and_line_breaks.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разделение строк на русские и английские фрагменты}
Библиотека SpaCy использует предобученные языковые модели, каждая из
которых оптимизирована для обработки одного языка (например, отдельно
для русского и английского).

Для новостных материалов ВШЭ, содержащих смешанные языковые фрагменты,
применение единой модели недопустимо. Решение заключается в предварительном
разделении текста на русскоязычные и англоязычные сегменты с последующей
обработкой соответствующими моделями.

Алгоритм разделения текста:
\begin{enumerate}
    \item \textbf{Инициализация языка:}
    \begin{itemize}
        \item Определение языка первого буквенного символа строки
        \item Установка текущего языкового идентификатора (RU/EN)
    \end{itemize}

    \item \textbf{Построение сегментов:}
    \begin{itemize}
        \item Посимвольное накопление символов во временном буфере
        \item Прерывание потока при обнаружении символа другого языка
    \end{itemize}

    \item \textbf{Сохранение результата:}
    \begin{itemize}
        \item Фиксация сегмента в формате (язык, текст)
        \item Сброс временного буфера
    \end{itemize}

    \item \textbf{Циклическое выполнение:}
    Повтор шагов 2"=3 до полной обработки строки с автоматическим переключением
    языкового идентификатора.
\end{enumerate}

Реализация функции представлена в листинге~\ref{lst:08}:

\lstinputlisting[
caption={Функция разделения текста на русско- и англоязычные фрагменты},
label={lst:08}
]{code/split_into_en_and_ru.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Обработка двоеточий и временных меток}
Библиотека BigARTM интерпретирует двоеточие как служебный символ, что
может привести к ошибкам обработки текстовых данных. Для устранения
проблемы требуется предварительная нормализация символа.

Стратегия обработки:

\begin{enumerate}
    \item Сохранение смысла в временных обозначениях: замена шаблонов
    времени (например, "12:30") на текстовый маркер "time";
    \item Удаление избыточных символов: устранение всех других
    двоеточий, не входящих в временные конструкции
\end{enumerate}

Алгоритм реализует контекстно"=зависимую обработку: анализ окружения
символа определяет его замену или удаление.

Реализация функции приведена в листинге~\ref{lst:09}:

\lstinputlisting[
caption={Функция нормализации двоеточий в тексте},
label={lst:09}
]{code/processing_token.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Токенизация, лемматизация и удаление стоп"=слов по словарю}
Библиотека SpaCy предоставляет унифицированный интерфейс для лингвистической
обработки текста. Её функционал позволяет выполнять в одном конвейере:

\begin{itemize}
    \item Токенизацию;
    \item Лемматизацию;
    \item Идентификацию стоп-слов
\end{itemize}

Принцип работы:

\begin{enumerate}
    \item На вход подаётся текстовая строка;
    \item Обработанные данные возвращаются в виде последовательности токенов;
    \item Каждый токен содержит:
    \begin{itemize}
        \item Исходную словоформу;
        \item Нормализованную лемму;
        \item Флаг принадлежности к стоп"=словам
    \end{itemize}
\end{enumerate}

Результирующая строка формируется путём фильтрации: сохраняются только
леммы токенов, не отнесённых к стоп"=словам.

Пример обработки русскоязычного текста показан в листинге~\ref{lst:10}:

\lstinputlisting[
caption={Обработка строки русского языка средствами SpaCy},
label={lst:10}
]{code/apply_spacy_for_one_str.py}

Полный алгоритм предобработки, объединяющий нормализацию пробелов,
токенизацию и фильтрацию, реализован в листинге~\ref{lst:11}:

\lstinputlisting[
caption={Комплексная обработка текста: нормализация, токенизация,
лемматизация, фильтрация стоп"=слов},
label={lst:11}
]{code/tokenize_lemmatize_and_del_stop_words.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление стоп"=слов с помощью метрики tfidf}
Как отмечалось ранее, удаление стоп"=слов исключительно по предзаданному
словарю имеет ограниченную эффективность. Для повышения качества фильтрации
предлагается дополнительное использование метрики TF"=IDF, позволяющей
оценивать значимость терминов в корпусе документов.

Алгоритм расширенной фильтрации:

\begin{enumerate}
    \item Вычисление TF"=IDF:
    \begin{enumerate}
        \item Формирование словаря терминов с помощью Gensim;
        \item Построение частотного корпуса документов;
        \item Расчёт весов TF"=IDF для каждого термина
    \end{enumerate}

    Реализация базового расчёта представлена в листинге~\ref{lst:12}:

    \lstinputlisting[
    caption={Вычисление TF"=IDF метрик для текстового корпуса},
    label={lst:12}
    ]{code/calc_dict_corpus_and_tfidf.py}

    \item Коррекция словаря:
    \begin{enumerate}
        \item Добавление терминов с нулевым TF"=IDF, исключённых Gensim
        по умолчанию;
        \item Нормализация структуры данных для последующего анализа;
    \end{enumerate}

    Соответствующая доработка реализована в листинге~\ref{lst:13}:

    \lstinputlisting[
    caption={Дополнение словаря нулевыми TF"=IDF значениями},
    label={lst:13}
    ]{code/add_missing_tfidf_words.py}

    \item Определение порога отсечения:
    \begin{enumerate}
        \item Вычисление n"=го процентиля распределения TF"=IDF;
        \item Установка границы для отбора малозначимых терминов;
    \end{enumerate}

    Логика расчёта границы показана в листинге~\ref{lst:14}:
    
    \lstinputlisting[
    caption={Определение порогового значения TF"=IDF},
    label={lst:14}
    ]{code/add_missing_tfidf_words.py}

    \item Фильтрация датасета:
    \begin{enumerate}
        \item Итеративное удаление терминов с TF'=IDF ниже порога;
        \item Дополнительная очистка низкочастотных слов (менее k вхождений);
    \end{enumerate}

    Финальный этап обработки представлен в листинге~\ref{lst:15}:

    \lstinputlisting[
    caption={Удаление стоп"=слов на основе TF"=IDF метрики},
    label={lst:15}
    ]{code/del_tfidf_stop_words.py}
\end{enumerate}

Полная реализация обработчика данных доступна в приложении~\ref{sec:02}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Количественные характеристики обработанного и необработанного
датасета}
В ходе исследования выполнена обработка новостного массива с вариацией
параметров, включая:

\begin{enumerate}
    \item Пороговые значения TF"=IDF;
    \item Комбинации методов предобработки.
\end{enumerate}

Количественные характеристики результатов представлены в таблицах
приложения~\ref{sec:03}.

Ключевые наблюдения:

\begin{enumerate}
    \item Объём документов: медианное количество токенов на документ
    составляет 305, что свидетельствует о содержательной насыщенности
    материалов;
    \item Эффективность фильтрации:
    \begin{itemize}
        \item Частота наиболее распространённого слова снизилась с 800,000+
        до 50,000 вхождений;
        \item Количество уникальных токенов сократилось на 50
        процентов (рис.~\ref{fig:01}"=\ref{fig:02}).
    \end{itemize}
\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics{./images/zips_law_for_not_preprocessed_data.png}
\caption{\label{fig:01}%
Распределение частот слов по закону Ципфа (исходные данные)}
\end{figure}
\newpage

\begin{figure}[!ht]
\centering
\includegraphics{./images/zips_law_for_preprocessed_data.png}
\caption{\label{fig:02}%
Распределение частот слов по закону Ципфа (обработанные данные)}
\end{figure}

Проблемные аспекты:

\begin{enumerate}
    \item Сохранение высокого числа уникальных токенов (?45 процентов от
    исходного);
    \item Наличие шумовых компонентов:
    \begin{itemize}
        \item Опечатки;
        \item Ненормализованные словоформы;
        \item Специфические аббревиатуры.
    \end{itemize}
\end{enumerate}

Указанные факторы могут негативно влиять на качество:

\begin{itemize}
    \item Тематического моделирования;
    \item Обучения ML"=алгоритмов;
    \item Интерпретации результатов.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Вычисление тематической модели}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор инструментов для тематического моделирования}
При разработке системы автоматической классификации новостей выбор
инструментов напрямую влияет на гибкость, скорость и качество модели.
Библиотека BigARTM (Additive Regularization of Topic Models) была выбрана
по нескольким ключевым критериям, которые делают её предпочтительной на
фоне альтернатив, таких как Gensim или Mallet.

Критерии выбора:

\begin{enumerate}
    \item Удобный интерфейс: BigARTM предоставляет простой API для
    работы с тематическими моделями, что ускоряет интеграцию в существующие
    пайплайны обработки текстов. Например, загрузка данных, настройка
    параметров и запуск обучения выполняются минимальным количеством кода,
    снижая риск ошибок и время на разработку;
    \item Разнообразие регуляризаторов: библиотека поддерживает множество
    регуляляризаторов (например, сглаживание, разреживание тем),
    которые можно комбинировать для улучшения интерпретируемости и
    точности модели. Это критически важно для новостных данных, где
    темы часто пересекаются (например, «экономика» и «политика»), а
    шумовые слова требуют фильтрации;
    \item Блочный синтаксис: настройка модели в BigARTM осуществляется
    через декларативное описание компонентов (блоков), что упрощает
    эксперименты с архитектурой. Например, можно быстро добавить
    регуляризатор для контроля за размером тем или подключить модуль
    для обработки мультимодальных данных;
    \item Доступность туториалов: BigARTM имеет подробную документацию
    и примеры использования, включая готовые сценарии для классификации
    текстов. Это сокращает время на изучение библиотеки и позволяет
    сосредоточиться на решении прикладных задач.
\end{enumerate}

BigARTM сочетает в себе специализацию для работы с текстами, гибкость
настройки и низкий порог входа благодаря понятному синтаксису. Это делает
её оптимальным выбором для задач автоматической классификации новостей,
где важно быстро адаптировать модель под изменяющиеся условия (например,
появление новых тем) и контролировать качество результатов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Недостающий функционал библиотеки BigARTM}
Тематическое моделирование с использованием библиотеки BigARTM обладает
практической ценностью, но имеет ряд ограничений:

\begin{enumerate}
\item Отсутствие встроенной метрики оценки когерентности тематик;
\item Сложность интеграции регуляризаторов из"=за многоэтапного API;
\item Трудоёмкое преобразование данных в требуемый формат представления;
\item Недостаток инструментов визуализации для мониторинга качества моделей;
\item Отсутствие автоматизированных методов подбора гиперпараметров.
\end{enumerate}

Наибольшее влияние на качество моделирования оказывает первый фактор.
Остальные ограничения преимущественно связаны с эргономикой рабочего
процесса, но их совокупность существенно увеличивает сложность поддержки
кодовой базы.

Для компенсации выявленных недостатков предлагается разработка двух
вспомогательных классов, расширяющих функционал библиотеки:

\begin{enumerate}
    \item Анализатор качества "--- реализация расчёта когерентности и
    визуализации метрик;
    \item Препроцессинг"=обёртка "--- автоматизация преобразования данных
    и управления регуляризацией.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Функциональности классов My\_BigARTM\_model и
\\ Hyperparameter\_optimizer}
В рамках класса My\_BigARTM\_Model целесообразно реализовать:

\begin{itemize}
    \item Расчёт метрик когерентности тематик;
    \item Упрощённый интерфейс для добавления регуляризаторов;
    \item Автоматизацию преобразования данных в требуемый формат;
    \item Визуализацию динамики метрик качества через графики.
\end{itemize}

Интеграцию функциональности по подбору гиперпараметров в данный класс
нецелесообразно, так как это:

\begin{itemize}
    \item Нарушит принцип единственной ответственности;
    \item Усложнит поддержку кодовой базы;
    \item Снизит читаемость реализации.
\end{itemize}

Для решения этих задач предложено выделение отдельного класса
Hyperparameter\_optimizer, который:

\begin{itemize}
    \item Инкапсулирует логику оптимизации;
    \item Обеспечивает удобное сохранение настроенных моделей;
    \item Поддерживает различные конфигурации предобработки данных.
\end{itemize}

Такое разделение обеспечивает модульность архитектуры и упрощает
дальнейшее расширение системы.

Следующим этапом работы является последовательная реализация обоих
компонентов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Преобразование новостного массива в приемлемый для BigARTM
формат}
Модель BigARTM поддерживает ограниченный набор форматов данных, включая
Vowpal Wabbit. Для интеграции с pandas DataFrame требуется предварительное
преобразование новостного массива, которое целесообразно реализовать
отдельной функцией.

Алгоритм преобразования:

\begin{enumerate}
    \item Извлечение строки из DataFrame;
    \item Конкатенация ячеек строки в единый текстовый блок;
    \item Запись результата в файл формата Vowpal Wabbit с меткой документа;
    \item Итеративная обработка всего массива новостей.
\end{enumerate}

Реализация функции преобразования представлена в листинге~\ref{lst:16}:

\lstinputlisting[
caption={Преобразование новостного массива в формат Vowpal Wabbit},
label={lst:16}
]{code/make_vowpal_wabbit.py}

Последующие этапы обработки:

\begin{enumerate}
    \item Разделение данных на батчи;
    \item Генерация словаря терминов.
\end{enumerate}

Оба действия выполняются средствами библиотеки BigARTM. Соответствующий
код приведён в листинге~\ref{lst:17}:

\lstinputlisting[
caption={Функция создания батчей и словаря},
label={lst:17}
]{code/make_batches.py}

Подготовленные данные готовы для передачи в модель BigARTM для тематического
моделирования.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удобное добавление регуляризаторов}
Библиотека BigARTM предоставляет обширный набор регуляризаторов,
однако их интеграция в модель требует сложного синтаксиса, что затрудняет
массовое использование. Для упрощения процесса предложен двухуровневый
подход:

\begin{enumerate}
    \item Базовая функция "--- добавляет регуляризатор по имени и значению
    гиперпараметра;
    \item Обёрточная функция "--- применяет первый метод для пакетного
    добавления.
\end{enumerate}

Преимущества решения:

\begin{itemize}
    \item Устранение необходимости работы с низкоуровневым API BigARTM;
    \item Единообразный интерфейс для одиночных и групповых операций;
    \item Повышение читаемости и поддерживаемости кода.
\end{itemize}

Фрагмент реализации базовой функции (листинг~\ref{lst:18}):

\lstinputlisting[
caption={Функция добавления одиночного регуляризатора},
label={lst:18}
]{code/add_regularizer.py}

Реализация пакетной обработки (листинг~\ref{lst:19}):

\lstinputlisting[
caption={Функция добавления набора регуляризаторов},
label={lst:19}
]{code/add_regularizers.py}

Данное решение существенно упрощает эксперименты с различными комбинациями
регуляризаторов, сохраняя при этом гибкость подхода BigARTM.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление когерентности}
Библиотека BigARTM включает набор встроенных метрик оценки качества,
однако не поддерживает расчёт когерентности "--- ключевого показателя
тематической согласованности. Для восполнения этого функционала
предлагается интеграция с библиотекой Gensim, предоставляющей
методы вычисления различных видов когерентности.

Алгоритм расчёта метрики:

\begin{enumerate}
    \item Экспорт тематических ядер:
    
    Получение списка тем, где каждая тема представлена N ключевыми терминами

    \item Подготовка текстового корпуса:
    
    Преобразование документов в структуру вида:

    [[токен\_1\_док\_1, токен\_2\_док\_1, ...], [токен\_1\_док\_2, ...], ...]

    \item Вычисление показателя:
    
    Передача данных в Gensim для расчёта выбранного типа когерентности
\end{enumerate}

Реализация функции представлена в листинге~\ref{lst:20}:

\lstinputlisting[
caption={Функция вычисления метрики когерентности},
label={lst:20}
]{code/calc_coherence.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Вычисление тематической модели и формирование графиков метрик}
Библиотека BigARTM не поддерживает мониторинг динамики метрик качества в
процессе обучения, особенно для пользовательских метрик. Для реализации
этого функционала требуется дополнительная разработка.

Алгоритм отслеживания метрик:

\begin{enumerate}
    \item Итеративное обучение модели:
    \begin{itemize}
        \item Установка num\_collection\_passes=1 для пошагового прохода;
        \item Циклическое выполнение обучения с накоплением метрик после
        каждой эпохи.
    \end{itemize}

    \item Визуализация результатов:
    \begin{itemize}
        \item Использование matplotlib для построения графиков;
        \item Унифицированный подход для различных типов метрик.
    \end{itemize}
\end{enumerate}

Реализация итеративного обучения представлена в листинге~\ref{lst:21}:

\lstinputlisting[
caption={Функция обучения модели с пошаговым расчётом метрик},
label={lst:21}
]{code/calc_model.py}

Пример визуализации для метрики когерентности (листинг~\ref{lst:22}):

\lstinputlisting[
caption={Функция построения графика динамики когерентности},
label={lst:22}
]{code/calc_coherence_graphik.py}

Для других метрик применяется аналогичная логика с заменой целевого
показателя.

Данная реализация завершает базовый функционал класса My\_BigARTM\_model.
Полный код доступен в приложении~\ref{sec:04}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подбор гиперпараметров для тематического моделирования}
Для интеллектуального подбора гиперпараметров целесообразно использовать
библиотеку Optuna, которая предоставляет:

\begin{itemize}
    \item Упрощённый API для настройки экспериментов;
    \item Поддержку байесовской оптимизации (вместо полного перебора);
    \item Автоматическое сокращение вычислительных ресурсов за счёт
    адаптивного выбора параметров.
\end{itemize}

Алгоритм работы:

\begin{enumerate}
    \item Реализация целевой функции:
    \begin{itemize}
        \item Определение пространства поиска гиперпараметров через
        trial.suggest\_int() и trial.suggest\_float();
        \item Вычисление и возврат метрик качества модели.
    \end{itemize}

    Ключевой фрагмент реализации (листинг~\ref{lst:23}):

    \lstinputlisting[
    caption={Целевая функция для оптимизации гиперпараметров},
    label={lst:23}
    ]{code/objective.py}

    \item Запуск оптимизации:
    \begin{itemize}
        \item Использование study.optimize() для выполнения экспериментов;
        \item Получение набора попыток с параметрами и метриками.
    \end{itemize}

    \item Выбор оптимальной конфигурации:
    \begin{itemize}
        \item Нормализация метрик;
        \item Выбор попытки с минимальной совокупной ошибкой.
    \end{itemize}

    Логика выбора (листинг~\ref{lst:24}):

    \lstinputlisting[
    caption={Функция выбора оптимальной конфигурации},
    label={lst:24}
    ]{code/select_best_trial.py}

    \item Финализация модели:
    \begin{itemize}
        \item Обучение на лучших гиперпараметрах;
        \item Возврат оптимизированной модели.
    \end{itemize}

    Завершающий этап (листинг~\ref{lst:25}):

    \lstinputlisting[
    caption={Обучение модели с оптимальными параметрами},
    label={lst:25}
    ]{code/optimizer.py}
\end{enumerate}

Полная реализация класса Hyperparameter\_optimizer доступна в
приложении~\ref{sec:05}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Результаты тематического моделирования}
В ходе исследования проведено тематическое моделирование 13 конфигураций
предобработанных данных. Для каждой конфигурации выполнены:

\begin{enumerate}
    \item Оптимизация гиперпараметров;
    \item Расчёт финальной модели;
    \item Оценка метрик качества.
\end{enumerate}


Результаты оценки представлены в таблице~\ref{tab:metrics}
(когерентность и перплексия) и таблице~\ref{tab:hyperparams}
(оптимальные гиперпараметры).

% Фиксированная ширина для колонки "Данные"
\newlength{\mydatalength}
\setlength{\mydatalength}{6cm}

\begin{longtable}{|p{\mydatalength}|c|c|}
  \caption{Метрики моделей} \label{tab:metrics} \\
  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline 
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{perplexity} & \textbf{coherence} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 3486 & 0.470 \\
  \hline
  Без tfidf с add. & 2974 & 0.456 \\
  \hline
  С tfidf 1 пр. & 3643 & 0.476 \\
  \hline
  С tfidf 2 пр. & 3848 & 0.479 \\
  \hline
  С tfidf 3 пр. & - & - \\
  \hline
  С tfidf 4 пр. & - & - \\
  \hline
  С tfidf 5 пр. & 4094 & 0.495 \\
  \hline
  С tfidf 6 пр. & 3982 & 0.505 \\
  \hline
  С tfidf 7 пр. & 4620 & 0.491 \\
  \hline
  С tfidf 8 пр. & 4183 & 0.514 \\
  \hline
  С tfidf 9 пр. & 3811 & 0.496 \\
  \hline
  С tfidf 10 пр. & 4022 & 0.490 \\
  \hline
  С tfidf 10 пр. с add. & 3284 & 0.486 \\
  \hline
\end{longtable}

\begin{longtable}{|p{\mydatalength}|c|c|c|c|c|}
  \caption{Гиперпараметры моделей} \label{tab:hyperparams} \\
  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endfirsthead

  \hline
  \textbf{Данные} & \textbf{topics} & \textbf{cols} & \textbf{docs} & \textbf{tau phi} & \textbf{tau theta} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  Без tfidf и add. & 8 & 6 & 7 & -1.561 & 0.809 \\
  \hline
  Без tfidf с add. & 8 & 5 & 6 & -0.004 & -0.653 \\
  \hline
  С tfidf 1 пр. & 6 & 7 & 5 & -1.540 & -0.038 \\
  \hline
  С tfidf 2 пр. & 8 & 6 & 4 & -0.101 & 0.146 \\
  \hline
  С tfidf 3 пр. & - & - & - & - & - \\
  \hline
  С tfidf 4 пр. & - & - & - & - & - \\
  \hline
  С tfidf 5 пр. & 8 & 6 & 6 & 1.139 & -1.981 \\
  \hline
  С tfidf 6 пр. & 8 & 6 & 7 & 0.954 & -1.353 \\
  \hline
  С tfidf 7 пр. & 8 & 5 & 5 & 0.942 & -0.102 \\
  \hline
  С tfidf 8 пр. & 6 & 7 & 7 & 1.757 & -1.222 \\
  \hline
  С tfidf 9 пр. & 8 & 6 & 7 & -0.449 & -0.365 \\
  \hline
  С tfidf 10 пр. & 8 & 5 & 6 & -0.184 & -1.826 \\
  \hline
  С tfidf 10 пр. с add. & 8 & 5 & 6 & 0.385 & -1.165 \\
  \hline
\end{longtable}

Ключевые наблюдения:

\begin{itemize}
    \item Наилучшее качество (когерентность 0.514) достигнуто при:
    \begin{itemize}
        \item Удалении низкочастотных слов;
        \item Отказе от TF"=IDF фильтрации стоп"=слов;
        \item Пороге TF"=IDF 8 процентов.
    \end{itemize}

    \item Потенциальные причины результата:
    \begin{itemize}
        \item Ограничения оптимизации: Неполный перебор гиперпараметров;
        \item Недостаток вариантов: Не исследованы комбинации TF"=IDF с
        удалением стоп-слов и низкочастотных терминов;
        \item Методические риски: Возможная некорректность TF"=IDF
        фильтрации стоп-слов (требует дополнительной проверки).
    \end{itemize}

    \item Влияние порогов TF"=IDF:
    \begin{itemize}
        \item Пороги > 8 процентов приводят к снижению качества;
        \item Высокие пороги удаляют смысловые термины;
        \item Оптимальный диапазон: 5"=8 процентов.
    \end{itemize}
\end{itemize}

Возможные пути улучшения:

\begin{itemize}
    \item Расширить пространство поиска гиперпараметров;
    \item Исследовать комбинированные стратегии очистки данных;
    \item Провести валидацию метода TF"=IDF фильтрации стоп"=слов.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Обучение модели классификатора}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор модели для тематической классификации}  

Как установлено ранее, для решения задачи наиболее эффективны трансформеры.
Существует три основных типа архитектур:  

\begin{itemize}  
    \item Encoder"=only (BERT, RoBERTa): Содержат только кодирующую
    часть;  
    \item Decoder"=only (GPT): Содержат только декодирующую часть;  
    \item Encoder"=Decoder (BART, T5): Комбинируют обе части.  
\end{itemize}  

Их функциональные различия можно описать следующим образом:

\begin{itemize}  
    \item Encoder модели (BERT, RoBERTa) специализируются на
    понимании текста (задачи классификации, извлечения информации);  
    \item Decoder модели (GPT) оптимизированы для задачи генерации текста;  
    \item Гибридные модели (BART, T5) предназначены для  задачи трансформации
    текста (перевод, суммаризация).  
\end{itemize}  

Для тематической классификации требуется глубокое понимание контекста,
поэтому оптимальны encoder"=only модели. Среди них RoBERTa
(Robustly optimized BERT approach) демонстрирует преимущества перед
BERT:  

\begin{itemize}  
    \item Обучена на большем объёме данных;  
    \item Использует динамическое маскирование слов;  
    \item Исключает задачу предсказания следующего предложения;  
    \item Показывает лучшие результаты на NLU"=задачах.  
\end{itemize}  

Таким образом, для классификации новостей выберем RoBERTa. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Выбор способа для получения предобученных моделей}
Существует несколько способо получения весов предобученной модели: от
их скачивания с облака и github репозиториев, до получения через API
разных сайтов. Их этих методов будет предпочтительнее выбрать последний,
так как есть портал Hugging Face.

Hugging Face предсталяет собой большое хранилище различных моделей,
в том числе и предобученных крупными компаниями и исследователями (Google,
Facebook, Sberbank). Кроме того, данный сайи предоставляет удобный,
лаконичный и унифицированный интерфейс для работы с ним, что позволяет
делать код максимально компактным и читабельным.

Таким образом, будет получать предобученные модели с помощью портала
Hugging Face.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Получение весов предобученной модели}
Для начала работы с нейронными сетями с платформы Hugging Face необходимо
подключить следующие зависимости:

\lstinputlisting[
    caption={Подключение необходимых зависимсотей для работы с Hugging Face},
    label={lst:26}
]{code/hugging_face_dependicies.py}

С помощью данных библиотек будут происходить подготовка данных, загрузка
весов моделей и их обучение.

Для загрузки модели потребуется класс AutoModelForSequenceClassification
и его метод from\_pretrained, в который будут задаваться параметры
загрузки (название модели и тип решаемой ей задачи, для загрузки предобученной
на соответствующих данных модели). Реализация соответствующего кода
представлена в соответствующем листинге~\ref{lst:27}.

\lstinputlisting[
    caption={Загрузка весов модели},
    label={lst:27}
]{code/load_model.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Подготовка данных для работы с моделью}
Для работы с моделью нужно уметь разделять текст на слова. Осуществлять это
можно с помощью встроенного токенизатора, для этого его нужно будет загрузить
и настроить.

Загрузка происходит с помощью класса AutoTokenizer и метода from\_pretrained.
Реализация загрузки представлена ниже.

\lstinputlisting[
    caption={Загрузка токенизатора},
    label={lst:28}
]{code/load_tokenizer.py}

Данные токенизатор будет использоваться для преобразования данных к
нужному виду с помощью класса DataSets и метода map. 

Реализацию соответствующей функции можно увидеть в следующем листинге~\ref{lst:29}.

\lstinputlisting[
    caption={Загрузка токенизатора},
    label={lst:29}
]{code/tokenize_data.py}

Также важно не забыть преобразовать метки классов к числам, сделать
это можно следующим образом~\ref{lst:30}.

\lstinputlisting[
    caption={Загрузка токенизатора},
    label={lst:30}
]{code/tokenize_data.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Дообучение модели}
Так как выбранная модель не является огромной, а ресурсы предоставляемые
Google Colab позволяют использовать мощные графические ускорители, то
будем дообучать нейронную сеть целиком, без заморозки слоёв энкодера.

Перед обучением нужно сначала задать его параметры, реализуется
это с помощью класса TrainingArguments, в конструктор которого передаются
соответствующие параметры. Среди них можно выделить следующие:

\begin{itemize}
    \item Стратегия обучения (eval\_strategy);
    \item Стратегия сохранения результата (save\_strategy);
    \item Шаг ошибки (learning\_rate);
    \item Размер батча (per\_device\_train\_batch\_size, per\_device\_eval\_batch\_size);
    \item Количество эпох обучения (num\_train\_epochs);
    \item Метрика качества подбора лучшей модели (metric\_for\_best\_model).
\end{itemize}

Соответствующий код можно увидеть в следующем листинге~\ref{lst:31}.

\lstinputlisting[
    caption={Код установки параметров обучения},
    label={lst:31}
]{code/train_args.py}

Осталось только создать объект тренировщика и запустить его. Делается
это с помощью класса Trainer следующим образом~\ref{lst32}.

\lstinputlisting[
    caption={Код класса обучения},
    label={lst:32}
]{code/trainer.py}

Таким образом, была реализована основная функциональнось для обучения
тематического классификатора. Полный код можно увидеть в соответствующем
приложении.

% Раздел "Заключение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conclusion


%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Листинг вебскраппера} \label{sec:01}
\lstinputlisting[
    caption={Полный код вебскраппера}
]{code/parse_news.py}

\section{Листинг обработчика новостного массива} \label{sec:02}
\lstinputlisting[
    caption={Полный код подготовки новостного массива}
]{code/parse_news.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Количественные характеристики подготовленного и неподготовленного
новостного массива} \label{sec:03}

\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF"=IDF 1\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 2\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 3\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{Неподгот.}} & 
\rotatebox{90}{\textbf{Стоп-слова}} & 
\rotatebox{90}{\textbf{+Низкочаст.}} & 
\rotatebox{90}{\textbf{TF"=IDF 1\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 2\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 3\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 12131111 & 6545045 & - & 6479545 & 6414045 & 6348544 \\
\hline
Кол. уник. ток. & 278724 & 148677 & - & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 47 & 31 & - & 31 & 31 & 30 \\
\hline
Среднее кол. ток. в док. & 695 & 375 & - & 371 & 367 & 364 \\
\hline
Медианное кол. ток. в док. & - & 313 & - & 312 & 310 & 309 \\
\hline
Макс. кол. ток. в док. & 6514 & 3151 & - & 2903 & 2825 & 2766 \\
\hline
Мин. кол. уник. ток. в док. & 6 & 4 & - & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 39 & 27 & - & 27 & 27 & 30 \\
\hline
Сред. кол. уник. ток. в док. & 346 & 214 & - & 211 & 208 & 205 \\
\hline
Мед. кол. уник. ток. в док. & - & 186 & - & 185 & 183 & 182 \\
\hline
Макс. кол. уник. ток. в док. & 2287 & 1353 & - & 1299 & 1262 & 1214 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{6}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 4\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 5\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 6\%.}} & 
\rotatebox{90}{\textbf{TF"=IDF 7\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 8\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 9\%}} \\
\hline
\endfirsthead

\multicolumn{7}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 4\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 5\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 6\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 7\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 8\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 9\%}} \\
\hline
\endhead

\hline
\multicolumn{7}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 & 17340 & 17340 & 17340 & 17340 \\
\hline
Кол. токенов & 6283046 & 6217544 & 6152044 & 6086544 & 6021044 & 5955543 \\
\hline
Кол. уник. ток. & 148677 & 148677 & 148677 & 148677 & 148677 & 148677 \\
\hline
Мин. кол. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Модальное кол. ток. в док. & 30 & 30 & 30 & 30 & 29 & 29 \\
\hline
Среднее кол. ток. в док. & 360 & 356 & 352 & 349 & 345 & 341 \\
\hline
Медианное кол. ток. в док. & 307 & 306 & 305 & 303 & 301 & 299 \\
\hline
Макс. кол. ток. в док. & 2713 & 2662 & 2595 & 2545 & 2501 & 2424 \\
\hline
Мин. кол. уник. ток. в док. & 4 & 4 & 4 & 4 & 4 & 4 \\
\hline
Мод. кол. уник. ток. в док. & 27 & 29 & 29 & 28 & 28 & 28 \\
\hline
Сред. кол. уник. ток. в док. & 201 & 198 & 195 & 192 & 189 & 186 \\
\hline
Мед. кол. уник. ток. в док. & 181 & 179 & 177 & 176 & 174 & 172 \\
\hline
Макс. кол. уник. ток. в док. & 1164 & 1122 & 1085 & 1047 & 1018 & 986 \\
\hline
\end{longtable}


\begin{center}
\renewcommand{\arraystretch}{1.5} % Увеличиваем межстрочное расстояние
\setlength{\tabcolsep}{4pt} % Уменьшаем отступы между столбцами
\footnotesize % Уменьшаем размер шрифта
\end{center}

\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|*{3}{>{\centering\arraybackslash}p{1.5cm}|}}
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 10\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 10\% + Низк.}} \\
\hline
\endfirsthead

\multicolumn{3}{c}{{\normalsize Продолжение таблицы}} \\
\hline
\rotatebox{90}{\parbox{4cm}{\centering\textbf{Характеристика}}} & 
\rotatebox{90}{\textbf{TF"=IDF 10\%}} & 
\rotatebox{90}{\textbf{TF"=IDF 10\% + Низк.}} \\
\hline
\endhead

\hline
\multicolumn{3}{r}{{Продолжение следует...}} \\
\endfoot

\hline
\endlastfoot

Кол. док. & 17340 & 17340 \\
\hline
Кол. токенов & 5890042 & - \\
\hline
Кол. уник. ток. & 148677 & - \\
\hline
Мин. кол. ток. в док. & 4 & - \\
\hline
Модальное кол. ток. в док. & 30 & - \\
\hline
Среднее кол. ток. в док. & 337 & - \\
\hline
Медианное кол. ток. в док. & 297 & - \\
\hline
Макс. кол. ток. в док. & 2391 & - \\
\hline
Мин. кол. уник. ток. в док. & 4 & - \\
\hline
Мод. кол. уник. ток. в док. & 28 & - \\
\hline
Сред. кол. уник. ток. в док. & 182 & - \\
\hline
Мед. кол. уник. ток. в док. & 170 & - \\
\hline
Макс. кол. уник. ток. в док. & 946 & - \\
\hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Полный код класса My\_BigARTM\_model} \label{sec:04}
\lstinputlisting[
    caption={Полный код класса My\_BigRTM\_model}
]{code/My_BigARTM_model.py}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Полный код класса Hyperparameter\_optimizer} \label{sec:05}
\lstinputlisting[
    caption={Полный код класса Hyperparameter\_optimizer}
]{code/hyperparameter_optimizer.py}

\section{Полный код обучения модели классификатора}
\lstinputlisting[
    caption={Полный код обучения модели классификатора},
    label={lst:33}
]{code/calc_classificator.py}


\end{document}
