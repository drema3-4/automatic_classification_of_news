\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
\intro

В настоящее время обработка больших объёмов текстовых данных,
включая новостные потоки, становится критически важной задачей. Как в
научной среде, так и в бизнесе требуется оперативно анализировать информацию,
отслеживать тенденции и принимать решения. Однако анализ всего массива данных
невозможен из-за его масштабов, необходимо фильтровать информацию, оставляя
только нужную. 

Помочь в решении этой проблемы может тематическая классификация. Хотя многие
сайты и порталы предлагают рубрикацию контента, её точность часто оказывается
низкой: теги присваиваются некорректно или поверхостно. Это приводит к ошибкам
в поиске и анализе информации.

В таком случае необходим механизм позволяющий получать правильную тематическую
классификацию данных, который смог бы присваивать темы тем же новостям
автоматически. Одни из возможных инструментов, которые позволяют реализовать
подобие такого механизма "--- это тематические модели и алгоритмы машинного и
глубокого обучения. Первый из них позволяет косвенно выявить темы текстового
набора данных и разметить данные для обучения второго инструмента, который
сможет тематически классифицировать последующий текст.

Таким образом, целью данной работы является реализация механизма автоматической
тематической классификации новостей с помощью методов тематического моделирования
и глубокого и машинного обучения.

Для достижения этой цели необходимо решить следующие задачи:

\begin{enumerate}
	\item Реализовать механизм получения новостных массивов данных;
	\item Реализовать механизм подготовки текстовых данных;
	\item Вычислить тематические модели;
	\item Путём сравнительного анализа выявить наиболее удачную тематическую
	модель;
	\item Разметить данные для обучения на них моделей машинного и глубокого
	обучения;
	\item Обучить несколько моделей машинного и глубокого обучения и выявить
	наиболее удачную путём сравнительного анализа;
	\item Провести анализ получившихся результатов.
\end{enumerate}

\section{Теоретические и методологические основы}

\subsection{Получение текстовых данных}

\subsubsection{Выбор инструмента}
Для получения каких"=либо данных с сайта существует три основных метода:
\begin{itemize}
	\item Ручной метод "--- выписывание необходимой информации с помощью
	человека;
	\item Получение данных путём предоставления их запроса у владельца, с
	их последующим скачиванием;
	\item Получение данных программным путём.
\end{itemize}

Первый метод из"=за своей неэффективности можно сразу отбросить. Второй метод
далеко не всегда можно применить, кроме того вряд ли владельцы информационных
платформ будут оперативно отсылать все данные по первой просьбе. Таким образом,
остаётся только третий метод.

Оперативно и достаточно эффективно в большинстве случаев можно получить данные
применяя инструменты вебскраппинга. Дальше будет использоваться этот вариант
получения новостного массива.

Различные библиотеки для вебскраппинга доступны на разных языках, однако
исходя из того, что наиболее популярным языком для обработки данных
и работы с машинным и глубоким обучением является python, выберем библиотеки
доступные на нём. Такими библиотеками ялвяются requests, beautifulsoap4
и selenium. Первая бибилиотека позволяет отсылать http запросы. Вторая
библиотека позволяет преобразовывать html код в подобие классов для
удобного получения информации. Последняя библиотека позволяет обрабатывать
сайта, которые по http запросу не выдают html код наблюдаемой пользователем
страницы. Данная библиотека позволяет эмулировать работу браузера и получать
html код страницы прямо из него.

Такого набора хватит для обработки подавляющего большинства сайтов.

\subsubsection{Подбор информационной платформы}
В рамках данной работы среди всех типов текстовых данных будут рассматриваться
новостные. Теперь нужно подобрать сайт.

Если для получения информации есть несоклько возможных веб"=источников,
то стоит выбирать сайт по следующим критериям:

\begin{enumerate}
	\item Сайт имеет единую структуру документов;
	\item Сайт не блокирует http запросы отправляемые вебскраппером;
	\item Сайт не является реактивным, то есть в момент просмотра страницы
	html код страницы полностью сформирован и доступен по запросу клиенту.
\end{enumerate}

Будет идеально, если все пункты соблюдаются, одако, даже в случае отсутствия
пунктов 2 и 3, ограничения в большинстве случаев можно достаточно просто
обойти. В случае несоответствия пункта 1 могут возникнуть серьёзные трудности,
которые, в худшем случае, решить только методами веб скраппинга не получится.

В рамках данной работы будет использоваться новостной сайт ВШЭ. Данный сайт
соответствует всем описанным выше критериям.

\subsection{Подготовка текстовых данных}  

Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы включают:  

\begin{enumerate}  
    \item Очистка от технического шума: 
    \begin{itemize}  
        \item Удаление лишних пробелов, переносов строк;  
        \item Очистка от спецсимволов (скобки, HTML-теги, эмодзи);  
        \item Нормализация регистра (приведение всего текста к нижнему регистру).  
    \end{itemize}  
    
    \item Токенизация:  
    Разделение текста на слова или предложения;
    
    \item Лемматизация:  
    Приведение слов к начальной форме (например, «бежал» $\Rightarrow$ «бежать»);
    
    \item Удаление стоп-слов:  
    Исключение частых слов без смысловой нагрузки (предлоги, частицы, местоимения);  
\end{enumerate}  

\textbf{Обоснование выбора лемматизации вместо стемминга:}  
Стемминг (например, алгоритм Snowball) «обрубает» окончания по шаблонам («бежал» $\Rightarrow$ «беж»),
что искажает смысл. Лемматизация сохраняет семантику, что критично для
тематического моделирования.

\subsubsection{Выбор инструментов}  

Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов. Определение стоп"=слов в обоих случаях происходит по
заранее заданным словарям, поэтому разницы здесь нет. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает интеграцию в проект.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по предопределённому
списку, который не является исчерпывающим. Это связано с тем, что набор
стоп"=слов зависит от тематики текста, и универсального решения не существует.
Для дополнительной фильтрации применим метрику TF-IDF, которая оценивает
значимость слов. Формула расчёта:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа $d$. Её
расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп-слов?) и Gensim (расширенная
фильтрация через TF-IDF).  

\section{Практико"=технологические основы}

\subsection{Получение новостного массива путём вебскраппинга}
Для обработки такого простого новостного сайта как у ВШЭ достаточно
использования requests и beautifulsoap4, без selenium.

Чтобы наиболее просто и эффективно получить данные необходимо разобрать
структуру сайта и разработать соответствующие функции под каждую из частей.
Сам портал представляет собой многостраничный сайт, на каждой странице
которого расположено по 10 новостей с краткой информацией по каждой:
ссылка, дата, заголовок, краткое содержание. На каждую новость можно перейти
по ссылке для получения полного её содержания. 

Теперь последовательно реализуем функции"=обработчики под соответствующие части
сайта.

Чтобы получать html код страницы, необходимо воспользоваться библиотекой
requests и методом get. Данный метод отправляет запрос на сайт и получает
соответствующий код в качестве ответа, который можно сохранить в файл
для последующей выгрузки и обработки. Соответствующая функция расположена
в листинге~\ref{lst:01}.

\lstinputlisting[
    caption={Функция получения html кода страницы},
    label={lst:01}
]{code/get_page_function.py}

Далее нужно реализовать получение краткой информации о новости:
ссылка, дата, краткое содержание. Для этого нужно загрузить код страницы
из файла и преобразовать его к классам с помощью библиотеки beautifulsoap4.
Далее можно будет воспользоваться поиском по тегам и классам с помощью
метода find и получить текстовое содержимое с помощью методов text и get.
Пример получения ссылки и краткого содержания новости можно увидеть
в данном листинге~\ref{lst:02}.

\lstinputlisting[
    caption={Получение ссылки и краткого содержания},
    label={lst:02}
]{code/get_link_and_summary.py}

Теперь нужно реализовать функцию получения полного содержания новости. Для
этого нужно воспользоваться реализованной функцией get\_page (получить
код страницы по полученной ранее ссылке на новость), преобразовать его
в классы с помощью beautifulsoap4 и получить текстовое содержимое
с помощью методов find и text. Реализацию соответствующей функции
можно увидеть в листинге~\ref{lst:03}.

\lstinputlisting[
    caption={Функция получения полного текстового содержания новости},
    label={lst:03}
]{code/get_news_content.py}

Следующим шагом нужно вспомнить, что на странице располагается 10 новостей,
каждая новость располагается в теге div с классом post. Таким образом, нужно
10 раз проитерироваться по данным тегам и получить 10 новостей. Сделать это
можно с помощью метода find\_next\_sibling (он ищет следующий тег, который
идентичен по типу и классу предыдущему) и обычного цикла. Хранить полученное
содержимое удобно в pandas DataFrame, так как с помощью него удобно
обрабатывать полученные массивы данных и вычислять их колличественные
характеристики. Ключевые части соответствующей функции представлены в
следующем листинге~\ref{lst:04}.

\lstinputlisting[
    caption={Функция обработки одной страницы новостей},
    label={lst:04}
]{code/get_one_news_page.py}

Далее необходимо реализовать функцию обрабатывающую все страницы с новостями.
Сделать это можно путём многократного применения описанной выше функции обработки
одной новостной страницы к изменяемой ссылке страницы. Благодаря простому
устройству сайта ВШЭ менять эту ссылку можно достаточно просто с помощью
обычного цикла путём изменения индекса в одной части. Соответствующий код
представлен в следующем листинге~\ref{lst:05}.

\lstinputlisting[
    caption={Функция обработки всех страниц новостей},
    label={lst:05}
]{code/crawling_pages_function.py}

Осталось только для ускорения получения данных с файла добавить многопоточность.
Сделать это можно с помощью стандартных средств языка python, только стоит учесть,
что под каждый отдельный поток нужно будет создать свой отдельный контейнер
pandas DataFrame, чтобы избежать проблем с записью. Соответствующий код
представлен в следующем листинге~\ref{lst:06}.

\lstinputlisting[
    caption={Многопоточное получение новостей},
    label={lst:06}
]{code/multithreading_apply_function.py}

Полный код вебскраппера можно увидеть в соответствующем приложении~\ref{sec:01}.

\subsection{Подготовка новостного массива}

\subsubsection{Удаление лишних пробелов и переносов строк}
Для корректной токенизации и просто для удобства анализа текстовых данных
важно удалить из них лишние пробелы и переносы строк, сделать это можно 
с помощью стандартных средств языка python.

Функция будет иметь следующий алгоритм:

\begin{enumerate}
	\item Записываем в результирующую строку символы из исходной
	строки, пока не будет встречен символ пробела или переноса строки;
	\item Добавляем к результирующей строке 1 символ пробела и
	прекращаем добавление символов, пока не встретим символ, отличный
	от пробела или переноса строки;
	\item В случае, когда встретится символ не являющийся пробелом
	или переносом строки переходим к пункту 1. Повторяем описанные выше действия
	пока не будет пройдена вся исходная строка.
\end{enumerate}

Реализация соответствующей функции представлена в следующем листнге~\ref{lst:07}.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк},
    label={lst:07}
]{code/remove_extra_spaces_and_line_breaks.py}

\subsubsection{Разделение строк на русские и английские фрагменты}
Библиотека SpaCy обрабатывает текст с помощью различных предобученных нейронных
сетей, такие сети обучаются работе только на одном языке, например, только
на русском или английском языке.

Текст новостей с новостного сайта ВШЭ имеет вставки на английском языке,
что делает некорректным использование только одной предобученной нейронной сети.
Поэтому, чтобы применять сразу два типа нейронных сетей необходимо
разбивать строки на русские и английские фрагменты. Решить данную задачу
можно с помощью стандартных средств языка python.

Функция будет иметь следующий алгоритм:

\begin{enumerate}
	\item Определяем к какому алфавиту принадлежит первый буквенный символ строки
	и устанавливаем идентификатор в состояние соответствующее типу алфавита;
	\item Записываем последовательно символы строки во временную подстроку,
	пока не встретим букву другого алфавита;
	\item После встречи символа противоположного алфавита записываем в список
	кортеж вида (идентификатор алфавита, временная подстрока);
	\item Очищаем временную подстроку и изменяем состояние идентификатора на
	противоположное. После этого повторяем описанные выше действия, пока не
	будет пройдена вся исходная строка.
\end{enumerate}

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:08}.

\lstinputlisting[
    caption={Функция разбиения строки на русские и английские фрагменты},
    label={lst:08}
]{code/split_into_en_and_ru.py}

\subsubsection{Обработка двоеточий и временных меток}
При вычислении тематической модели BigARTM использует символ двоеточия как
служебный, поэтому наличие его в текстовых данных приведёт к возникновению
ошибок.

Само двоеточие, чаще всего, используется при написании времени, данные случаи
можно обработать. Другие случаи применения предусмотреть проблематично, поэтому
работать функция будет следующим образом: если двоеточие располагается в
шаблоне временной метки, то будем заменять её на строку time, в противном
случае будем просто удалять двоеточие.

Реализация соответствуещей функции представлена в следующем листинге~\ref{lst:09}.

\lstinputlisting[
    caption={Функция обработки двоеточий и временных меток},
    label={lst:09}
]{code/processing_token.py}

\subsubsection{Токенизация, лемматизация и удаление стоп"=слов по словарю}
Библиотека SpaCy имеет простой и удобный интерфейс. Для проведения
токенизации, лемматизации и обнаружении стоп слов достаточно просто передать
ей на вход строку. На выходе будет получен список объектов, в каждом из которых
содержится по одному из токенов, их принадлежность к стоп"=словам из словаря,
начальная и исходная формы. С помощью этих объектов удобно записать в
результирующую строку начальные формы токенов, которые не являются
стоп"=словами.

Пример применения библиотеки SpaCy к одной строке русского языка имеет
следующий вид~\ref{lst:10}.

\lstinputlisting[
    caption={Пример применения библиотеки SpaCy для обработки одной строки
	русского языка},
    label={lst:10}
]{code/apply_spacy_for_one_str.py}

Реализация полного алгоритма, сожержащего описанные выше функции представлена
в следующем листинге~\ref{lst:11}.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк,
	токенизации, лемматизации и удаления стоп"=слов по словарю},
    label={lst:11}
]{code/tokenize_lemmatize_and_del_stop_words.py}

\subsubsection{Удаление стоп"=слов с помощью метрики tfidf}
Как говорилось ранее удаление стоп"=слов только по словарю не может быть
исчерпывающим, поэтому можно применить метрику tfidf для расчёта значимости
слов и удалять слова с малой значимостью.

Расчёт этой метрики удобно с помощью библиотеки Gensim. Для этого нужно
вычислить по коллекции документов словарь, затем по словарю сформировать
частотный словаь "--- corpus, а уже по нему вычислить tfidf метрики для
слов.

Реализация соответствующей функции представлена в следующем листинге~\ref{lst:12}.

\lstinputlisting[
    caption={Функция вычисления tfidf метрики для слов документов},
    label={lst:12}
]{code/calc_dict_corpus_and_tfidf.py}

Однако данное вычисление не является полным, так как библиотека Gensim не
добавляет в словарь слова, значение tfidf которых точно будет равняться нулю.
В таком случае необходимо добавить недостающие слова. Реализация соответствующей
функции представлена в следующем листинге~\ref{lst:13}.

\lstinputlisting[
    caption={Функция добавление недостающих tfidf слов},
    label={lst:13}
]{code/add_missing_tfidf_words.py}

Последним шагом перед удалением стоп"=слов является вычисление границы,
по которой будет определяться принадлежность к стоп"=словам. Сделать это
можно следующим образом~\ref{lst:14}.

\lstinputlisting[
    caption={Функция вычисления tfidf границы},
    label={lst:14}
]{code/add_missing_tfidf_words.py}

В данном случае к стоп"=словам будут относиться слова, значение tfidf
метрики которых будет относится к $n$ минимальным процентам значений.

Теперь осталось только пройтись по датасету и удалить соответствующие
стоп"=слова. Реализация соответсвующей функции представлена в следующем
листинге~\ref{lst:15}.

\lstinputlisting[
    caption={Функция удаление вычисленных по метрике tfidf стоп"=слов},
    label={lst:15}
]{code/del_tfidf_stop_words.py}

Также стоит сказать, что также дополнительно стоит добавить удаление
низкочастотных слов, так как это может положительно повлиять на результаты
тематического моделирования.

Полный код обработчика новостного массива можно увидеть в соответствующем
приложении~\ref{sec:02}.

% Раздел "Заключение"
\conclusion


%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Нумеруемые объекты в приложении}


\section{Листинг вебскраппера} \label{sec:01}
\lstinputlisting[
    caption={Полный код вебскраппера}
]{code/parse_news.py}

\section{Листинг обработчика новостного массива} \label{sec:02}
\lstinputlisting[
    caption={Полный код подготовки новостного массива}
]{code/parse_news.py}



\end{document}
