\documentclass[bachelor, och, diploma]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{cancel}

\usepackage{listings}
\usepackage{xcolor}       % Для цветной подсветки
\usepackage{upquote}      % Для корректных кавычек в коде
\usepackage{graphicx}     % Для \scalebox (если нужно масштабировать)

\lstset{
    language=Python,                  % Язык программирования
    basicstyle=\ttfamily\small,       % Базовый шрифт
    keywordstyle=\color{blue},        % Стиль ключевых слов
    commentstyle=\color{green!50!black}, % Стиль комментариев
    stringstyle=\color{red},          % Стиль строк
    showstringspaces=false,           % Не показывать пробелы в строках
	breakatwhitespace=true,    % Переносить только на пробелах
    breakindent=20pt,         % Отступ при переносе строки
    postbreak=\space\space\space\space, % Отступ после переноса
    breaklines=true,                  % Переносить длинные строки
    frame=single,                     % Рамка вокруг кода
    numbers=left,                     % Нумерация строк слева
    numberstyle=\tiny\color{gray},    % Стиль номеров строк
    stepnumber=1,                     % Шаг нумерации
    tabsize=4,                        % Размер табуляции
    captionpos=b,                     % Позиция подписи (bottom)
    belowcaptionskip=5pt,             % Отступ после подписи
    xleftmargin=10pt,                 % Отступ слева
    xrightmargin=10pt,                % Отступ справа
	frame=none,  % Убирает рамку полностью
    literate=                         % Поддержка кириллицы (если нужно)
        {а}{{\cyra}}1 {б}{{\cyrb}}1 {в}{{\cyrv}}1
        {г}{{\cyrg}}1 {д}{{\cyrd}}1 {е}{{\cyre}}1
        {ё}{{\cyryo}}1 {ж}{{\cyrzh}}1 {з}{{\cyrz}}1
        {и}{{\cyri}}1 {й}{{\cyrishrt}}1 {к}{{\cyrk}}1
        {л}{{\cyrl}}1 {м}{{\cyrm}}1 {н}{{\cyrn}}1
        {о}{{\cyro}}1 {п}{{\cyrp}}1 {р}{{\cyrr}}1
        {с}{{\cyrs}}1 {т}{{\cyrt}}1 {у}{{\cyru}}1
        {ф}{{\cyrf}}1 {х}{{\cyrh}}1 {ц}{{\cyrc}}1
        {ч}{{\cyrch}}1 {ш}{{\cyrsh}}1 {щ}{{\cyrshch}}1
        {ъ}{{\cyrhrdsn}}1 {ы}{{\cyrery}}1 {ь}{{\cyrsftsn}}1
        {э}{{\cyrerev}}1 {ю}{{\cyryu}}1 {я}{{\cyrya}}1
        {А}{{\CYRA}}1 {Б}{{\CYRB}}1 {В}{{\CYRV}}1
        {Г}{{\CYRG}}1 {Д}{{\CYRD}}1 {Е}{{\CYRE}}1
        {Ё}{{\CYRYO}}1 {Ж}{{\CYRZH}}1 {З}{{\CYRZ}}1
        {И}{{\CYRI}}1 {Й}{{\CYRISHRT}}1 {К}{{\CYRK}}1
        {Л}{{\CYRL}}1 {М}{{\CYRM}}1 {Н}{{\CYRN}}1
        {О}{{\CYRO}}1 {П}{{\CYRP}}1 {Р}{{\CYRR}}1
        {С}{{\CYRS}}1 {Т}{{\CYRT}}1 {У}{{\CYRU}}1
        {Ф}{{\CYRF}}1 {Х}{{\CYRH}}1 {Ц}{{\CYRC}}1
        {Ч}{{\CYRCH}}1 {Ш}{{\CYRSH}}1 {Щ}{{\CYRSHCH}}1
        {Ъ}{{\CYRHRDSN}}1 {Ы}{{\CYRERY}}1 {Ь}{{\CYRSFTSN}}1
        {Э}{{\CYREREV}}1 {Ю}{{\CYRYU}}1 {Я}{{\CYRYA}}1
}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Автоматическая тематическая классификация новостного массива}

% Курс
\course{4}

% Группа
\group{451}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item $|A|$  "--- количество элементов в конечном множестве $A$;
%     \item $\det B$  "--- определитель матрицы $B$;
%     \item ИНС "--- Искусственная нейронная сеть;
%     \item FANN "--- Feedforward Artifitial Neural Network
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr

% Раздел "Введение"
\intro

В настоящее время обработка больших объёмов текстовых данных,
включая новостные потоки, становится критически важной задачей. Как в
научной среде, так и в бизнесе требуется оперативно анализировать информацию,
отслеживать тенденции и принимать решения. Однако анализ всего массива данных
невозможен из-за его масштабов, необходимо фильтровать информацию, оставляя
только нужную. 

Помочь в решении этой проблемы может тематическая классификация. Хотя многие
сайты и порталы предлагают рубрикацию контента, её точность часто оказывается
низкой: теги присваиваются некорректно или поверхостно. Это приводит к ошибкам
в поиске и анализе информации.

В таком случае необходим механизм позволяющий получать правильную тематическую
классификацию данных, который смог бы присваивать темы тем же новостям
автоматически. Одни из возможных инструментов, которые позволяют реализовать
подобие такого механизма "--- это тематические модели и алгоритмы машинного и
глубокого обучения. Первый из них позволяет косвенно выявить темы текстового
набора данных и разметить данные для обучения второго инструмента, который
сможет тематически классифицировать последующий текст.

Таким образом, целью данной работы является реализация механизма автоматической
тематической классификации новостей с помощью методов тематического моделирования
и глубокого и машинного обучения.

Для достижения этой цели необходимо решить следующие задачи:

\begin{enumerate}
	\item Реализовать механизм получения новостных массивов данных;
	\item Реализовать механизм подготовки текстовых данных;
	\item Вычислить тематические модели;
	\item Путём сравнительного анализа выявить наиболее удачную тематическую
	модель;
	\item Разметить данные для обучения на них моделей машинного и глубокого
	обучения;
	\item Обучить несколько моделей машинного и глубокого обучения и выявить
	наиболее удачную путём сравнительного анализа;
	\item Провести анализ получившихся результатов.
\end{enumerate}

\section{Теоретические и методологические основы}

\subsection{Получение текстовых данных}

\subsubsection{Выбор инструмента}
Для получения каких"=либо данных с сайта существует три основных метода:
\begin{itemize}
	\item Ручной метод "--- выписывание необходимой информации с помощью
	человека;
	\item Получение данных путём предоставления их запроса у владельца, с
	их последующим скачиванием;
	\item Получение данных программным путём.
\end{itemize}

Первый метод из"=за своей неэффективности можно сразу отбросить. Второй метод
далеко не всегда можно применить, кроме того вряд ли владельцы информационных
платформ будут оперативно отсылать все данные по первой просьбе. Таким образом,
остаётся только третий метод.

Оперативно и достаточно эффективно в большинстве случаев можно получить данные
применяя инструменты вебскраппинга. Дальше будет использоваться этот вариант
получения новостного массива.

Различные библиотеки для вебскраппинга доступны на разных языках, однако
исходя из того, что наиболее популярным языком для обработки данных
и работы с машинным и глубоким обучением является python, выберем библиотеки
доступные на нём. Такими библиотеками ялвяются requests, beautifulsoap4
и selenium. Первая бибилиотека позволяет отсылать http запросы. Вторая
библиотека позволяет преобразовывать html код в подобие классов для
удобного получения информации. Последняя библиотека позволяет обрабатывать
сайта, которые по http запросу не выдают html код наблюдаемой пользователем
страницы. Данная библиотека позволяет эмулировать работу браузера и получать
html код страницы прямо из него.

Такого набора хватит для обработки подавляющего большинства сайтов.

\subsubsection{Подбор информационной платформы}
В рамках данной работы среди всех типов текстовых данных будут рассматриваться
новостные. Теперь нужно подобрать сайт.

Если для получения информации есть несоклько возможных веб"=источников,
то стоит выбирать сайт по следующим критериям:

\begin{enumerate}
	\item Сайт имеет единую структуру документов;
	\item Сайт не блокирует http запросы отправляемые вебскраппером;
	\item Сайт не является реактивным, то есть в момент просмотра страницы
	html код страницы полностью сформирован и доступен по запросу клиенту.
\end{enumerate}

Будет идеально, если все пункты соблюдаются, одако, даже в случае отсутствия
пунктов 2 и 3, ограничения в большинстве случаев можно достаточно просто
обойти. В случае несоответствия пункта 1 могут возникнуть серьёзные трудности,
которые, в худшем случае, решить только методами веб скраппинга не получится.

В рамках данной работы будет использоваться новостной сайт ВШЭ. Данный сайт
соответствует всем описанным выше критериям.

\subsection{Подготовка текстовых данных}  

Полученные данные требуют предварительной обработки для устранения шума и
повышения качества анализа. Основные этапы включают:  

\begin{enumerate}  
    \item Очистка от технического шума: 
    \begin{itemize}  
        \item Удаление лишних пробелов, переносов строк;  
        \item Очистка от спецсимволов (скобки, HTML-теги, эмодзи);  
        \item Нормализация регистра (приведение всего текста к нижнему регистру).  
    \end{itemize}  
    
    \item Токенизация:  
    Разделение текста на слова или предложения;
    
    \item Лемматизация:  
    Приведение слов к начальной форме (например, «бежал» $\Rightarrow$ «бежать»);
    
    \item Удаление стоп-слов:  
    Исключение частых слов без смысловой нагрузки (предлоги, частицы, местоимения);  
\end{enumerate}  

\textbf{Обоснование выбора лемматизации вместо стемминга:}  
Стемминг (например, алгоритм Snowball) «обрубает» окончания по шаблонам («бежал» $\Rightarrow$ «беж»),
что искажает смысл. Лемматизация сохраняет семантику, что критично для
тематического моделирования.

\subsubsection{Выбор инструментов}  

Чтобы не повышать количество используемых языков, будем рассматривать только
инструменты, доступные на Python. Среди них выделяются: NLTK, Pymorphy3,
SpaCy и Gensim.  

Сделаем выбор между связкой NLTK + Pymorphy3 и SpaCy. Обе группы библиотек
позволяют проводить лемматизацию и удаление стоп"=слов, но реализуют это
по"=разному. NLTK и Pymorphy3 приводят слова к начальной форме без учёта
контекста, тогда как SpaCy "--- нейросетевой инструмент, анализирующий
окружение терминов. Определение стоп"=слов в обоих случаях происходит по
заранее заданным словарям, поэтому разницы здесь нет. Однако SpaCy
обеспечивает не только более точную лемматизацию, но и лаконичный интерфейс,
что упрощает интеграцию в проект.  

Как упоминалось ранее библиотека SpaCy определяет стоп"=слова только по предопределённому
списку, который не является исчерпывающим. Это связано с тем, что набор
стоп"=слов зависит от тематики текста, и универсального решения не существует.
Для дополнительной фильтрации применим метрику TF-IDF, которая оценивает
значимость слов. Формула расчёта:  

\begin{equation} \label{eq:01}
    tfidf(w, d) = \frac{n_{wd}}{n_{d}} \cdot \log\left(\frac{|D|}{|\{d \in D : w \in d\}|}\right),  
\end{equation}  
где:
\begin{itemize}
	\item $w$ "--- термин;
	\item $d$ "--- документ;
	\item $n_{wd}$ "--- частота встречаемости $w$ в $d$;
	\item $n_{d}$ "--- число терминов в $d$;
	\item $|D|$ "--- число документов в коллекции;
	\item $|\{d \in D : w \in d\}|$ "--- количество документов, содержащих $w$.
\end{itemize}

Данная метрика будет тем выше для термина $w$ в документе $d$, чем чаще
будет встречаться термин $w$ в документе $d$ и реже во всех
остальных документах коллекции. Таким образом, данную метрику можно
интерпретировать как метрику значимости слова $w$ для документа $d$. Её
расчёт будет производиться с помощью билиотеки Gensim.

Таким образом, для обработки текста выбраны SpaCy
(токенизация, лемматизация, базовые стоп-слов?) и Gensim (расширенная
фильтрация через TF-IDF).  

\section{Практико"=технологические основы}

\subsection{Получение новостного массива путём вебскраппинга}
Для обработки такого простого новостного сайта как у ВШЭ достаточно
использования requests и beautifulsoap4, без selenium.

Чтобы наиболее просто и эффективно получить данные необходимо разобрать
структуру сайта и разработать соответствующие функции под каждую из частей.
Сам портал представляет собой многостраничный сайт, на каждой странице
которого расположено по 10 новостей с краткой информацией по каждой:
ссылка, дата, заголовок, краткое содержание. На каждую новость можно перейти
по ссылке для получения полного её содержания. 

Теперь последовательно реализуем функции"=обработчики под соответствующие части
сайта.

Чтобы получать html код страницы, необходимо воспользоваться библиотекой
requests и методом get. Данный метод отправляет запрос на сайт и получает
соответствующий код в качестве ответа, который можно сохранить в файл
для последующей выгрузки и обработки. Соответствующая функция расположена
в листинге~\ref{lst:01}.

\lstinputlisting[
    caption={Функция получения html кода страницы},
    label={lst:01}
]{code/get_page_function.py}

Далее нужно реализовать получение краткой информации о новости:
ссылка, дата, краткое содержание. Для этого нужно загрузить код страницы
из файла и преобразовать его к классам с помощью библиотеки beautifulsoap4.
Далее можно будет воспользоваться поиском по тегам и классам с помощью
метода find и получить текстовое содержимое с помощью методов text и get.
Пример получения ссылки и краткого содержания новости можно увидеть
в данном листинге~\ref{lst:02}.

\lstinputlisting[
    caption={Получение ссылки и краткого содержания},
    label={lst:02}
]{code/get_link_and_summary.py}

Теперь нужно реализовать функцию получения полного содержания новости. Для
этого нужно воспользоваться реализованной функцией get\_page (получить
код страницы по полученной ранее ссылке на новость), преобразовать его
в классы с помощью beautifulsoap4 и получить текстовое содержимое
с помощью методов find и text. Реализацию соответствующей функции
можно увидеть в листинге~\ref{lst:03}.

\lstinputlisting[
    caption={Функция получения полного текстового содержания новости},
    label={lst:03}
]{code/get_news_content.py}

Следующим шагом нужно вспомнить, что на странице располагается 10 новостей,
каждая новость располагается в теге div с классом post. Таким образом, нужно
10 раз проитерироваться по данным тегам и получить 10 новостей. Сделать это
можно с помощью метода find\_next\_sibling (он ищет следующий тег, который
идентичен по типу и классу предыдущему) и обычного цикла. Хранить полученное
содержимое удобно в pandas DataFrame, так как с помощью него удобно
обрабатывать полученные массивы данных и вычислять их колличественные
характеристики. Ключевые части соответствующей функции представлены в
следующем листинге~\ref{lst:04}.

\lstinputlisting[
    caption={Функция обработки одной страницы новостей},
    label={lst:04}
]{code/get_one_news_page.py}

Далее необходимо реализовать функцию обрабатывающую все страницы с новостями.
Сделать это можно путём многократного применения описанной выше функции обработки
одной новостной страницы к изменяемой ссылке страницы. Благодаря простому
устройству сайта ВШЭ менять эту ссылку можно достаточно просто с помощью
обычного цикла путём изменения индекса в одной части. Соответствующий код
представлен в следующем листинге~\ref{lst:05}.

\lstinputlisting[
    caption={Функция обработки всех страниц новостей},
    label={lst:05}
]{code/crawling_pages_function.py}

Осталось только для ускорения получения данных с файла добавить многопоточность.
Сделать это можно с помощью стандартных средств языка python, только стоит учесть,
что под каждый отдельный поток нужно будет создать свой отдельный контейнер
pandas DataFrame, чтобы избежать проблем с записью. Соответствующий код
представлен в следующем листинге~\ref{lst:06}.

\lstinputlisting[
    caption={Многопоточное получение новостей},
    label={lst:06}
]{code/multithreading_apply_function.py}

Полный код вебскраппера можно увидеть в соответствующем приложении~\ref{sec:01}.

\subsection{Подготовка новостного массива}

\subsubsection{Очистка от лишних пробелов и переноса строк}
Воизбежание некорректной токенизации и просто ради удобства просмотра
коллекции документов необходимо провести удаление лишних пробелов и переносов
строк. Сделать это можно с помощью стандартных средств языка python. Код
соответствующего алгоритма представлен в следующем листинге~\ref{lst:08}.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк},
    label={lst:08}
]{code/remove_extra_spaces_and_line_breaks.py}

\subsubsection{Разделение строки фрагменты с русским и английским}
Библиотека SpaCy является нейросетевой. Она предоставляет различные предобученные
модели, как для русского языка, так и для английского, но не для мультиязычного.
Из"=за этого возникает проблема, так как в выбранном новостном массиве
присутствует текст на английском, а модель, предобученная для русского обработать
его не сможет.

Решить эту проблему можно, если обрабатывать русский и английский текст по
отдельности. Для этого нужно разбивать строку на подстроки только с русским
или английским текстом. Реализовать это можно с помощью стандартных средств
языка python.

Алгоритм действия будет следующим:
\begin{enumerate}
	\item Ищем первую букву в исходной последовательности;
	\item Определяем к какому алфавиту принадлежит буква;
	\item Далее записываем в подстроку символы, пока не встретим букву из
	другого алфавита;
	\item Если была встречена буква из другого алфавита, то сохраняем
	кортеж вида (тип алфавита, подстрока) в соответствующий массив,
	очищаем входную последовательность, изменяем тип алфавита и переходим
	к пункту 3. Так выполняем, пока исходная строка не пройдена полностью.
\end{enumerate}

Реализация соответствующих функций представлена в следующем листинге.

\lstinputlisting[
    caption={Функция удаления лишних пробелов и переносов строк},
    label={lst:08}
]{code/split_into_en_and_ru.py}

\subsubsection{Обработка временных меток и двоеточий}
Важно также обработать все токены, содержащие двоеточия. Так как
для BigARTM тематической модели двоеточие является спецсимволом,
то присутствие его в тексте может приветсти к ошибкам в вычислении тематической
модели.

Двоеточие может указывать как на время, так и использоваться в других случаях,
значение которых угадать будет трудно. Тогда все случаи употребления
символа двоеточия во временных метках будем заменять на строку time, а
в остальных случая будем просто данный символ удалять. Таким образом,
мы обработаем временные метки и удалим лишние двоеточия.

Реализация соответствующей функции представлена в следующем листинге.


\subsubsection{Токенизация, лемматизация, очистка от стоп"=слов}
После того как исходная последовательность разбита на русские и английские
подстроки её можно обработать. Обработка будет происходить с помощью
библиотеки SpaCy. Для этого ей достаточно просто передать последовательность,
а она уже внутри себя токенизирует последовательность, приведёт токены к
начальной форме, посредством лемматизации, а также по заданному в ней словарю
определит принадлежность каждого из слов к стоп"=словам. На выходе
получится набор объектов, в каждом из которых содержится по одному из токентов,
его характеристи и начальная форма. Набор будет иметь тот же порядок, что и
входная последовательность.

Реализация 

% Раздел "Заключение"
\conclusion


%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Нумеруемые объекты в приложении}


\section{Листинг вебскраппера} \label{sec:01}
\lstinputlisting[
    caption={Полный код вебскраппера}
]{code/parse_news.py}


\end{document}
